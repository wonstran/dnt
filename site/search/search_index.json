{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"dnt API Manual","text":"<p>This site is generated with MkDocs and <code>mkdocstrings</code> from the Python source in <code>src/dnt</code>.</p> <p>Use the navigation to browse package modules and APIs.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<ol> <li>Install the package in your environment.</li> <li>Run one of the examples in <code>examples/</code>.</li> <li>Open the API reference for detailed class and function docs.</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>The pages in this section are rendered from module docstrings and object signatures.</p>"},{"location":"api/dnt/","title":"dnt Package","text":"<p>DNT package for detection and tracking.</p> <p>This package provides detection and tracking functionality.</p>"},{"location":"api/detect/","title":"Detection","text":"<p>Detection module for object detection using YOLOv8.</p>"},{"location":"api/detect/#dnt.detect.Detector","title":"Detector","text":"<pre><code>Detector(\n    model: DetectorModel = DetectorModel.YOLO26x,\n    weights: str | None = None,\n    conf: float = 0.25,\n    nms: float = 0.7,\n    max_det: int = 300,\n    device: str = \"auto\",\n    half: bool = False,\n)\n</code></pre> <p>A wrapper around Ultralytics detection models for running object detection on videos and selected frames.</p> <p>This class loads a YOLO (v8, v11, 26) or RT-DETR model from a local <code>models/</code> directory (or from a user-supplied .pt file) and provides convenience methods to:</p> <ul> <li>detect objects frame by frame in a video and return results as a   pandas DataFrame,</li> <li>run detection only on specified frame indices,</li> <li>process a batch of videos and save per-video detection text files, and</li> <li>query basic video properties (FPS, frame count).</li> </ul> <p>The detector automatically chooses an inference device (<code>cuda</code>, <code>xpu</code>, <code>mps</code>, or <code>cpu</code>) when <code>device=\"auto\"</code>, and it can optionally enable half-precision inference on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DetectorModel</code> <p>Built-in model weights to use (for example <code>DetectorModel.YOLO26x</code>). Default is <code>DetectorModel.YOLO26x</code>.</p> <code>YOLO26x</code> <code>weights</code> <code>str</code> <p>Optional custom model weights to load. If relative, path is resolved under <code>&lt;module_dir&gt;/models/</code>. Default is None.</p> <code>None</code> <code>conf</code> <code>float</code> <p>Confidence threshold for detections. Default is <code>0.25</code>.</p> <code>0.25</code> <code>nms</code> <code>float</code> <p>IoU / non-maximum suppression threshold. Default is <code>0.7</code>.</p> <code>0.7</code> <code>max_det</code> <code>int</code> <p>Maximum number of detections per frame. Default is <code>300</code>.</p> <code>300</code> <code>device</code> <code>(auto, cuda, xpu, cpu, mps)</code> <p>Inference device to use. If <code>\"auto\"</code>, the detector will pick an available accelerator first (<code>cuda</code> \u2192 <code>xpu</code> \u2192 <code>mps</code>) and fall back to CPU. Default is <code>\"auto\"</code>.</p> <code>\"auto\"</code> <code>enable_half</code> <code>bool</code> <p>Whether to enable half-precision inference. This is only effective on GPU (CUDA). Default is <code>False</code>.</p> required Notes <ul> <li>The class expects model weight files to be located under   <code>&lt;module_dir&gt;/models/</code> when using the built-in weight names.</li> <li>Returned detection tables typically contain the columns:   <code>frame, res, x, y, w, h, conf, class</code>.</li> </ul> <p>Initialize a Detector for Ultralytics YOLO/RT-DETR models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DetectorModel</code> <p>Built-in model to use. Default is \"yolo26x\".</p> <code>YOLO26x</code> <code>weights</code> <code>str</code> <p>Customized model weights to load. Default is None, which means using the built-in weights in <code>model</code> choice.</p> <code>None</code> <code>conf</code> <code>float</code> <p>Confidence threshold. Default is 0.25.</p> <code>0.25</code> <code>nms</code> <code>float</code> <p>IoU/NMS threshold. Default is 0.7.</p> <code>0.7</code> <code>max_det</code> <code>int</code> <p>Maximum detections per frame. Default is 300. In crowded scenes, you may want to increase this.</p> <code>300</code> <code>device</code> <code>(auto, cuda, xpu, cpu, mps)</code> <p>Inference device. Default is \"auto\".</p> <code>\"auto\"</code> <code>half</code> <code>bool</code> <p>Whether to use half precision (GPU only). Default is False.</p> <code>False</code> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def __init__(\n    self,\n    model: DetectorModel = DetectorModel.YOLO26x,\n    weights: str | None = None,\n    conf: float = 0.25,\n    nms: float = 0.7,\n    max_det: int = 300,\n    device: str = \"auto\",\n    half: bool = False,\n):\n    \"\"\"Initialize a Detector for Ultralytics YOLO/RT-DETR models.\n\n    Parameters\n    ----------\n    model : DetectorModel, optional\n        Built-in model to use. Default is \"yolo26x\".\n    weights : str, optional\n        Customized model weights to load.\n        Default is None, which means using the built-in weights in `model` choice.\n    conf : float, optional\n        Confidence threshold. Default is 0.25.\n    nms : float, optional\n        IoU/NMS threshold. Default is 0.7.\n    max_det : int, optional\n        Maximum detections per frame.\n        Default is 300. In crowded scenes, you may want to increase this.\n    device : {\"auto\", \"cuda\", \"xpu\", \"cpu\", \"mps\"}, optional\n        Inference device. Default is \"auto\".\n    half : bool, optional\n        Whether to use half precision (GPU only). Default is False.\n\n    \"\"\"\n    # Load model\n    cwd = Path(__file__).parent.absolute()\n    model_dir = cwd / \"models\"\n    if not model_dir.exists():\n        os.makedirs(model_dir)\n\n    if weights:\n        model_path = Path(weights) if os.path.isabs(weights) else model_dir / weights\n    else:\n        model_path = model_dir / f\"{model.value}\"\n\n    # actually load model\n    if (\"yolo\" in str(weights).lower()) or (model in YOLO_MODELS):\n        self.model = YOLO(str(model_path))\n    elif (\"rtdetr\" in str(weights).lower()) or (model in RTDETR_MODELS):\n        self.model = RTDETR(str(model_path))\n    else:\n        raise ValueError(\n            f\"Cannot infer model family from model={model} and weights={weights!r}. \"\n            \"Use a known DetectorModel or provide weights containing 'yolo' or 'rtdetr'.\"\n        )\n    self.conf = conf\n    self.nms = nms\n    self.max_det = max_det\n\n    # device selection\n    requested_device = str(device).lower().strip()\n    requested_backend = requested_device.split(\":\", maxsplit=1)[0]\n    valid_devices = {\"auto\", \"cuda\", \"xpu\", \"mps\", \"cpu\"}\n    if requested_backend not in valid_devices:\n        raise ValueError(\n            f\"Invalid device={device!r}. Choose one of {sorted(valid_devices)} or backend:index like 'cuda:0'.\"\n        )\n\n    backend_available = {\n        \"cuda\": torch.cuda.is_available(),\n        \"xpu\": hasattr(torch, \"xpu\") and hasattr(torch.xpu, \"is_available\") and torch.xpu.is_available(),\n        \"mps\": hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available(),\n        \"cpu\": True,\n    }\n\n    if requested_backend == \"auto\":\n        auto_priority = (\"cuda\", \"xpu\", \"mps\", \"cpu\")\n        self.device = next(d for d in auto_priority if backend_available[d])\n    else:\n        self.device = requested_device if backend_available[requested_backend] else \"cpu\"\n\n    # half precision only makes sense on GPU\n    self.half = half and (self.device == \"cuda\")\n</code></pre>"},{"location":"api/detect/#dnt.detect.Detector.detect","title":"detect","text":"<pre><code>detect(\n    input_video: str,\n    iou_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    verbose: bool = True,\n    show: bool = False,\n    disp_filename: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run object detection on a video and return per-frame detections.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>iou_file</code> <code>str</code> <p>If provided, detection results are written to this file (CSV without header).</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Index of this video in a batch, used only for progress display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total number of videos in the batch, used only for progress display.</p> <code>None</code> <code>start_frame</code> <code>int</code> <p>Frame index to start detection from. If None or out of range, starts at 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Frame index to stop detection at. If None or out of range, uses the last frame.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Whether to display the video frames with detections. Default is False.</p> <code>False</code> <code>disp_filename</code> <code>bool</code> <p>Whether to show the file name in the progress bar. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: <code>frame, res, x, y, w, h, conf, class</code>. If the video cannot be opened or no detections are found, an empty DataFrame with those columns is returned.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect(\n    self,\n    input_video: str,\n    iou_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    verbose: bool = True,\n    show: bool = False,\n    disp_filename: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run object detection on a video and return per-frame detections.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to the input video file.\n    iou_file : str, optional\n        If provided, detection results are written to this file (CSV without header).\n    video_index : int, optional\n        Index of this video in a batch, used only for progress display.\n    video_tot : int, optional\n        Total number of videos in the batch, used only for progress display.\n    start_frame : int, optional\n        Frame index to start detection from. If None or out of range, starts at 0.\n    end_frame : int, optional\n        Frame index to stop detection at. If None or out of range, uses the last frame.\n    verbose : bool, optional\n        Whether to show a progress bar. Default is True.\n    show : bool, optional\n        Whether to display the video frames with detections. Default is False.\n    disp_filename: bool, optional\n        Whether to show the file name in the progress bar. Default is False.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with columns:\n        `frame, res, x, y, w, h, conf, class`.\n        If the video cannot be opened or no detections are found, an empty DataFrame\n        with those columns is returned.\n\n    \"\"\"\n    # validate path\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        if verbose:\n            print(f\"Cannot open video: {input_video}\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # normalize start_frame\n    if start_frame is None or start_frame &lt; 0 or start_frame &gt;= tot_frames:\n        start_frame = 0\n    # normalize end_frame\n    if end_frame is None or end_frame &lt; 0 or end_frame &gt;= tot_frames:\n        end_frame = tot_frames - 1\n    if start_frame &gt; end_frame:\n        cap.release()\n        raise ValueError(\"start_frame must be less than or equal to end_frame.\")\n\n    frame_total = end_frame - start_frame + 1\n\n    # Some codecs return 0 or -1 for frame count\n    if verbose:\n        if tot_frames &lt;= 0:\n            pbar = tqdm(desc=\"Detecting\", unit=\"frame\")\n        else:\n            pbar = tqdm(total=frame_total, desc=\"Detecting\", unit=\"frame\")\n\n        if (video_index is not None) and (video_tot is not None):\n            desc = f\"Detecting {video_index} of {video_tot}\"\n            if disp_filename:\n                desc += f\" - {Path(input_video).name}\"\n                pbar.set_description_str(desc)\n        else:\n            desc = \"Detecting\"\n            if disp_filename:\n                desc += f\" {Path(input_video).name}\"\n                pbar.set_description_str(desc)\n\n    results: list[dict] = []\n    frame_idx = start_frame\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n\n    win_name = \"Detection (press q/ESC to quit)\"\n    if show:\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n\n    # optional FPS calc\n    t0 = time()\n    n_show = 0\n\n    while cap.isOpened():\n        pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if end_frame is not None and frame_idx &gt; end_frame:\n            break\n\n        preds = self.model.predict(\n            source=frame,\n            conf=self.conf,\n            iou=self.nms,\n            max_det=self.max_det,\n            device=self.device,\n            half=self.half,\n            verbose=False,\n        )\n\n        det = preds[0]\n        boxes = det.boxes\n        if boxes is not None and len(boxes) &gt; 0:\n            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n            confs = boxes.conf.cpu().numpy()  # (N,)\n            clss = boxes.cls.cpu().numpy().astype(int)  # (N,)\n\n            for (x1, y1, x2, y2), cf, c in zip(xyxy, confs, clss, strict=True):\n                results.append({\n                    \"frame\": pos_frame,\n                    \"res\": -1,\n                    \"x\": float(x1),\n                    \"y\": float(y1),\n                    \"x2\": float(x2),\n                    \"y2\": float(y2),\n                    \"conf\": float(cf),\n                    \"class\": int(c),\n                })\n\n        if show:\n            # Ultralytics built-in drawing (fast &amp; clean)\n            vis = det.plot()  # returns BGR image with boxes/labels\n\n            # add simple overlay: frame index + FPS\n            n_show += 1\n            dt = time() - t0\n            fps = n_show / dt if dt &gt; 0 else 0.0\n            cv2.putText(\n                vis,\n                f\"frame={pos_frame}/{frame_total}  fps={fps:.1f}\",\n                (10, 25),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.7,\n                (0, 255, 255),\n                2,\n            )\n\n            cv2.imshow(win_name, vis)\n            key = cv2.waitKey(1) &amp; 0xFF\n            if key == ord(\"q\") or key == 27:  # q or ESC\n                break\n\n        if verbose and pbar is not None:\n            pbar.update(1)\n\n        frame_idx += 1\n\n    cap.release()\n    if verbose and pbar is not None:\n        pbar.close()\n\n    if not results:\n        empty_df = pd.DataFrame(columns=self.DET_FIELDS)\n        if iou_file:\n            empty_df.to_csv(iou_file, index=False, header=False)\n        return empty_df\n\n    else:\n        results_df = pd.DataFrame(results, columns=[\"frame\", \"res\", \"x\", \"y\", \"x2\", \"y2\", \"conf\", \"class\"])\n        results_df[\"w\"] = (results_df[\"x2\"] - results_df[\"x\"]).astype(int)\n        results_df[\"h\"] = (results_df[\"y2\"] - results_df[\"y\"]).astype(int)\n        results_df[\"x\"] = results_df[\"x\"].astype(int)\n        results_df[\"y\"] = results_df[\"y\"].astype(int)\n        results_df[\"conf\"] = results_df[\"conf\"].round(2)\n        results_df = results_df[self.DET_FIELDS].reset_index(drop=True)\n\n    if iou_file:\n        folder = Path(iou_file).parent\n        if not folder.exists():\n            Path(folder).mkdir(parents=True, exist_ok=True)\n\n        results_df.to_csv(iou_file, index=False, header=False)\n        if verbose:\n            print(f\"Wrote detections to {iou_file}\")\n\n    return results_df\n</code></pre>"},{"location":"api/detect/#dnt.detect.Detector.detect_frames","title":"detect_frames","text":"<pre><code>detect_frames(\n    input_video: str,\n    frames: list[int],\n    verbose: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run object detection on specific frames of a video.</p> <p>This method is useful when you don't need to process the entire video and only want detections for selected frame indices.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>frames</code> <code>list of int</code> <p>List of frame indices to process.</p> required <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>frame, res, x, y, w, h, conf, class</code>. If the video cannot be opened or no detections are found, an empty DataFrame with those columns is returned.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect_frames(\n    self,\n    input_video: str,\n    frames: list[int],\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Run object detection on specific frames of a video.\n\n    This method is useful when you don't need to process the entire video and\n    only want detections for selected frame indices.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to the input video file.\n    frames : list of int\n        List of frame indices to process.\n    verbose : bool, optional\n        Whether to show a progress bar. Default is True.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with columns\n        `frame, res, x, y, w, h, conf, class`.\n        If the video cannot be opened or no detections are found, an empty\n        DataFrame with those columns is returned.\n\n    \"\"\"\n    # validate path\n    if not os.path.exists(input_video):\n        # return an empty, well-shaped DataFrame instead of None\n        if verbose:\n            print(f\"{input_video} does not exist!\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        if verbose:\n            print(f\"Cannot open {input_video}\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    results: list[dict] = []\n\n    pbar = tqdm(total=len(frames), unit=\" frames\") if verbose else None\n\n    for pos_frame in frames:\n        # move to target frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame)\n        ret, frame = cap.read()\n        if not ret:\n            # e.g. frame index out of range\n            continue\n\n        preds = self.model.predict(\n            frame,\n            verbose=False,\n            conf=self.conf,\n            iou=self.nms,\n            max_det=self.max_det,\n            device=self.device,\n            half=self.half,\n        )\n        det = preds[0]\n        boxes = det.boxes\n        if boxes is not None and len(boxes) &gt; 0:\n            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n            confs = boxes.conf.cpu().numpy()  # (N,)\n            clss = boxes.cls.cpu().numpy().astype(int)  # (N,)\n\n            for (x1, y1, x2, y2), cf, c in zip(xyxy, confs, clss, strict=True):\n                results.append({\n                    \"frame\": pos_frame,\n                    \"res\": -1,\n                    \"x\": float(x1),\n                    \"y\": float(y1),\n                    \"x2\": float(x2),\n                    \"y2\": float(y2),\n                    \"conf\": float(cf),\n                    \"class\": int(c),\n                })\n\n        if pbar is not None:\n            pbar.update()\n\n    if pbar is not None:\n        pbar.close()\n    cap.release()\n\n    # no detections at all\n\n    if not results:\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    df = pd.DataFrame(results)\n    # compute width/height and round\n    df[\"w\"] = (df[\"x2\"] - df[\"x\"]).round(0)\n    df[\"h\"] = (df[\"y2\"] - df[\"y\"]).round(0)\n    df[\"x\"] = df[\"x\"].round(1)\n    df[\"y\"] = df[\"y\"].round(1)\n    df[\"conf\"] = df[\"conf\"].round(2)\n    df[\"class\"] = df[\"class\"].round(0).astype(int)\n\n    df = df[self.DET_FIELDS].reset_index(drop=True)\n\n    return df\n</code></pre>"},{"location":"api/detect/#dnt.detect.Detector.detect_batch","title":"detect_batch","text":"<pre><code>detect_batch(\n    input_videos: list[str],\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n    verbose: bool = True,\n    disp_filename: bool = True,\n) -&gt; list[str]\n</code></pre> <p>Run detection on multiple videos and optionally write per-video output files.</p> <p>Parameters:</p> Name Type Description Default <code>input_videos</code> <code>list of str</code> <p>Paths to the input video files to be processed.</p> required <code>output_path</code> <code>str</code> <p>Directory where per-video detection files will be written. If None, detections are not written to disk and the returned list will be empty.</p> <code>None</code> <code>is_overwrite</code> <code>bool</code> <p>If False (default), existing detection files with the same name will be skipped. If True, they will be regenerated.</p> <code>False</code> <code>is_report</code> <code>bool</code> <p>If True (default), existing detection files (that were skipped) are still included in the returned list.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints progress messages. Default is True.</p> <code>True</code> <code>disp_filename</code> <code>bool</code> <p>If True, prints file name in progress bar. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of paths to detection files that were created or already existed. If <code>output_path</code> is None, this will be an empty list.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect_batch(\n    self,\n    input_videos: list[str],\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n    verbose: bool = True,\n    disp_filename: bool = True,\n) -&gt; list[str]:\n    \"\"\"Run detection on multiple videos and optionally write per-video output files.\n\n    Parameters\n    ----------\n    input_videos : list of str\n        Paths to the input video files to be processed.\n    output_path : str, optional\n        Directory where per-video detection files will be written. If None,\n        detections are not written to disk and the returned list will be empty.\n    is_overwrite : bool, optional\n        If False (default), existing detection files with the same name will be\n        skipped. If True, they will be regenerated.\n    is_report : bool, optional\n        If True (default), existing detection files (that were skipped) are still\n        included in the returned list.\n    verbose : bool, optional\n        If True, prints progress messages. Default is True.\n    disp_filename: bool, optional\n        If True, prints file name in progress bar. Default is True.\n\n    Returns\n    -------\n    list of str\n        A list of paths to detection files that were created or already existed.\n        If `output_path` is None, this will be an empty list.\n\n    \"\"\"\n    results: list[str] = []\n    total_videos = len(input_videos)\n\n    for idx, input_video in enumerate(input_videos, start=1):\n        # default: no output file\n        iou_file = None\n\n        # build output path / file name if requested\n        if output_path is not None:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            base_filename = os.path.splitext(os.path.basename(input_video))[0]\n            iou_file = os.path.join(output_path, f\"{base_filename}_iou.txt\")\n\n        # if we have an output file name, check overwrite logic\n        if (iou_file is not None) and (not is_overwrite) and os.path.exists(iou_file):\n            if is_report:\n                results.append(iou_file)\n            # skip processing this video\n            continue\n\n        # run detection (may write to iou_file if not None)\n        self.detect(\n            input_video=input_video,\n            iou_file=iou_file,\n            video_index=idx,\n            video_tot=total_videos,\n            verbose=verbose,\n            disp_filename=disp_filename,\n        )\n\n        if iou_file is not None:\n            results.append(iou_file)\n\n    return results\n</code></pre>"},{"location":"api/detect/#dnt.detect.Detector.get_fps","title":"get_fps  <code>staticmethod</code>","text":"<pre><code>get_fps(video: str) -&gt; float\n</code></pre> <p>Return the frames-per-second (FPS) value of a video file.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>float</code> <p>FPS of the video. Returns 0.0 if the video cannot be opened.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>@staticmethod\ndef get_fps(video: str) -&gt; float:\n    \"\"\"Return the frames-per-second (FPS) value of a video file.\n\n    Parameters\n    ----------\n    video : str\n        Path to the video file.\n\n    Returns\n    -------\n    float\n        FPS of the video. Returns 0.0 if the video cannot be opened.\n\n    \"\"\"\n    if not Path(video).exists():\n        print(f\"{video} does not exist!\")\n        return 0.0\n    cap = cv2.VideoCapture(video)\n    if not cap.isOpened():\n        print(f\"Failed to open the video: {video}\")\n        return 0.0\n\n    fps = float(cap.get(cv2.CAP_PROP_FPS))\n    cap.release()\n    return fps\n</code></pre>"},{"location":"api/detect/#dnt.detect.Detector.get_frames","title":"get_frames  <code>staticmethod</code>","text":"<pre><code>get_frames(video: str) -&gt; int\n</code></pre> <p>Return the total number of frames in a video file.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total frame count. Returns 0 if the video cannot be opened.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>@staticmethod\ndef get_frames(video: str) -&gt; int:\n    \"\"\"Return the total number of frames in a video file.\n\n    Parameters\n    ----------\n    video : str\n        Path to the video file.\n\n    Returns\n    -------\n    int\n        Total frame count. Returns 0 if the video cannot be opened.\n\n    \"\"\"\n    if not Path(video).exists():\n        print(f\"{video} does not exist!\")\n        return 0\n    cap = cv2.VideoCapture(video)\n    if not cap.isOpened():\n        print(f\"Failed to open the video: {video}\")\n        return 0\n\n    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return frames\n</code></pre>"},{"location":"api/detect/#dnt.detect.DetectorModel","title":"DetectorModel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of available YOLO and RT-DETR model weights.</p> <p>Each member represents a different model variant with its corresponding weight file name.</p>"},{"location":"api/detect/signal/","title":"Signal Detector","text":""},{"location":"api/detect/signal/#dnt.detect.signal.detector.Model","title":"Model","text":"<pre><code>Model(num_class=2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet18-based model for traffic signal detection.</p> <p>A neural network model that uses ResNet18 as the backbone for classifying traffic signal states.</p> <p>Attributes:</p> Name Type Description <code>resnet18</code> <code>ResNet</code> <p>The ResNet18 backbone model with a custom fully connected layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Pass input through the model and return predictions.</p> <p>Initialize the Model with ResNet18 backbone.</p> <p>Parameters:</p> Name Type Description Default <code>num_class</code> <code>int</code> <p>The number of output classes, by default 2</p> <code>2</code> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def __init__(self, num_class=2):\n    \"\"\"Initialize the Model with ResNet18 backbone.\n\n    Parameters\n    ----------\n    num_class : int, optional\n        The number of output classes, by default 2\n\n    \"\"\"\n    super().__init__()\n\n    self.resnet18 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n    self.resnet18.fc = nn.Linear(512, num_class)\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector","title":"SignalDetector","text":"<pre><code>SignalDetector(\n    det_zones: list,\n    model: str = \"ped\",\n    weights: str = None,\n    batchsz: int = 64,\n    num_class: int = 2,\n    threshold: float = 0.98,\n    device=\"auto\",\n)\n</code></pre> <p>Detector for traffic signal status in videos.</p> <p>A class that uses a pre-trained ResNet18 model to detect pedestrian signal states (walking or not walking) in video frames.</p> <p>Attributes:</p> Name Type Description <code>det_zones</code> <code>list</code> <p>List of detection zones as tuples (x, y, w, h).</p> <code>model</code> <code>Model</code> <p>The neural network model for signal detection.</p> <code>device</code> <code>str</code> <p>The device to run the model on ('cuda', 'mps', or 'cpu').</p> <code>batchsz</code> <code>int</code> <p>The batch size for predictions.</p> <code>threshold</code> <code>float</code> <p>The confidence threshold for positive detections.</p> <p>Methods:</p> Name Description <code>detect</code> <p>Detect signal states in a video file.</p> <code>gen_ped_interval</code> <p>Generate pedestrian signal intervals from detections.</p> <code>crop_zone</code> <p>Crop detection zones from a video frame.</p> <code>predict</code> <p>Make predictions on a batch of cropped images.</p> <code>generate_labels</code> <p>Generate visualization labels for signal states.</p> <p>Detect traffic signal status.</p> <p>Parameters:</p> Name Type Description Default <code>det_zones</code> <code>list</code> <p>Cropped zones for detection list[(x, y, w, h)]</p> required <code>model</code> <code>str</code> <p>Detection model, default is 'ped', 'custom'</p> <code>'ped'</code> <code>weights</code> <code>str</code> <p>Path of weights, default is None</p> <code>None</code> <code>batchsz</code> <code>int</code> <p>The batch size for prediction, default is 64</p> <code>64</code> <code>num_class</code> <code>int</code> <p>The number of classes, default is 2</p> <code>2</code> <code>threshold</code> <code>float</code> <p>The threshold for detection, default is 0.98</p> <code>0.98</code> <code>device</code> <code>str</code> <p>The device to run the model on ('cuda', 'mps', 'cpu', or 'auto'), default is 'auto'</p> <code>'auto'</code> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def __init__(\n    self,\n    det_zones: list,\n    model: str = \"ped\",\n    weights: str = None,\n    batchsz: int = 64,\n    num_class: int = 2,\n    threshold: float = 0.98,\n    device=\"auto\",\n):\n    \"\"\"Detect traffic signal status.\n\n    Parameters\n    ----------\n    det_zones : list\n        Cropped zones for detection list[(x, y, w, h)]\n    model : str, optional\n        Detection model, default is 'ped', 'custom'\n    weights : str, optional\n        Path of weights, default is None\n    batchsz : int, optional\n        The batch size for prediction, default is 64\n    num_class : int, optional\n        The number of classes, default is 2\n    threshold : float, optional\n        The threshold for detection, default is 0.98\n    device : str, optional\n        The device to run the model on ('cuda', 'mps', 'cpu', or 'auto'), default is 'auto'\n\n    \"\"\"\n    self.det_zones = det_zones\n\n    cwd = Path(__file__).parent.absolute()\n    if not weights and model == \"ped\":\n        weights = os.path.join(cwd, \"weights\", \"ped_signal.pt\")\n\n    if not os.path.exists(weights):\n        url = \"https://its.cutr.usf.edu/alms/downloads/ped_signal.pt\"\n        download_file(url, weights)\n\n    self.model = Model(num_class)\n    if device == \"auto\":\n        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    elif device == \"cuda\" and torch.cuda.is_available():\n        self.device = \"cuda\"\n    elif device == \"mps\" and torch.backends.mps.is_available():\n        self.device = \"mps\"\n    else:\n        self.device = \"cpu\"\n\n    self.model.load_state_dict(torch.load(weights))\n    self.model.to(self.device)\n\n    self.batchsz = batchsz\n    self.threshold = threshold\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.detect","title":"detect","text":"<pre><code>detect(\n    input_video: str,\n    det_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Detect signal states in a video file.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>det_file</code> <code>str</code> <p>Path to save detection results CSV. If None, results are not saved.</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Detection results with columns: ['frame', 'signal', 'detection'].</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def detect(\n    self,\n    input_video: str,\n    det_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Detect signal states in a video file.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to the input video file.\n    det_file : str, optional\n        Path to save detection results CSV. If None, results are not saved.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n\n    Returns\n    -------\n    pd.DataFrame\n        Detection results with columns: ['frame', 'signal', 'detection'].\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    # initialize the result array\n    results = np.array([])  # np.full((0, len(self.det_zones)), -1)\n    frames = np.array([])\n    zones = np.array([])\n    batch = []\n    temp_frames = []\n\n    # open input video\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(f\"Failed to open video: {input_video}\")\n\n    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    pbar = tqdm(total=tot_frames, unit=\" frame\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Detecting signals {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Detecting signals \")\n\n    while cap.isOpened():\n        pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        crop_img = self.crop_zone(frame)\n        batch.append(crop_img)\n        temp_frames.append(pos_frame)\n\n        if ((pos_frame + 1) % self.batchsz == 0) or (pos_frame &gt;= tot_frames - 1):\n            # batch_pred = self.predict(batch).reshape(-1, len(self.det_zones))\n            # results = np.append(results, batch_pred, axis=0)\n\n            batch_pred = self.predict(batch).flatten()\n            results = np.append(results, batch_pred, axis=0)\n            zones = np.append(zones, np.tile(np.array(list(range(len(self.det_zones)))), self.batchsz), axis=0)\n            frames = np.append(frames, np.repeat(np.array(temp_frames), len(self.det_zones)), axis=0)\n\n            batch = []\n            temp_frames = []\n\n        pbar.update()\n\n    pbar.close()\n    cap.release()\n\n    df = pd.DataFrame(list(zip(frames, zones, results, strict=True)), columns=[\"frame\", \"signal\", \"detection\"])\n\n    if det_file:\n        df.to_csv(det_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.gen_ped_interval","title":"gen_ped_interval","text":"<pre><code>gen_ped_interval(\n    dets: DataFrame,\n    input_video: str,\n    walk_interval: int,\n    countdown_interval: int,\n    out_file: str | None,\n    factor: float = 0.75,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate pedestrian signal intervals from detections.</p> <p>Parameters:</p> Name Type Description Default <code>dets</code> <code>DataFrame</code> <p>Signal detection results from <code>detect()</code> method.</p> required <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>walk_interval</code> <code>int</code> <p>Walking signal interval in seconds (typically 4-7 seconds).</p> required <code>countdown_interval</code> <code>int</code> <p>Countdown interval in seconds. Typically calculated as: crossing length (ft) / 4 (ft/s).</p> required <code>out_file</code> <code>str</code> <p>Path to save interval results CSV. If None, results are not saved.</p> required <code>factor</code> <code>float</code> <p>Detection threshold (0-1). If factor portion of frames in a sliding window show walking signal, it's classified as walking. Default is 0.75 (75%).</p> <code>0.75</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Interval data with columns: ['signal', 'status', 'beg_frame', 'end_frame'].</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def gen_ped_interval(\n    self,\n    dets: pd.DataFrame,\n    input_video: str,\n    walk_interval: int,\n    countdown_interval: int,\n    out_file: str | None,\n    factor: float = 0.75,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate pedestrian signal intervals from detections.\n\n    Parameters\n    ----------\n    dets : pd.DataFrame\n        Signal detection results from `detect()` method.\n    input_video : str\n        Path to the input video file.\n    walk_interval : int\n        Walking signal interval in seconds (typically 4-7 seconds).\n    countdown_interval : int\n        Countdown interval in seconds. Typically calculated as:\n        crossing length (ft) / 4 (ft/s).\n    out_file : str, optional\n        Path to save interval results CSV. If None, results are not saved.\n    factor : float, optional\n        Detection threshold (0-1). If *factor* portion of frames in a\n        sliding window show walking signal, it's classified as walking.\n        Default is 0.75 (75%).\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n\n    Returns\n    -------\n    pd.DataFrame\n        Interval data with columns: ['signal', 'status', 'beg_frame', 'end_frame'].\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    # open input video\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(f\"Failed to open video: {input_video}\")\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    cap.release()\n\n    results = []\n    for i in range(len(self.det_zones)):\n        dets_i = dets[dets[\"signal\"] == i]\n        results.append(\n            self.scan_walk_interval(dets_i, int(fps * walk_interval), factor, i, fps, countdown_interval)\n        )\n\n    df = pd.concat(results, axis=0)\n    if out_file:\n        df.to_csv(out_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.scan_walk_interval","title":"scan_walk_interval","text":"<pre><code>scan_walk_interval(\n    dets: DataFrame,\n    window: int,\n    factor: float,\n    zone: int,\n    fps: int = 30,\n    countdown_interval: int = 10,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Scan detection sequence to identify walking signal intervals.</p> <p>Parameters:</p> Name Type Description Default <code>dets</code> <code>DataFrame</code> <p>Detection data for a specific signal zone.</p> required <code>window</code> <code>int</code> <p>Sliding window size in frames.</p> required <code>factor</code> <code>float</code> <p>Threshold ratio (0-1) for positive detection within window.</p> required <code>zone</code> <code>int</code> <p>Signal zone identifier.</p> required <code>fps</code> <code>int</code> <p>Video frame rate. Default is 30.</p> <code>30</code> <code>countdown_interval</code> <code>int</code> <p>Duration in seconds for countdown phase after walk signal. Default is 10.</p> <code>10</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display (currently unused).</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display (currently unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Interval data with columns: ['signal', 'status', 'beg_frame', 'end_frame']. Status: 1 = walking, 2 = countdown.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def scan_walk_interval(\n    self,\n    dets: pd.DataFrame,\n    window: int,\n    factor: float,\n    zone: int,\n    fps: int = 30,\n    countdown_interval: int = 10,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Scan detection sequence to identify walking signal intervals.\n\n    Parameters\n    ----------\n    dets : pd.DataFrame\n        Detection data for a specific signal zone.\n    window : int\n        Sliding window size in frames.\n    factor : float\n        Threshold ratio (0-1) for positive detection within window.\n    zone : int\n        Signal zone identifier.\n    fps : int, optional\n        Video frame rate. Default is 30.\n    countdown_interval : int, optional\n        Duration in seconds for countdown phase after walk signal.\n        Default is 10.\n    video_index : int, optional\n        Video index for batch processing display (currently unused).\n    video_tot : int, optional\n        Total video count for batch processing display (currently unused).\n\n    Returns\n    -------\n    pd.DataFrame\n        Interval data with columns: ['signal', 'status', 'beg_frame', 'end_frame'].\n        Status: 1 = walking, 2 = countdown.\n\n    \"\"\"\n    sequence = dets[\"detection\"].to_numpy()\n\n    if len(sequence) == 0:\n        return pd.DataFrame(columns=[\"signal\", \"status\", \"beg_frame\", \"end_frame\"])\n\n    frame_intervals = []\n    pre_walk = False\n    tmp_cnt = 0\n\n    pbar = tqdm(total=len(sequence) - window, unit=\" frame\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Scanning intervals for signal {zone}, {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(f\"Scanning intervals for signal {zone}\")\n\n    for i in range(len(sequence) - window):\n        count = sum(sequence[i : i + window])\n\n        # check if the current frame can be a start of green light\n        is_walking = count &gt;= factor * window\n\n        # if the current is green\n        # 1) if prev status is green, update the latest interval\n        # 2) if prev status is not green, append a new interval\n        if is_walking:\n            if not pre_walk:\n                frame_intervals.append([i, i + window])\n                tmp_cnt = 0\n            else:\n                if count &gt; tmp_cnt:\n                    tmp_cnt = count\n                    frame_intervals[-1] = [i, i + window]\n\n        pre_walk = is_walking\n\n        pbar.update()\n    pbar.close()\n\n    results = []\n    for start, end in frame_intervals:\n        results.append([zone, 1, int(dets[\"frame\"].iloc[start]), int(dets[\"frame\"].iloc[end])])\n        results.append([\n            zone,\n            2,\n            int(dets[\"frame\"].iloc[end]) + 1,\n            int(dets[\"frame\"].iloc[end] + int(countdown_interval * fps)),\n        ])\n\n    df = pd.DataFrame(results, columns=[\"signal\", \"status\", \"beg_frame\", \"end_frame\"])\n    return df\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.crop_zone","title":"crop_zone","text":"<pre><code>crop_zone(frame: ndarray) -&gt; list[Image.Image]\n</code></pre> <p>Crop detection zones from a video frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>Input video frame (BGR image).</p> required <p>Returns:</p> Type Description <code>list[Image]</code> <p>List of cropped PIL Images, one per detection zone.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def crop_zone(self, frame: np.ndarray) -&gt; list[Image.Image]:\n    \"\"\"Crop detection zones from a video frame.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        Input video frame (BGR image).\n\n    Returns\n    -------\n    list[Image.Image]\n        List of cropped PIL Images, one per detection zone.\n\n    \"\"\"\n    crop_regions = []\n    for _, region in enumerate(self.det_zones):\n        x, y, w, h = region\n        cropped = frame[y : y + h, x : x + w]\n        cropped = Image.fromarray(cropped)\n        crop_regions.append(cropped)\n    return crop_regions\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.predict","title":"predict","text":"<pre><code>predict(batch: list[list[Image]]) -&gt; np.ndarray\n</code></pre> <p>Make predictions on a batch of cropped images.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Image]]</code> <p>Batch of cropped images. Outer list represents frames, inner list represents zones per frame.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Binary predictions (0 or 1) with shape (batch_size, num_zones). 1 indicates detected walking signal, 0 otherwise.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def predict(self, batch: list[list[Image.Image]]) -&gt; np.ndarray:\n    \"\"\"Make predictions on a batch of cropped images.\n\n    Parameters\n    ----------\n    batch : list[list[Image.Image]]\n        Batch of cropped images. Outer list represents frames, inner list\n        represents zones per frame.\n\n    Returns\n    -------\n    np.ndarray\n        Binary predictions (0 or 1) with shape (batch_size, num_zones).\n        1 indicates detected walking signal, 0 otherwise.\n\n    \"\"\"\n    self.model.eval()\n    sf = nn.Softmax(dim=1)\n    batchsz = len(batch)\n    num_group = len(batch[0])\n\n    # define the image transform\n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    # Transform all images and stack to different groups(corresponding light)\n    transformed_crops = [[transform(image) for image in crop] for crop in batch]\n    transformed_batch = [torch.stack([transformed_crops[j][i] for j in range(batchsz)]) for i in range(num_group)]\n\n    # send to model and make predictions zone by zone\n    batch_pred = []\n    for group in transformed_batch:\n        # compute the probability of the detections\n        outputs = self.model(group.to(self.device))\n        sf_output = sf(outputs)\n\n        # predict green only if the probability &gt; threshold\n        y_pred = (sf_output[:, 1] &gt; self.threshold).int()\n        batch_pred.append(y_pred.data.cpu().numpy())\n\n    pred_array = np.array(batch_pred).T\n    # prob_array = np.array(batch_prob).T\n\n    return pred_array\n</code></pre>"},{"location":"api/detect/signal/#dnt.detect.signal.detector.SignalDetector.generate_labels","title":"generate_labels","text":"<pre><code>generate_labels(\n    signals: DataFrame,\n    input_video: str,\n    label_file: str | None,\n    size_factor: float = 1.5,\n    thick: int = 1,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate visualization labels for signal states.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>DataFrame</code> <p>Signal interval data from <code>gen_ped_interval()</code> method.</p> required <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>label_file</code> <code>str</code> <p>Path to save label CSV. If None, results are not saved.</p> required <code>size_factor</code> <code>float</code> <p>Multiplier for circle radius relative to zone size. Default is 1.5.</p> <code>1.5</code> <code>thick</code> <code>int</code> <p>Circle outline thickness. Use -1 for filled circles. Default is 1.</p> <code>1</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Label data with columns: ['frame', 'type', 'coords', 'color', 'size', 'thick', 'desc']. Colors: red (0,0,255) = no walking, green (0,255,0) = walking, yellow (0,255,255) = countdown.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/detect/signal/detector.py</code> <pre><code>def generate_labels(\n    self,\n    signals: pd.DataFrame,\n    input_video: str,\n    label_file: str | None,\n    size_factor: float = 1.5,\n    thick: int = 1,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate visualization labels for signal states.\n\n    Parameters\n    ----------\n    signals : pd.DataFrame\n        Signal interval data from `gen_ped_interval()` method.\n    input_video : str\n        Path to the input video file.\n    label_file : str, optional\n        Path to save label CSV. If None, results are not saved.\n    size_factor : float, optional\n        Multiplier for circle radius relative to zone size. Default is 1.5.\n    thick : int, optional\n        Circle outline thickness. Use -1 for filled circles. Default is 1.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n\n    Returns\n    -------\n    pd.DataFrame\n        Label data with columns: ['frame', 'type', 'coords', 'color', 'size', 'thick', 'desc'].\n        Colors: red (0,0,255) = no walking, green (0,255,0) = walking,\n        yellow (0,255,255) = countdown.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    # open input video\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(f\"Failed to open video: {input_video}\")\n    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n\n    pbar = tqdm(total=tot_frames)\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Generating signal labeles for {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Generating signal labels\")\n\n    results = []\n    for i in range(tot_frames):\n        for j in range(len(self.det_zones)):\n            status = 0  # no walking\n            selected = signals[(signals[\"beg_frame\"] &lt;= i) &amp; (signals[\"end_frame\"] &gt;= i) &amp; (signals[\"signal\"] == j)]\n            if len(selected) &gt; 0:\n                status = selected[\"status\"].iloc[0]\n\n            x, y, w, h = self.det_zones[j]\n            cx = int(x + w / 2)\n            cy = int(y + h / 2)\n            r = int(max(w, h) * size_factor)\n\n            if status == 0:\n                results.append([i, \"circle\", [(cx, cy)], (0, 0, 255), r, thick, \"\"])\n            elif status == 1:\n                results.append([i, \"circle\", [(cx, cy)], (0, 255, 0), r, thick, \"\"])\n            elif status == 2:\n                results.append([i, \"circle\", [(cx, cy)], (0, 255, 255), r, thick, \"\"])\n\n            pbar.update()\n\n    df = pd.DataFrame(results, columns=[\"frame\", \"type\", \"coords\", \"color\", \"size\", \"thick\", \"desc\"])\n    df.sort_values(by=\"frame\")\n\n    if label_file:\n        df.to_csv(label_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/detect/timestamp/","title":"Timestamp Detector","text":"<p>Timestamp extraction module using OCR.</p> <p>This module provides functionality to extract timestamps from images and videos using optical character recognition (OCR). It includes the TimestampExtractor class which can extract timestamps from: - Static images - Specific frames in video files - The first readable frame in a video file</p>"},{"location":"api/detect/timestamp/#dnt.detect.timestamp.TimestampExtractor","title":"TimestampExtractor","text":"<pre><code>TimestampExtractor(\n    zone: ndarray = None, allowlist=\"0123456789-:/\"\n)\n</code></pre> <p>A class for extracting timestamps from images and videos using OCR.</p> <p>Attributes:</p> Name Type Description <code>zone</code> <code>(ndarray, optional)</code> <p>The coordinates of the timestamp area in the image.</p> <code>reader</code> <code>Reader</code> <p>The EasyOCR reader instance for text recognition.</p> <code>allowlist</code> <code>str</code> <p>Characters to allow in the OCR process.</p> <p>Methods:</p> Name Description <code>extract_timestamp</code> <p>Extract timestamp from an image using OCR.</p> <code>extract_timestamp_video</code> <p>Extract timestamp from a specific frame in a video file.</p> <code>extract_timestamp_video_auto</code> <p>Extract timestamp from the first readable frame in a video file.</p> <p>Initialize the TimestampExtractor.</p> <p>Args:     zone (np.ndarray): The coordinates of the timestamp area in the image.     allowlist (str): Characters to allow in the OCR process.</p> Source code in <code>src/dnt/detect/timestamp.py</code> <pre><code>def __init__(self, zone: np.ndarray = None, allowlist=\"0123456789-:/\"):\n    \"\"\"Initialize the TimestampExtractor.\n\n    Args:\n        zone (np.ndarray): The coordinates of the timestamp area in the image.\n        allowlist (str): Characters to allow in the OCR process.\n\n    \"\"\"\n    self.zone = zone\n    self.reader = easyocr.Reader([\"en\"])\n    self.allowlist = allowlist\n</code></pre>"},{"location":"api/detect/timestamp/#dnt.detect.timestamp.TimestampExtractor.extract_timestamp","title":"extract_timestamp","text":"<pre><code>extract_timestamp(\n    img: ndarray, gray: bool = False\n) -&gt; datetime\n</code></pre> <p>Extract timestamp from the image using OCR.</p> <p>Args:     img (np.ndarray): The input image from which to extract the timestamp.     gray (bool): Whether to convert the image to grayscale before OCR. returns:     datetime: The extracted timestamp.</p> Source code in <code>src/dnt/detect/timestamp.py</code> <pre><code>def extract_timestamp(\n    self,\n    img: np.ndarray,\n    gray: bool = False,\n) -&gt; datetime:\n    \"\"\"Extract timestamp from the image using OCR.\n\n    Args:\n        img (np.ndarray): The input image from which to extract the timestamp.\n        gray (bool): Whether to convert the image to grayscale before OCR.\n    returns:\n        datetime: The extracted timestamp.\n\n    \"\"\"\n    # Ensure the zone is a numpy array\n    if self.zone is None:\n        crop_img = img\n    else:\n        x1, y1, x2, y2 = self.zone[0][0], self.zone[0][1], self.zone[2][0], self.zone[2][1]\n        crop_img = img[y1:y2, x1:x2]\n\n    crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY) if gray else crop_img\n\n    result = self.reader.readtext(crop_img, detail=0, allowlist=\"0123456789-:/\")\n    dt = dateparser.parse(\" \".join(result))\n    return dt\n</code></pre>"},{"location":"api/detect/timestamp/#dnt.detect.timestamp.TimestampExtractor.extract_timestamp_video","title":"extract_timestamp_video","text":"<pre><code>extract_timestamp_video(\n    video_path: str, frame: int = 0, gray: bool = False\n) -&gt; datetime\n</code></pre> <p>Extract timestamp from a video file using OCR.</p> <p>Args:     video_path (str): The path to the video file.     frame (int): The frame index to extract the timestamp from.     gray (bool): Whether to convert the image to grayscale before OCR. returns:     datetime: The extracted timestamp.</p> Source code in <code>src/dnt/detect/timestamp.py</code> <pre><code>def extract_timestamp_video(\n    self,\n    video_path: str,\n    frame: int = 0,\n    gray: bool = False,\n) -&gt; datetime:\n    \"\"\"Extract timestamp from a video file using OCR.\n\n    Args:\n        video_path (str): The path to the video file.\n        frame (int): The frame index to extract the timestamp from.\n        gray (bool): Whether to convert the image to grayscale before OCR.\n    returns:\n        datetime: The extracted timestamp.\n\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n    ret, img = cap.read()\n    cap.release()\n\n    if not ret:\n        raise ValueError(\"Could not read video file\")\n\n    return self.extract_timestamp(img, gray=gray)\n</code></pre>"},{"location":"api/detect/timestamp/#dnt.detect.timestamp.TimestampExtractor.extract_timestamp_video_auto","title":"extract_timestamp_video_auto","text":"<pre><code>extract_timestamp_video_auto(\n    video_path: str, gray: bool = False\n) -&gt; tuple\n</code></pre> <p>Extract the initial timestamp from the first readable frame in a video file using OCR.</p> <p>Args:     video_path (str): The path to the video file.     gray (bool): Whether to convert the image to grayscale before OCR. returns:     dt, frame: The extracted timestamp and the frame index.</p> Source code in <code>src/dnt/detect/timestamp.py</code> <pre><code>def extract_timestamp_video_auto(\n    self,\n    video_path: str,\n    gray: bool = False,\n) -&gt; tuple:\n    \"\"\"Extract the initial timestamp from the first readable frame in a video file using OCR.\n\n    Args:\n        video_path (str): The path to the video file.\n        gray (bool): Whether to convert the image to grayscale before OCR.\n    returns:\n        dt, frame: The extracted timestamp and the frame index.\n\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n    tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    for frame in range(0, tot - 1):\n        ret, img = cap.read()\n        if not ret:\n            break\n        dt = self.extract_timestamp(img, gray=gray)\n        if dt:\n            cap.release()\n            return dt, frame\n\n    cap.release()\n    return None, None\n</code></pre>"},{"location":"api/detect/yolov8/","title":"YOLOv8 Detector","text":"<p>Detection wrapper around Ultralytics YOLO/RT-DETR for video frames.</p> <p>Revised by wonstran     01/28/2026     11/11/2025.</p>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.DetectorModel","title":"DetectorModel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of available YOLO and RT-DETR model weights.</p> <p>Each member represents a different model variant with its corresponding weight file name.</p>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector","title":"Detector","text":"<pre><code>Detector(\n    model: DetectorModel = DetectorModel.YOLO26x,\n    weights: str | None = None,\n    conf: float = 0.25,\n    nms: float = 0.7,\n    max_det: int = 300,\n    device: str = \"auto\",\n    half: bool = False,\n)\n</code></pre> <p>A wrapper around Ultralytics detection models for running object detection on videos and selected frames.</p> <p>This class loads a YOLO (v8, v11, 26) or RT-DETR model from a local <code>models/</code> directory (or from a user-supplied .pt file) and provides convenience methods to:</p> <ul> <li>detect objects frame by frame in a video and return results as a   pandas DataFrame,</li> <li>run detection only on specified frame indices,</li> <li>process a batch of videos and save per-video detection text files, and</li> <li>query basic video properties (FPS, frame count).</li> </ul> <p>The detector automatically chooses an inference device (<code>cuda</code>, <code>xpu</code>, <code>mps</code>, or <code>cpu</code>) when <code>device=\"auto\"</code>, and it can optionally enable half-precision inference on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DetectorModel</code> <p>Built-in model weights to use (for example <code>DetectorModel.YOLO26x</code>). Default is <code>DetectorModel.YOLO26x</code>.</p> <code>YOLO26x</code> <code>weights</code> <code>str</code> <p>Optional custom model weights to load. If relative, path is resolved under <code>&lt;module_dir&gt;/models/</code>. Default is None.</p> <code>None</code> <code>conf</code> <code>float</code> <p>Confidence threshold for detections. Default is <code>0.25</code>.</p> <code>0.25</code> <code>nms</code> <code>float</code> <p>IoU / non-maximum suppression threshold. Default is <code>0.7</code>.</p> <code>0.7</code> <code>max_det</code> <code>int</code> <p>Maximum number of detections per frame. Default is <code>300</code>.</p> <code>300</code> <code>device</code> <code>(auto, cuda, xpu, cpu, mps)</code> <p>Inference device to use. If <code>\"auto\"</code>, the detector will pick an available accelerator first (<code>cuda</code> \u2192 <code>xpu</code> \u2192 <code>mps</code>) and fall back to CPU. Default is <code>\"auto\"</code>.</p> <code>\"auto\"</code> <code>enable_half</code> <code>bool</code> <p>Whether to enable half-precision inference. This is only effective on GPU (CUDA). Default is <code>False</code>.</p> required Notes <ul> <li>The class expects model weight files to be located under   <code>&lt;module_dir&gt;/models/</code> when using the built-in weight names.</li> <li>Returned detection tables typically contain the columns:   <code>frame, res, x, y, w, h, conf, class</code>.</li> </ul> <p>Initialize a Detector for Ultralytics YOLO/RT-DETR models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DetectorModel</code> <p>Built-in model to use. Default is \"yolo26x\".</p> <code>YOLO26x</code> <code>weights</code> <code>str</code> <p>Customized model weights to load. Default is None, which means using the built-in weights in <code>model</code> choice.</p> <code>None</code> <code>conf</code> <code>float</code> <p>Confidence threshold. Default is 0.25.</p> <code>0.25</code> <code>nms</code> <code>float</code> <p>IoU/NMS threshold. Default is 0.7.</p> <code>0.7</code> <code>max_det</code> <code>int</code> <p>Maximum detections per frame. Default is 300. In crowded scenes, you may want to increase this.</p> <code>300</code> <code>device</code> <code>(auto, cuda, xpu, cpu, mps)</code> <p>Inference device. Default is \"auto\".</p> <code>\"auto\"</code> <code>half</code> <code>bool</code> <p>Whether to use half precision (GPU only). Default is False.</p> <code>False</code> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def __init__(\n    self,\n    model: DetectorModel = DetectorModel.YOLO26x,\n    weights: str | None = None,\n    conf: float = 0.25,\n    nms: float = 0.7,\n    max_det: int = 300,\n    device: str = \"auto\",\n    half: bool = False,\n):\n    \"\"\"Initialize a Detector for Ultralytics YOLO/RT-DETR models.\n\n    Parameters\n    ----------\n    model : DetectorModel, optional\n        Built-in model to use. Default is \"yolo26x\".\n    weights : str, optional\n        Customized model weights to load.\n        Default is None, which means using the built-in weights in `model` choice.\n    conf : float, optional\n        Confidence threshold. Default is 0.25.\n    nms : float, optional\n        IoU/NMS threshold. Default is 0.7.\n    max_det : int, optional\n        Maximum detections per frame.\n        Default is 300. In crowded scenes, you may want to increase this.\n    device : {\"auto\", \"cuda\", \"xpu\", \"cpu\", \"mps\"}, optional\n        Inference device. Default is \"auto\".\n    half : bool, optional\n        Whether to use half precision (GPU only). Default is False.\n\n    \"\"\"\n    # Load model\n    cwd = Path(__file__).parent.absolute()\n    model_dir = cwd / \"models\"\n    if not model_dir.exists():\n        os.makedirs(model_dir)\n\n    if weights:\n        model_path = Path(weights) if os.path.isabs(weights) else model_dir / weights\n    else:\n        model_path = model_dir / f\"{model.value}\"\n\n    # actually load model\n    if (\"yolo\" in str(weights).lower()) or (model in YOLO_MODELS):\n        self.model = YOLO(str(model_path))\n    elif (\"rtdetr\" in str(weights).lower()) or (model in RTDETR_MODELS):\n        self.model = RTDETR(str(model_path))\n    else:\n        raise ValueError(\n            f\"Cannot infer model family from model={model} and weights={weights!r}. \"\n            \"Use a known DetectorModel or provide weights containing 'yolo' or 'rtdetr'.\"\n        )\n    self.conf = conf\n    self.nms = nms\n    self.max_det = max_det\n\n    # device selection\n    requested_device = str(device).lower().strip()\n    requested_backend = requested_device.split(\":\", maxsplit=1)[0]\n    valid_devices = {\"auto\", \"cuda\", \"xpu\", \"mps\", \"cpu\"}\n    if requested_backend not in valid_devices:\n        raise ValueError(\n            f\"Invalid device={device!r}. Choose one of {sorted(valid_devices)} or backend:index like 'cuda:0'.\"\n        )\n\n    backend_available = {\n        \"cuda\": torch.cuda.is_available(),\n        \"xpu\": hasattr(torch, \"xpu\") and hasattr(torch.xpu, \"is_available\") and torch.xpu.is_available(),\n        \"mps\": hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available(),\n        \"cpu\": True,\n    }\n\n    if requested_backend == \"auto\":\n        auto_priority = (\"cuda\", \"xpu\", \"mps\", \"cpu\")\n        self.device = next(d for d in auto_priority if backend_available[d])\n    else:\n        self.device = requested_device if backend_available[requested_backend] else \"cpu\"\n\n    # half precision only makes sense on GPU\n    self.half = half and (self.device == \"cuda\")\n</code></pre>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector.detect","title":"detect","text":"<pre><code>detect(\n    input_video: str,\n    iou_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    verbose: bool = True,\n    show: bool = False,\n    disp_filename: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run object detection on a video and return per-frame detections.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>iou_file</code> <code>str</code> <p>If provided, detection results are written to this file (CSV without header).</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Index of this video in a batch, used only for progress display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total number of videos in the batch, used only for progress display.</p> <code>None</code> <code>start_frame</code> <code>int</code> <p>Frame index to start detection from. If None or out of range, starts at 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Frame index to stop detection at. If None or out of range, uses the last frame.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Whether to display the video frames with detections. Default is False.</p> <code>False</code> <code>disp_filename</code> <code>bool</code> <p>Whether to show the file name in the progress bar. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: <code>frame, res, x, y, w, h, conf, class</code>. If the video cannot be opened or no detections are found, an empty DataFrame with those columns is returned.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect(\n    self,\n    input_video: str,\n    iou_file: str | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    verbose: bool = True,\n    show: bool = False,\n    disp_filename: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run object detection on a video and return per-frame detections.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to the input video file.\n    iou_file : str, optional\n        If provided, detection results are written to this file (CSV without header).\n    video_index : int, optional\n        Index of this video in a batch, used only for progress display.\n    video_tot : int, optional\n        Total number of videos in the batch, used only for progress display.\n    start_frame : int, optional\n        Frame index to start detection from. If None or out of range, starts at 0.\n    end_frame : int, optional\n        Frame index to stop detection at. If None or out of range, uses the last frame.\n    verbose : bool, optional\n        Whether to show a progress bar. Default is True.\n    show : bool, optional\n        Whether to display the video frames with detections. Default is False.\n    disp_filename: bool, optional\n        Whether to show the file name in the progress bar. Default is False.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with columns:\n        `frame, res, x, y, w, h, conf, class`.\n        If the video cannot be opened or no detections are found, an empty DataFrame\n        with those columns is returned.\n\n    \"\"\"\n    # validate path\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        if verbose:\n            print(f\"Cannot open video: {input_video}\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # normalize start_frame\n    if start_frame is None or start_frame &lt; 0 or start_frame &gt;= tot_frames:\n        start_frame = 0\n    # normalize end_frame\n    if end_frame is None or end_frame &lt; 0 or end_frame &gt;= tot_frames:\n        end_frame = tot_frames - 1\n    if start_frame &gt; end_frame:\n        cap.release()\n        raise ValueError(\"start_frame must be less than or equal to end_frame.\")\n\n    frame_total = end_frame - start_frame + 1\n\n    # Some codecs return 0 or -1 for frame count\n    if verbose:\n        if tot_frames &lt;= 0:\n            pbar = tqdm(desc=\"Detecting\", unit=\"frame\")\n        else:\n            pbar = tqdm(total=frame_total, desc=\"Detecting\", unit=\"frame\")\n\n        if (video_index is not None) and (video_tot is not None):\n            desc = f\"Detecting {video_index} of {video_tot}\"\n            if disp_filename:\n                desc += f\" - {Path(input_video).name}\"\n                pbar.set_description_str(desc)\n        else:\n            desc = \"Detecting\"\n            if disp_filename:\n                desc += f\" {Path(input_video).name}\"\n                pbar.set_description_str(desc)\n\n    results: list[dict] = []\n    frame_idx = start_frame\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n\n    win_name = \"Detection (press q/ESC to quit)\"\n    if show:\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n\n    # optional FPS calc\n    t0 = time()\n    n_show = 0\n\n    while cap.isOpened():\n        pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if end_frame is not None and frame_idx &gt; end_frame:\n            break\n\n        preds = self.model.predict(\n            source=frame,\n            conf=self.conf,\n            iou=self.nms,\n            max_det=self.max_det,\n            device=self.device,\n            half=self.half,\n            verbose=False,\n        )\n\n        det = preds[0]\n        boxes = det.boxes\n        if boxes is not None and len(boxes) &gt; 0:\n            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n            confs = boxes.conf.cpu().numpy()  # (N,)\n            clss = boxes.cls.cpu().numpy().astype(int)  # (N,)\n\n            for (x1, y1, x2, y2), cf, c in zip(xyxy, confs, clss, strict=True):\n                results.append({\n                    \"frame\": pos_frame,\n                    \"res\": -1,\n                    \"x\": float(x1),\n                    \"y\": float(y1),\n                    \"x2\": float(x2),\n                    \"y2\": float(y2),\n                    \"conf\": float(cf),\n                    \"class\": int(c),\n                })\n\n        if show:\n            # Ultralytics built-in drawing (fast &amp; clean)\n            vis = det.plot()  # returns BGR image with boxes/labels\n\n            # add simple overlay: frame index + FPS\n            n_show += 1\n            dt = time() - t0\n            fps = n_show / dt if dt &gt; 0 else 0.0\n            cv2.putText(\n                vis,\n                f\"frame={pos_frame}/{frame_total}  fps={fps:.1f}\",\n                (10, 25),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.7,\n                (0, 255, 255),\n                2,\n            )\n\n            cv2.imshow(win_name, vis)\n            key = cv2.waitKey(1) &amp; 0xFF\n            if key == ord(\"q\") or key == 27:  # q or ESC\n                break\n\n        if verbose and pbar is not None:\n            pbar.update(1)\n\n        frame_idx += 1\n\n    cap.release()\n    if verbose and pbar is not None:\n        pbar.close()\n\n    if not results:\n        empty_df = pd.DataFrame(columns=self.DET_FIELDS)\n        if iou_file:\n            empty_df.to_csv(iou_file, index=False, header=False)\n        return empty_df\n\n    else:\n        results_df = pd.DataFrame(results, columns=[\"frame\", \"res\", \"x\", \"y\", \"x2\", \"y2\", \"conf\", \"class\"])\n        results_df[\"w\"] = (results_df[\"x2\"] - results_df[\"x\"]).astype(int)\n        results_df[\"h\"] = (results_df[\"y2\"] - results_df[\"y\"]).astype(int)\n        results_df[\"x\"] = results_df[\"x\"].astype(int)\n        results_df[\"y\"] = results_df[\"y\"].astype(int)\n        results_df[\"conf\"] = results_df[\"conf\"].round(2)\n        results_df = results_df[self.DET_FIELDS].reset_index(drop=True)\n\n    if iou_file:\n        folder = Path(iou_file).parent\n        if not folder.exists():\n            Path(folder).mkdir(parents=True, exist_ok=True)\n\n        results_df.to_csv(iou_file, index=False, header=False)\n        if verbose:\n            print(f\"Wrote detections to {iou_file}\")\n\n    return results_df\n</code></pre>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector.detect_frames","title":"detect_frames","text":"<pre><code>detect_frames(\n    input_video: str,\n    frames: list[int],\n    verbose: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run object detection on specific frames of a video.</p> <p>This method is useful when you don't need to process the entire video and only want detections for selected frame indices.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to the input video file.</p> required <code>frames</code> <code>list of int</code> <p>List of frame indices to process.</p> required <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>frame, res, x, y, w, h, conf, class</code>. If the video cannot be opened or no detections are found, an empty DataFrame with those columns is returned.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect_frames(\n    self,\n    input_video: str,\n    frames: list[int],\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Run object detection on specific frames of a video.\n\n    This method is useful when you don't need to process the entire video and\n    only want detections for selected frame indices.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to the input video file.\n    frames : list of int\n        List of frame indices to process.\n    verbose : bool, optional\n        Whether to show a progress bar. Default is True.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with columns\n        `frame, res, x, y, w, h, conf, class`.\n        If the video cannot be opened or no detections are found, an empty\n        DataFrame with those columns is returned.\n\n    \"\"\"\n    # validate path\n    if not os.path.exists(input_video):\n        # return an empty, well-shaped DataFrame instead of None\n        if verbose:\n            print(f\"{input_video} does not exist!\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        if verbose:\n            print(f\"Cannot open {input_video}\")\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    results: list[dict] = []\n\n    pbar = tqdm(total=len(frames), unit=\" frames\") if verbose else None\n\n    for pos_frame in frames:\n        # move to target frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame)\n        ret, frame = cap.read()\n        if not ret:\n            # e.g. frame index out of range\n            continue\n\n        preds = self.model.predict(\n            frame,\n            verbose=False,\n            conf=self.conf,\n            iou=self.nms,\n            max_det=self.max_det,\n            device=self.device,\n            half=self.half,\n        )\n        det = preds[0]\n        boxes = det.boxes\n        if boxes is not None and len(boxes) &gt; 0:\n            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n            confs = boxes.conf.cpu().numpy()  # (N,)\n            clss = boxes.cls.cpu().numpy().astype(int)  # (N,)\n\n            for (x1, y1, x2, y2), cf, c in zip(xyxy, confs, clss, strict=True):\n                results.append({\n                    \"frame\": pos_frame,\n                    \"res\": -1,\n                    \"x\": float(x1),\n                    \"y\": float(y1),\n                    \"x2\": float(x2),\n                    \"y2\": float(y2),\n                    \"conf\": float(cf),\n                    \"class\": int(c),\n                })\n\n        if pbar is not None:\n            pbar.update()\n\n    if pbar is not None:\n        pbar.close()\n    cap.release()\n\n    # no detections at all\n\n    if not results:\n        return pd.DataFrame(columns=self.DET_FIELDS)\n\n    df = pd.DataFrame(results)\n    # compute width/height and round\n    df[\"w\"] = (df[\"x2\"] - df[\"x\"]).round(0)\n    df[\"h\"] = (df[\"y2\"] - df[\"y\"]).round(0)\n    df[\"x\"] = df[\"x\"].round(1)\n    df[\"y\"] = df[\"y\"].round(1)\n    df[\"conf\"] = df[\"conf\"].round(2)\n    df[\"class\"] = df[\"class\"].round(0).astype(int)\n\n    df = df[self.DET_FIELDS].reset_index(drop=True)\n\n    return df\n</code></pre>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector.detect_batch","title":"detect_batch","text":"<pre><code>detect_batch(\n    input_videos: list[str],\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n    verbose: bool = True,\n    disp_filename: bool = True,\n) -&gt; list[str]\n</code></pre> <p>Run detection on multiple videos and optionally write per-video output files.</p> <p>Parameters:</p> Name Type Description Default <code>input_videos</code> <code>list of str</code> <p>Paths to the input video files to be processed.</p> required <code>output_path</code> <code>str</code> <p>Directory where per-video detection files will be written. If None, detections are not written to disk and the returned list will be empty.</p> <code>None</code> <code>is_overwrite</code> <code>bool</code> <p>If False (default), existing detection files with the same name will be skipped. If True, they will be regenerated.</p> <code>False</code> <code>is_report</code> <code>bool</code> <p>If True (default), existing detection files (that were skipped) are still included in the returned list.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints progress messages. Default is True.</p> <code>True</code> <code>disp_filename</code> <code>bool</code> <p>If True, prints file name in progress bar. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of paths to detection files that were created or already existed. If <code>output_path</code> is None, this will be an empty list.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>def detect_batch(\n    self,\n    input_videos: list[str],\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n    verbose: bool = True,\n    disp_filename: bool = True,\n) -&gt; list[str]:\n    \"\"\"Run detection on multiple videos and optionally write per-video output files.\n\n    Parameters\n    ----------\n    input_videos : list of str\n        Paths to the input video files to be processed.\n    output_path : str, optional\n        Directory where per-video detection files will be written. If None,\n        detections are not written to disk and the returned list will be empty.\n    is_overwrite : bool, optional\n        If False (default), existing detection files with the same name will be\n        skipped. If True, they will be regenerated.\n    is_report : bool, optional\n        If True (default), existing detection files (that were skipped) are still\n        included in the returned list.\n    verbose : bool, optional\n        If True, prints progress messages. Default is True.\n    disp_filename: bool, optional\n        If True, prints file name in progress bar. Default is True.\n\n    Returns\n    -------\n    list of str\n        A list of paths to detection files that were created or already existed.\n        If `output_path` is None, this will be an empty list.\n\n    \"\"\"\n    results: list[str] = []\n    total_videos = len(input_videos)\n\n    for idx, input_video in enumerate(input_videos, start=1):\n        # default: no output file\n        iou_file = None\n\n        # build output path / file name if requested\n        if output_path is not None:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            base_filename = os.path.splitext(os.path.basename(input_video))[0]\n            iou_file = os.path.join(output_path, f\"{base_filename}_iou.txt\")\n\n        # if we have an output file name, check overwrite logic\n        if (iou_file is not None) and (not is_overwrite) and os.path.exists(iou_file):\n            if is_report:\n                results.append(iou_file)\n            # skip processing this video\n            continue\n\n        # run detection (may write to iou_file if not None)\n        self.detect(\n            input_video=input_video,\n            iou_file=iou_file,\n            video_index=idx,\n            video_tot=total_videos,\n            verbose=verbose,\n            disp_filename=disp_filename,\n        )\n\n        if iou_file is not None:\n            results.append(iou_file)\n\n    return results\n</code></pre>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector.get_fps","title":"get_fps  <code>staticmethod</code>","text":"<pre><code>get_fps(video: str) -&gt; float\n</code></pre> <p>Return the frames-per-second (FPS) value of a video file.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>float</code> <p>FPS of the video. Returns 0.0 if the video cannot be opened.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>@staticmethod\ndef get_fps(video: str) -&gt; float:\n    \"\"\"Return the frames-per-second (FPS) value of a video file.\n\n    Parameters\n    ----------\n    video : str\n        Path to the video file.\n\n    Returns\n    -------\n    float\n        FPS of the video. Returns 0.0 if the video cannot be opened.\n\n    \"\"\"\n    if not Path(video).exists():\n        print(f\"{video} does not exist!\")\n        return 0.0\n    cap = cv2.VideoCapture(video)\n    if not cap.isOpened():\n        print(f\"Failed to open the video: {video}\")\n        return 0.0\n\n    fps = float(cap.get(cv2.CAP_PROP_FPS))\n    cap.release()\n    return fps\n</code></pre>"},{"location":"api/detect/yolov8/#dnt.detect.yolo.detector.Detector.get_frames","title":"get_frames  <code>staticmethod</code>","text":"<pre><code>get_frames(video: str) -&gt; int\n</code></pre> <p>Return the total number of frames in a video file.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total frame count. Returns 0 if the video cannot be opened.</p> Source code in <code>src/dnt/detect/yolo/detector.py</code> <pre><code>@staticmethod\ndef get_frames(video: str) -&gt; int:\n    \"\"\"Return the total number of frames in a video file.\n\n    Parameters\n    ----------\n    video : str\n        Path to the video file.\n\n    Returns\n    -------\n    int\n        Total frame count. Returns 0 if the video cannot be opened.\n\n    \"\"\"\n    if not Path(video).exists():\n        print(f\"{video} does not exist!\")\n        return 0\n    cap = cv2.VideoCapture(video)\n    if not cap.isOpened():\n        print(f\"Failed to open the video: {video}\")\n        return 0\n\n    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return frames\n</code></pre>"},{"location":"api/engine/","title":"Engine","text":"<p>Engine module for detection tracking.</p> <p>This module provides utilities for bounding box interpolation, IoU calculations, clustering, and IoB (Intersection over Background) operations.</p>"},{"location":"api/engine/#dnt.engine.interpolate_bbox","title":"interpolate_bbox","text":"<pre><code>interpolate_bbox(\n    boxes: ndarray,\n    frames: ndarray,\n    target_frame: int,\n    method=\"cubic\",\n) -&gt; np.ndarray\n</code></pre> <p>Interpolates bounding boxes using cubic splines.</p> <p>Args:     boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).     frames: a (n,) array of frame indices corresponding to the bounding boxes.     target_frame: the frame index at which to interpolate the bounding box.     method: the spline function, default is 'cubic', 'nearest', 'linear'</p> <p>Returns:     A 1d array (x, y, width, height) representing the interpolated bounding box.</p> Source code in <code>src/dnt/engine/bbox_interp.py</code> <pre><code>def interpolate_bbox(boxes: np.ndarray, frames: np.ndarray, target_frame: int, method=\"cubic\") -&gt; np.ndarray:\n    \"\"\"Interpolates bounding boxes using cubic splines.\n\n    Args:\n        boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).\n        frames: a (n,) array of frame indices corresponding to the bounding boxes.\n        target_frame: the frame index at which to interpolate the bounding box.\n        method: the spline function, default is 'cubic', 'nearest', 'linear'\n\n    Returns:\n        A 1d array (x, y, width, height) representing the interpolated bounding box.\n\n    \"\"\"\n    n = boxes.shape[0]\n\n    if n != frames.shape[0]:\n        raise ValueError(\"Length of boxes and frames must be equal.\")\n\n    if n == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    if target_frame &lt; frames[0] or target_frame &gt; frames[-1]:\n        raise ValueError(\"Target frame is out of bounds.\")\n\n    # Unpack the boxes into separate arrays for x, y, width, and height\n    x = boxes[:, 0]\n    y = boxes[:, 1]\n    w = boxes[:, 2]\n    h = boxes[:, 3]\n\n    # Create the cubic splines for each parameter\n    if method == \"cubic\":\n        spline_x = CubicSpline(frames, x)\n        spline_y = CubicSpline(frames, y)\n        spline_w = CubicSpline(frames, w)\n        spline_h = CubicSpline(frames, h)\n    elif method == \"nearest\":\n        spline_x = interp1d(frames, x, kind=\"nearest\")\n        spline_y = interp1d(frames, y, kind=\"nearest\")\n        spline_w = interp1d(frames, w, kind=\"nearest\")\n        spline_h = interp1d(frames, h, kind=\"nearest\")\n    else:\n        spline_x = interp1d(frames, x, kind=\"linear\")\n        spline_y = interp1d(frames, y, kind=\"linear\")\n        spline_w = interp1d(frames, w, kind=\"linear\")\n        spline_h = interp1d(frames, h, kind=\"linear\")\n\n    # Evaluate the splines at the target frame\n    x_t = int(spline_x(target_frame))\n    y_t = int(spline_y(target_frame))\n    w_t = int(spline_w(target_frame))\n    h_t = int(spline_h(target_frame))\n\n    return np.array([x_t, y_t, w_t, h_t])\n</code></pre>"},{"location":"api/engine/#dnt.engine.interpolate_bboxes","title":"interpolate_bboxes","text":"<pre><code>interpolate_bboxes(\n    boxes: ndarray,\n    frames: ndarray,\n    target_frames: ndarray,\n    method=\"cubic\",\n) -&gt; np.ndarray\n</code></pre> <p>Interpolates bounding boxes using cubic splines.</p> <p>Args:     boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).     frames: a (n,) array of frame indices corresponding to the bounding boxes.     target_frames: the frame indexes at which to interpolate the bounding boxes.     method: the spline function, default is 'cubic', 'nearest', 'linear'</p> <p>Returns:     A (m, 4) array of interpolated bounding boxes for the target frames.</p> Source code in <code>src/dnt/engine/bbox_interp.py</code> <pre><code>def interpolate_bboxes(boxes: np.ndarray, frames: np.ndarray, target_frames: np.ndarray, method=\"cubic\") -&gt; np.ndarray:\n    \"\"\"Interpolates bounding boxes using cubic splines.\n\n    Args:\n        boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).\n        frames: a (n,) array of frame indices corresponding to the bounding boxes.\n        target_frames: the frame indexes at which to interpolate the bounding boxes.\n        method: the spline function, default is 'cubic', 'nearest', 'linear'\n\n    Returns:\n        A (m, 4) array of interpolated bounding boxes for the target frames.\n\n    \"\"\"\n    n_frames = target_frames.shape[0]\n    results = []\n\n    for i in range(n_frames):\n        target_frame = target_frames[i]\n        results.append(interpolate_bbox(boxes, frames, target_frame, method))\n\n    return np.vstack(results)\n</code></pre>"},{"location":"api/engine/#dnt.engine.ious","title":"ious","text":"<pre><code>ious(atlbrs, btlbrs)\n</code></pre> <p>Compute cost based on IoU.</p> <p>:type atlbrs: list[tlbr] | np.ndarray :type atlbrs: list[tlbr] | np.ndarray</p> <p>:rtype ious np.ndarray</p> Source code in <code>src/dnt/engine/bbox_iou.py</code> <pre><code>def ious(atlbrs, btlbrs):\n    \"\"\"Compute cost based on IoU.\n\n    :type atlbrs: list[tlbr] | np.ndarray\n    :type atlbrs: list[tlbr] | np.ndarray\n\n    :rtype ious np.ndarray\n    \"\"\"\n    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float64)\n    if ious.size == 0:\n        return ious\n\n    ious = bbox_overlaps(np.ascontiguousarray(atlbrs, dtype=np.float64), np.ascontiguousarray(btlbrs, dtype=np.float64))\n\n    return ious\n</code></pre>"},{"location":"api/engine/#dnt.engine.cluster_by_gap","title":"cluster_by_gap","text":"<pre><code>cluster_by_gap(arr: ndarray, gap: int) -&gt; list[list]\n</code></pre> <p>Group array elements into clusters based on a gap threshold.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>A 1D numpy array of numbers to be clustered.</p> required <code>gap</code> <code>int</code> <p>The maximum allowed difference between consecutive elements in a cluster. Must be non-negative.</p> required <p>Returns:</p> Type Description <code>list[list]</code> <p>A list of clusters, where each cluster is a list of numbers from the input array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array is empty, not 1-dimensional, or gap is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=2)\n[[1, 2], [5, 6], [10]]\n&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=1)\n[[1, 2], [5, 6], [10]]\n&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=4)\n[[1, 2, 5, 6, 10]]\n</code></pre> Source code in <code>src/dnt/engine/cluster.py</code> <pre><code>def cluster_by_gap(arr: np.ndarray, gap: int) -&gt; list[list]:\n    \"\"\"Group array elements into clusters based on a gap threshold.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        A 1D numpy array of numbers to be clustered.\n    gap : int\n        The maximum allowed difference between consecutive elements in a cluster.\n        Must be non-negative.\n\n    Returns\n    -------\n    list[list]\n        A list of clusters, where each cluster is a list of numbers from the input array.\n\n    Raises\n    ------\n    ValueError\n        If the input array is empty, not 1-dimensional, or gap is negative.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=2)\n    [[1, 2], [5, 6], [10]]\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=1)\n    [[1, 2], [5, 6], [10]]\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=4)\n    [[1, 2, 5, 6, 10]]\n\n    \"\"\"\n    if arr.size == 0:\n        raise ValueError(\"Input array cannot be empty.\")\n    if arr.ndim != 1:\n        raise ValueError(\"Input array must be 1-dimensional.\")\n    if gap &lt; 0:\n        raise ValueError(\"Gap threshold must be non-negative.\")\n\n    arr = np.sort(arr)\n    clusters = []\n    current_cluster = [arr[0]]\n\n    for i in range(1, len(arr)):\n        if arr[i] - arr[i - 1] &lt;= gap:\n            current_cluster.append(arr[i])\n        else:\n            clusters.append(current_cluster)\n            current_cluster = [arr[i]]\n\n    clusters.append(current_cluster)\n    return clusters\n</code></pre>"},{"location":"api/engine/#dnt.engine.iobs","title":"iobs","text":"<pre><code>iobs(\n    alrbs: ndarray, blrbs: ndarray\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Calculate the IoB matrix for multiple bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>alrbs</code> <code>ndarray</code> <p>Array of shape (N, 4) containing N bounding boxes with format [left, top, width, height].</p> required <code>blrbs</code> <code>ndarray</code> <p>Array of shape (M, 4) containing M bounding boxes with format [left, top, width, height].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple (iobs_a, iobs_b) where: - iobs_a: array of shape (N, M) with IoB values relative to alrbs - iobs_b: array of shape (N, M) with IoB values relative to blrbs</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If arrays do not have shape (N, 4) and (M, 4) respectively, or contain negative width/height values.</p> Notes <p>Bounding box format is [left, top, width, height] where: - left: x-coordinate of top-left corner - top: y-coordinate of top-left corner - width: box width (must be non-negative) - height: box height (must be non-negative)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; boxes1 = np.array([[0, 0, 10, 10], [5, 5, 10, 10]])\n&gt;&gt;&gt; boxes2 = np.array([[0, 0, 5, 5], [10, 10, 5, 5]])\n&gt;&gt;&gt; iobs_a, iobs_b = iobs(boxes1, boxes2)\n&gt;&gt;&gt; iobs_a.shape\n(2, 2)\n</code></pre> Source code in <code>src/dnt/engine/iob.py</code> <pre><code>def iobs(alrbs: np.ndarray, blrbs: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate the IoB matrix for multiple bounding boxes.\n\n    Parameters\n    ----------\n    alrbs : np.ndarray\n        Array of shape (N, 4) containing N bounding boxes with format\n        [left, top, width, height].\n    blrbs : np.ndarray\n        Array of shape (M, 4) containing M bounding boxes with format\n        [left, top, width, height].\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple (iobs_a, iobs_b) where:\n        - iobs_a: array of shape (N, M) with IoB values relative to alrbs\n        - iobs_b: array of shape (N, M) with IoB values relative to blrbs\n\n    Raises\n    ------\n    ValueError\n        If arrays do not have shape (N, 4) and (M, 4) respectively, or contain\n        negative width/height values.\n\n    Notes\n    -----\n    Bounding box format is [left, top, width, height] where:\n    - left: x-coordinate of top-left corner\n    - top: y-coordinate of top-left corner\n    - width: box width (must be non-negative)\n    - height: box height (must be non-negative)\n\n    Examples\n    --------\n    &gt;&gt;&gt; boxes1 = np.array([[0, 0, 10, 10], [5, 5, 10, 10]])\n    &gt;&gt;&gt; boxes2 = np.array([[0, 0, 5, 5], [10, 10, 5, 5]])\n    &gt;&gt;&gt; iobs_a, iobs_b = iobs(boxes1, boxes2)\n    &gt;&gt;&gt; iobs_a.shape\n    (2, 2)\n\n    \"\"\"\n    if alrbs.ndim != 2 or alrbs.shape[1] != 4:\n        raise ValueError(\"alrbs must have shape (N, 4) with format [left, top, width, height].\")\n    if blrbs.ndim != 2 or blrbs.shape[1] != 4:\n        raise ValueError(\"blrbs must have shape (M, 4) with format [left, top, width, height].\")\n    if np.any(alrbs[:, 2:] &lt; 0) or np.any(blrbs[:, 2:] &lt; 0):\n        raise ValueError(\"Box width and height must be non-negative.\")\n\n    num_a = alrbs.shape[0]\n    num_b = blrbs.shape[0]\n    iobs_a = np.zeros((num_a, num_b))\n    iobs_b = np.zeros((num_a, num_b))\n\n    for i in range(num_a):\n        for j in range(num_b):\n            iobs_a[i, j], iobs_b[i, j] = iob(alrbs[i, :], blrbs[j, :])\n\n    return iobs_a, iobs_b\n</code></pre>"},{"location":"api/engine/bbox_interp/","title":"Bounding Box Interpolation","text":"<p>Bounding box interpolation module for object tracking.</p>"},{"location":"api/engine/bbox_interp/#dnt.engine.bbox_interp.interpolate_bbox","title":"interpolate_bbox","text":"<pre><code>interpolate_bbox(\n    boxes: ndarray,\n    frames: ndarray,\n    target_frame: int,\n    method=\"cubic\",\n) -&gt; np.ndarray\n</code></pre> <p>Interpolates bounding boxes using cubic splines.</p> <p>Args:     boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).     frames: a (n,) array of frame indices corresponding to the bounding boxes.     target_frame: the frame index at which to interpolate the bounding box.     method: the spline function, default is 'cubic', 'nearest', 'linear'</p> <p>Returns:     A 1d array (x, y, width, height) representing the interpolated bounding box.</p> Source code in <code>src/dnt/engine/bbox_interp.py</code> <pre><code>def interpolate_bbox(boxes: np.ndarray, frames: np.ndarray, target_frame: int, method=\"cubic\") -&gt; np.ndarray:\n    \"\"\"Interpolates bounding boxes using cubic splines.\n\n    Args:\n        boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).\n        frames: a (n,) array of frame indices corresponding to the bounding boxes.\n        target_frame: the frame index at which to interpolate the bounding box.\n        method: the spline function, default is 'cubic', 'nearest', 'linear'\n\n    Returns:\n        A 1d array (x, y, width, height) representing the interpolated bounding box.\n\n    \"\"\"\n    n = boxes.shape[0]\n\n    if n != frames.shape[0]:\n        raise ValueError(\"Length of boxes and frames must be equal.\")\n\n    if n == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    if target_frame &lt; frames[0] or target_frame &gt; frames[-1]:\n        raise ValueError(\"Target frame is out of bounds.\")\n\n    # Unpack the boxes into separate arrays for x, y, width, and height\n    x = boxes[:, 0]\n    y = boxes[:, 1]\n    w = boxes[:, 2]\n    h = boxes[:, 3]\n\n    # Create the cubic splines for each parameter\n    if method == \"cubic\":\n        spline_x = CubicSpline(frames, x)\n        spline_y = CubicSpline(frames, y)\n        spline_w = CubicSpline(frames, w)\n        spline_h = CubicSpline(frames, h)\n    elif method == \"nearest\":\n        spline_x = interp1d(frames, x, kind=\"nearest\")\n        spline_y = interp1d(frames, y, kind=\"nearest\")\n        spline_w = interp1d(frames, w, kind=\"nearest\")\n        spline_h = interp1d(frames, h, kind=\"nearest\")\n    else:\n        spline_x = interp1d(frames, x, kind=\"linear\")\n        spline_y = interp1d(frames, y, kind=\"linear\")\n        spline_w = interp1d(frames, w, kind=\"linear\")\n        spline_h = interp1d(frames, h, kind=\"linear\")\n\n    # Evaluate the splines at the target frame\n    x_t = int(spline_x(target_frame))\n    y_t = int(spline_y(target_frame))\n    w_t = int(spline_w(target_frame))\n    h_t = int(spline_h(target_frame))\n\n    return np.array([x_t, y_t, w_t, h_t])\n</code></pre>"},{"location":"api/engine/bbox_interp/#dnt.engine.bbox_interp.interpolate_bboxes","title":"interpolate_bboxes","text":"<pre><code>interpolate_bboxes(\n    boxes: ndarray,\n    frames: ndarray,\n    target_frames: ndarray,\n    method=\"cubic\",\n) -&gt; np.ndarray\n</code></pre> <p>Interpolates bounding boxes using cubic splines.</p> <p>Args:     boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).     frames: a (n,) array of frame indices corresponding to the bounding boxes.     target_frames: the frame indexes at which to interpolate the bounding boxes.     method: the spline function, default is 'cubic', 'nearest', 'linear'</p> <p>Returns:     A (m, 4) array of interpolated bounding boxes for the target frames.</p> Source code in <code>src/dnt/engine/bbox_interp.py</code> <pre><code>def interpolate_bboxes(boxes: np.ndarray, frames: np.ndarray, target_frames: np.ndarray, method=\"cubic\") -&gt; np.ndarray:\n    \"\"\"Interpolates bounding boxes using cubic splines.\n\n    Args:\n        boxes: a (n, 4) array of bounding box coordinates (x, y, width, height).\n        frames: a (n,) array of frame indices corresponding to the bounding boxes.\n        target_frames: the frame indexes at which to interpolate the bounding boxes.\n        method: the spline function, default is 'cubic', 'nearest', 'linear'\n\n    Returns:\n        A (m, 4) array of interpolated bounding boxes for the target frames.\n\n    \"\"\"\n    n_frames = target_frames.shape[0]\n    results = []\n\n    for i in range(n_frames):\n        target_frame = target_frames[i]\n        results.append(interpolate_bbox(boxes, frames, target_frame, method))\n\n    return np.vstack(results)\n</code></pre>"},{"location":"api/engine/bbox_iou/","title":"Bounding Box IoU","text":"<p>Compute IoU (Intersection over Union) between bounding boxes.</p> <p>This module provides functions to calculate the IoU metric for bounding boxes.</p>"},{"location":"api/engine/bbox_iou/#dnt.engine.bbox_iou.ious","title":"ious","text":"<pre><code>ious(atlbrs, btlbrs)\n</code></pre> <p>Compute cost based on IoU.</p> <p>:type atlbrs: list[tlbr] | np.ndarray :type atlbrs: list[tlbr] | np.ndarray</p> <p>:rtype ious np.ndarray</p> Source code in <code>src/dnt/engine/bbox_iou.py</code> <pre><code>def ious(atlbrs, btlbrs):\n    \"\"\"Compute cost based on IoU.\n\n    :type atlbrs: list[tlbr] | np.ndarray\n    :type atlbrs: list[tlbr] | np.ndarray\n\n    :rtype ious np.ndarray\n    \"\"\"\n    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float64)\n    if ious.size == 0:\n        return ious\n\n    ious = bbox_overlaps(np.ascontiguousarray(atlbrs, dtype=np.float64), np.ascontiguousarray(btlbrs, dtype=np.float64))\n\n    return ious\n</code></pre>"},{"location":"api/engine/cluster/","title":"Clustering","text":"<p>Clustering utilities for grouping array elements by gap threshold.</p> <p>This module provides:     - cluster_by_gap: Groups array elements into clusters based on a gap threshold.</p>"},{"location":"api/engine/cluster/#dnt.engine.cluster.cluster_by_gap","title":"cluster_by_gap","text":"<pre><code>cluster_by_gap(arr: ndarray, gap: int) -&gt; list[list]\n</code></pre> <p>Group array elements into clusters based on a gap threshold.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>A 1D numpy array of numbers to be clustered.</p> required <code>gap</code> <code>int</code> <p>The maximum allowed difference between consecutive elements in a cluster. Must be non-negative.</p> required <p>Returns:</p> Type Description <code>list[list]</code> <p>A list of clusters, where each cluster is a list of numbers from the input array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array is empty, not 1-dimensional, or gap is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=2)\n[[1, 2], [5, 6], [10]]\n&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=1)\n[[1, 2], [5, 6], [10]]\n&gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=4)\n[[1, 2, 5, 6, 10]]\n</code></pre> Source code in <code>src/dnt/engine/cluster.py</code> <pre><code>def cluster_by_gap(arr: np.ndarray, gap: int) -&gt; list[list]:\n    \"\"\"Group array elements into clusters based on a gap threshold.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        A 1D numpy array of numbers to be clustered.\n    gap : int\n        The maximum allowed difference between consecutive elements in a cluster.\n        Must be non-negative.\n\n    Returns\n    -------\n    list[list]\n        A list of clusters, where each cluster is a list of numbers from the input array.\n\n    Raises\n    ------\n    ValueError\n        If the input array is empty, not 1-dimensional, or gap is negative.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=2)\n    [[1, 2], [5, 6], [10]]\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=1)\n    [[1, 2], [5, 6], [10]]\n    &gt;&gt;&gt; cluster_by_gap(np.array([1, 2, 5, 6, 10]), gap=4)\n    [[1, 2, 5, 6, 10]]\n\n    \"\"\"\n    if arr.size == 0:\n        raise ValueError(\"Input array cannot be empty.\")\n    if arr.ndim != 1:\n        raise ValueError(\"Input array must be 1-dimensional.\")\n    if gap &lt; 0:\n        raise ValueError(\"Gap threshold must be non-negative.\")\n\n    arr = np.sort(arr)\n    clusters = []\n    current_cluster = [arr[0]]\n\n    for i in range(1, len(arr)):\n        if arr[i] - arr[i - 1] &lt;= gap:\n            current_cluster.append(arr[i])\n        else:\n            clusters.append(current_cluster)\n            current_cluster = [arr[i]]\n\n    clusters.append(current_cluster)\n    return clusters\n</code></pre>"},{"location":"api/engine/iob/","title":"IoB","text":"<p>Module for calculating Intersection over Box (IoB) metrics.</p> <p>This module provides functions to compute IoB values between single and multiple bounding boxes, useful for evaluating object detection and tracking results.</p>"},{"location":"api/engine/iob/#dnt.engine.iob.iob","title":"iob","text":"<pre><code>iob(box_a: ndarray, box_b: ndarray) -&gt; tuple[float, float]\n</code></pre> <p>Calculate the Intersection over Box (IoB) between two bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>box_a</code> <code>ndarray</code> <p>Bounding box of shape (4,) with format [left, top, width, height].</p> required <code>box_b</code> <code>ndarray</code> <p>Bounding box of shape (4,) with format [left, top, width, height].</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>A tuple (iob_a, iob_b) where: - iob_a: ratio of intersection area to box_a area - iob_b: ratio of intersection area to box_b area</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If either box does not have shape (4,) or contains negative width/height.</p> Notes <p>Bounding box format is [left, top, width, height] where: - left: x-coordinate of top-left corner - top: y-coordinate of top-left corner - width: box width (must be non-negative) - height: box height (must be non-negative)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; box1 = np.array([0, 0, 10, 10])\n&gt;&gt;&gt; box2 = np.array([5, 5, 10, 10])\n&gt;&gt;&gt; iob(box1, box2)\n(0.25, 0.25)\n</code></pre> Source code in <code>src/dnt/engine/iob.py</code> <pre><code>def iob(box_a: np.ndarray, box_b: np.ndarray) -&gt; tuple[float, float]:\n    \"\"\"Calculate the Intersection over Box (IoB) between two bounding boxes.\n\n    Parameters\n    ----------\n    box_a : np.ndarray\n        Bounding box of shape (4,) with format [left, top, width, height].\n    box_b : np.ndarray\n        Bounding box of shape (4,) with format [left, top, width, height].\n\n    Returns\n    -------\n    tuple[float, float]\n        A tuple (iob_a, iob_b) where:\n        - iob_a: ratio of intersection area to box_a area\n        - iob_b: ratio of intersection area to box_b area\n\n    Raises\n    ------\n    ValueError\n        If either box does not have shape (4,) or contains negative width/height.\n\n    Notes\n    -----\n    Bounding box format is [left, top, width, height] where:\n    - left: x-coordinate of top-left corner\n    - top: y-coordinate of top-left corner\n    - width: box width (must be non-negative)\n    - height: box height (must be non-negative)\n\n    Examples\n    --------\n    &gt;&gt;&gt; box1 = np.array([0, 0, 10, 10])\n    &gt;&gt;&gt; box2 = np.array([5, 5, 10, 10])\n    &gt;&gt;&gt; iob(box1, box2)\n    (0.25, 0.25)\n\n    \"\"\"\n    if box_a.shape != (4,) or box_b.shape != (4,):\n        raise ValueError(\"Both boxes must have shape (4,) with format [left, top, width, height].\")\n    if box_a[2] &lt; 0 or box_a[3] &lt; 0 or box_b[2] &lt; 0 or box_b[3] &lt; 0:\n        raise ValueError(\"Box width and height must be non-negative.\")\n\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    x_l = max(box_a[0], box_b[0])\n    y_t = max(box_a[1], box_b[1])\n    x_r = min(box_a[0] + box_a[2], box_b[0] + box_b[2])\n    y_b = min(box_a[1] + box_a[3], box_b[1] + box_b[3])\n\n    # Compute the area of intersection rectangle\n    inter_width = max(0, x_r - x_l)\n    inter_height = max(0, y_b - y_t)\n    inter_area = inter_width * inter_height\n\n    # Compute the area of both bounding boxes\n    box_a_area = box_a[2] * box_a[3]\n    box_b_area = box_b[2] * box_b[3]\n\n    # Compute the intersection over box by dividing intersection area by each box's area\n    iob_a = inter_area / box_a_area if box_a_area != 0 else 0.0\n    iob_b = inter_area / box_b_area if box_b_area != 0 else 0.0\n\n    return iob_a, iob_b\n</code></pre>"},{"location":"api/engine/iob/#dnt.engine.iob.iobs","title":"iobs","text":"<pre><code>iobs(\n    alrbs: ndarray, blrbs: ndarray\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Calculate the IoB matrix for multiple bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>alrbs</code> <code>ndarray</code> <p>Array of shape (N, 4) containing N bounding boxes with format [left, top, width, height].</p> required <code>blrbs</code> <code>ndarray</code> <p>Array of shape (M, 4) containing M bounding boxes with format [left, top, width, height].</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple (iobs_a, iobs_b) where: - iobs_a: array of shape (N, M) with IoB values relative to alrbs - iobs_b: array of shape (N, M) with IoB values relative to blrbs</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If arrays do not have shape (N, 4) and (M, 4) respectively, or contain negative width/height values.</p> Notes <p>Bounding box format is [left, top, width, height] where: - left: x-coordinate of top-left corner - top: y-coordinate of top-left corner - width: box width (must be non-negative) - height: box height (must be non-negative)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; boxes1 = np.array([[0, 0, 10, 10], [5, 5, 10, 10]])\n&gt;&gt;&gt; boxes2 = np.array([[0, 0, 5, 5], [10, 10, 5, 5]])\n&gt;&gt;&gt; iobs_a, iobs_b = iobs(boxes1, boxes2)\n&gt;&gt;&gt; iobs_a.shape\n(2, 2)\n</code></pre> Source code in <code>src/dnt/engine/iob.py</code> <pre><code>def iobs(alrbs: np.ndarray, blrbs: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate the IoB matrix for multiple bounding boxes.\n\n    Parameters\n    ----------\n    alrbs : np.ndarray\n        Array of shape (N, 4) containing N bounding boxes with format\n        [left, top, width, height].\n    blrbs : np.ndarray\n        Array of shape (M, 4) containing M bounding boxes with format\n        [left, top, width, height].\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple (iobs_a, iobs_b) where:\n        - iobs_a: array of shape (N, M) with IoB values relative to alrbs\n        - iobs_b: array of shape (N, M) with IoB values relative to blrbs\n\n    Raises\n    ------\n    ValueError\n        If arrays do not have shape (N, 4) and (M, 4) respectively, or contain\n        negative width/height values.\n\n    Notes\n    -----\n    Bounding box format is [left, top, width, height] where:\n    - left: x-coordinate of top-left corner\n    - top: y-coordinate of top-left corner\n    - width: box width (must be non-negative)\n    - height: box height (must be non-negative)\n\n    Examples\n    --------\n    &gt;&gt;&gt; boxes1 = np.array([[0, 0, 10, 10], [5, 5, 10, 10]])\n    &gt;&gt;&gt; boxes2 = np.array([[0, 0, 5, 5], [10, 10, 5, 5]])\n    &gt;&gt;&gt; iobs_a, iobs_b = iobs(boxes1, boxes2)\n    &gt;&gt;&gt; iobs_a.shape\n    (2, 2)\n\n    \"\"\"\n    if alrbs.ndim != 2 or alrbs.shape[1] != 4:\n        raise ValueError(\"alrbs must have shape (N, 4) with format [left, top, width, height].\")\n    if blrbs.ndim != 2 or blrbs.shape[1] != 4:\n        raise ValueError(\"blrbs must have shape (M, 4) with format [left, top, width, height].\")\n    if np.any(alrbs[:, 2:] &lt; 0) or np.any(blrbs[:, 2:] &lt; 0):\n        raise ValueError(\"Box width and height must be non-negative.\")\n\n    num_a = alrbs.shape[0]\n    num_b = blrbs.shape[0]\n    iobs_a = np.zeros((num_a, num_b))\n    iobs_b = np.zeros((num_a, num_b))\n\n    for i in range(num_a):\n        for j in range(num_b):\n            iobs_a[i, j], iobs_b[i, j] = iob(alrbs[i, :], blrbs[j, :])\n\n    return iobs_a, iobs_b\n</code></pre>"},{"location":"api/filter/filter/","title":"Filter","text":"<p>Filter module for detection and track filtering operations.</p> <p>This module provides filtering utilities for detections and tracks based on: - Intersection over Union (IoU) with zones - Spatial containment within polygons - Line crossing detection - Various reference points and offsets</p>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter","title":"Filter","text":"<pre><code>Filter()\n</code></pre> <p>Filter class for detection and track filtering operations.</p> <p>Provides static methods for filtering detections and tracks based on: - Intersection over Union (IoU) with zones - Spatial containment within polygons - Line crossing detection - Various reference points and offsets</p> <p>Methods:</p> Name Description <code>filter_iou</code> <p>Filter detections by IoU with zones and class list.</p> <code>filter_tracks</code> <p>Filter tracks by inclusion and exclusion zones.</p> <code>filter_tracks_by_zones_agg</code> <p>Filter tracks aggregated by zones with configurable reference point.</p> <code>filter_frames_by_zones_agg</code> <p>Filter frames aggregated by zones with configurable reference point.</p> <code>filter_tracks_by_zones</code> <p>Filter tracks by zones with list, filter, or label methods.</p> <code>filter_tracks_by_lines</code> <p>Filter tracks by line crossing detection.</p> <code>filter_tracks_by_lines_v2</code> <p>Advanced line crossing detection with configurable tolerance and forced line indexes.</p> <p>Initialize the Filter class.</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Filter class.\"\"\"\n    pass\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_iou","title":"filter_iou  <code>staticmethod</code>","text":"<pre><code>filter_iou(\n    detections: DataFrame,\n    zones: MultiPolygon | None = None,\n    class_list: list[int] | None = None,\n    score_threshold: float = 0,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter detections by IoU with zones and class list.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>DataFrame</code> <p>DataFrame of detections with columns for x, y, width, height, score, and class.</p> required <code>zones</code> <code>multipolygon</code> <p>MultiPolygon zones to filter detections within. Default is None.</p> <code>None</code> <code>class_list</code> <code>list[int]</code> <p>List of class IDs to include in filtering. Default is None.</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>Minimum confidence score threshold. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered detections within zones and matching class list and score threshold.</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_iou(\n    detections: pd.DataFrame,\n    zones: geometry.MultiPolygon | None = None,\n    class_list: list[int] | None = None,\n    score_threshold: float = 0,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter detections by IoU with zones and class list.\n\n    Parameters\n    ----------\n    detections : pd.DataFrame\n        DataFrame of detections with columns for x, y, width, height, score, and class.\n    zones : geometry.multipolygon, optional\n        MultiPolygon zones to filter detections within. Default is None.\n    class_list : list[int], optional\n        List of class IDs to include in filtering. Default is None.\n    score_threshold : float, optional\n        Minimum confidence score threshold. Default is 0.\n\n    Returns\n    -------\n    pd.DataFrame\n        Filtered detections within zones and matching class list and score threshold.\n\n    \"\"\"\n    detections = detections.loc[detections[6] &gt;= score_threshold].copy()\n\n    # filter classess\n    if class_list:\n        detections = detections.loc[detections[7].isin(class_list)].copy()\n\n    if zones:\n        # filter locations\n        g = [\n            geometry.Point(xy)\n            for xy in zip((detections[2] + detections[4] / 2), (detections[3] + detections[5] / 2), strict=True)\n        ]\n        geo_detections = gpd.GeoDataFrame(detections, geometry=g)\n\n        frames = geo_detections.loc[geo_detections.geometry.within(zones)].drop(columns=\"geometry\")\n\n        if frames:\n            results = pd.concat(frames)\n            results = results[~results.index.duplicated()].reset_index(drop=True)\n        else:\n            results = pd.DataFrame()\n\n    else:\n        results = detections\n\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_tracks","title":"filter_tracks  <code>staticmethod</code>","text":"<pre><code>filter_tracks(\n    tracks: DataFrame,\n    include_zones: MultiPolygon | None = None,\n    exclude_zones: MultiPolygon | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by inclusion and exclusion zones.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame</code> <p>DataFrame of tracks with columns for x, y, width, height, and track ID.</p> required <code>include_zones</code> <code>MultiPolygon</code> <p>MultiPolygon zones to include tracks within. Default is None.</p> <code>None</code> <code>exclude_zones</code> <code>MultiPolygon</code> <p>MultiPolygon zones to exclude tracks from. Default is None.</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Index of the current video being processed. Default is None.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total number of videos being processed. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered tracks after applying inclusion and exclusion zones.</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_tracks(\n    tracks: pd.DataFrame,\n    include_zones: geometry.MultiPolygon | None = None,\n    exclude_zones: geometry.MultiPolygon | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by inclusion and exclusion zones.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame\n        DataFrame of tracks with columns for x, y, width, height, and track ID.\n    include_zones : geometry.MultiPolygon, optional\n        MultiPolygon zones to include tracks within. Default is None.\n    exclude_zones : geometry.MultiPolygon, optional\n        MultiPolygon zones to exclude tracks from. Default is None.\n    video_index : int, optional\n        Index of the current video being processed. Default is None.\n    video_tot : int, optional\n        Total number of videos being processed. Default is None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Filtered tracks after applying inclusion and exclusion zones.\n\n    \"\"\"\n    g = [geometry.Point(xy) for xy in zip((tracks[2] + tracks[4] / 2), (tracks[3] + tracks[5] / 2), strict=True)]\n    geo_tracks = gpd.GeoDataFrame(tracks, geometry=g)\n\n    track_ids = tracks[1].unique()\n    include_ids = []\n    exclude_ids = []\n\n    pbar = tqdm(total=len(track_ids), unit=\" tracks\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering zones {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering zones \")\n\n    for track_id in track_ids:\n        if include_zones:\n            selected_tracks = geo_tracks.loc[\n                (geo_tracks[1] == track_id) &amp; (geo_tracks.geometry.within(include_zones))\n            ]\n            if len(selected_tracks) &gt; 0:\n                include_ids.append(track_id)\n\n        if exclude_zones:\n            selected_tracks = geo_tracks.loc[\n                (geo_tracks[1] == track_id) &amp; (geo_tracks.geometry.within(exclude_zones))\n            ]\n            if len(selected_tracks) &gt; 0:\n                exclude_ids.append(track_id)\n\n        pbar.update()\n\n    pbar.close()\n\n    if len(include_ids) &gt; 0:\n        results = tracks.loc[tracks[1].isin(include_ids)].copy()\n    else:\n        results = tracks.copy()\n\n    if len(exclude_ids) &gt; 0:\n        results = results.loc[~results[1].isin(exclude_ids)].copy()\n\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_tracks_by_zones_agg","title":"filter_tracks_by_zones_agg  <code>staticmethod</code>","text":"<pre><code>filter_tracks_by_zones_agg(\n    tracks: DataFrame,\n    zones: MultiPolygon | None = None,\n    method: str = \"include\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by zones. Inputs:     tracks: tracks     zones: a list of polygons     method - 'include' (default), 'exclude'     ref_point - the reference point of a track bbox,         defalt is br, others - bl, bc, tl, tc, tr, cl, cc, cr     offset - the offset to ref_point, default is (0, 0)     video_index - video index     video_tot - total videos Return:     Filtered tracks</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_tracks_by_zones_agg(\n    tracks: pd.DataFrame,\n    zones: geometry.MultiPolygon | None = None,\n    method: str = \"include\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by zones.\n    Inputs:\n        tracks: tracks\n        zones: a list of polygons\n        method - 'include' (default), 'exclude'\n        ref_point - the reference point of a track bbox,\n            defalt is br, others - bl, bc, tl, tc, tr, cl, cc, cr\n        offset - the offset to ref_point, default is (0, 0)\n        video_index - video index\n        video_tot - total videos\n    Return:\n        Filtered tracks\n    \"\"\"\n    if offset is None:\n        offset = (0, 0)\n    if col_names is None:\n        col_names = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"r3\", \"r4\"]\n\n    try:\n        tracks.columns = col_names\n    except Exception:\n        print(\"Tracks is invalid!\")\n\n    if ref_point == \"cc\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]),\n                (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]),\n                strict=True,\n            )\n        ]\n    elif ref_point == \"tc\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"bc\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n    elif ref_point == \"cl\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]), strict=True)\n        ]\n    elif ref_point == \"cr\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]), strict=True\n            )\n        ]\n    elif ref_point == \"tl\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)]\n    elif ref_point == \"tr\":\n        g = [\n            Point(xy) for xy in zip((tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"bl\":\n        g = [\n            Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"br\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n    else:\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n\n    geo_tracks = gpd.GeoDataFrame(tracks, geometry=g)\n\n    matched_ids = []\n    pbar = tqdm(total=len(zones), unit=\" zones\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering zones {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering zones \")\n\n    for zone in zones:\n        matched = geo_tracks[geo_tracks.geometry.within(zone)]\n        if len(matched) &gt; 0:\n            matched_ids.extend(matched[\"track\"].unique().tolist())\n        pbar.update()\n\n    pbar.close()\n\n    if len(matched_ids) &gt; 0:\n        if method == \"include\":\n            results = tracks.loc[tracks[\"track\"].isin(matched_ids)].copy()\n        else:\n            results = tracks.loc[~tracks[\"track\"].isin(matched_ids)].copy()\n    else:\n        results = tracks.copy()\n\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_frames_by_zones_agg","title":"filter_frames_by_zones_agg  <code>staticmethod</code>","text":"<pre><code>filter_frames_by_zones_agg(\n    tracks: DataFrame,\n    zones: MultiPolygon | None = None,\n    method: str = \"include\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by zones. Inputs:     tracks - tracks     zones - zones (polygon)     method - 'include' (default) include the tracks if they are within the zones; 'exclude' exclude the tracks if they are within the zones     ref_point - the reference point of a track bbox, defalt is bottom_point, center_point, left_up, right_up, left_buttom, right_buttom     offset - the offset to ref_point, default is (0, 0)     video_index - video index     video_tot - total videos Return:     Filtered tracks</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_frames_by_zones_agg(\n    tracks: pd.DataFrame,\n    zones: geometry.MultiPolygon | None = None,\n    method: str = \"include\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by zones.\n    Inputs:\n        tracks - tracks\n        zones - zones (polygon)\n        method - 'include' (default) include the tracks if they are within the zones; 'exclude' exclude the tracks if they are within the zones\n        ref_point - the reference point of a track bbox, defalt is bottom_point, center_point, left_up, right_up, left_buttom, right_buttom\n        offset - the offset to ref_point, default is (0, 0)\n        video_index - video index\n        video_tot - total videos\n    Return:\n        Filtered tracks\n    \"\"\"\n    if offset is None:\n        offset = (0, 0)\n    if col_names is None:\n        col_names = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"r3\", \"r4\"]\n\n    try:\n        tracks.columns = col_names\n    except:\n        print(\"Tracks is invalid!\")\n\n    if ref_point == \"cc\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]),\n                (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]),\n                strict=True,\n            )\n        ]\n    elif ref_point == \"tc\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"bc\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n    elif ref_point == \"cl\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]), strict=True)\n        ]\n    elif ref_point == \"cr\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]), strict=True\n            )\n        ]\n    elif ref_point == \"tl\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)]\n    elif ref_point == \"tr\":\n        g = [\n            Point(xy) for xy in zip((tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"bl\":\n        g = [\n            Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True)\n        ]\n    elif ref_point == \"br\":\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n    else:\n        g = [\n            Point(xy)\n            for xy in zip(\n                (tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]), strict=True\n            )\n        ]\n\n    geo_tracks = gpd.GeoDataFrame(tracks, geometry=g)\n\n    matched_frames = []\n    pbar = tqdm(total=len(zones), unit=\" zones\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering zones {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering zones \")\n\n    for zone in zones:\n        matched = geo_tracks[geo_tracks.geometry.within(zone)]\n        if len(matched) &gt; 0:\n            matched_frames.extend(matched.index.values.tolist())\n        pbar.update()\n\n    pbar.close()\n\n    if len(matched_frames) &gt; 0:\n        if method == \"include\":\n            results = tracks.iloc[matched_frames].copy()\n        else:\n            results = tracks.drop(matched_frames, axis=0).copy()\n    else:\n        results = tracks.copy()\n\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_tracks_by_zones","title":"filter_tracks_by_zones  <code>staticmethod</code>","text":"<pre><code>filter_tracks_by_zones(\n    tracks: DataFrame,\n    zones: list[Polygon] | None = None,\n    method: str = \"list\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    zone_name: str = \"zone\",\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by zones.</p> <p>Inputs:     tracks - tracks     zones - zones (polygon)     method - 'list' (default) - List track ids within zones              'filter' - filter tracks within zones              'label' - label tracks with zone index     ref_point - the reference point of a track bbox,                 br - buttom_right,                 bl - bottom_left                 bc - bottom_center                 cc - center_point,                 cl - left_center,                 cr - right_center,                 tc - top_center,                 tl - top_left,                 tr - top_right,     offset - the offset to ref_point, default is (0, 0)     aggregate - combine outputs to one dataframe, add zone column     zone_name - if aggregate, the field name of zone variable, default is 'zone'     video_index - video index     video_tot - total videos Return:     Filtered tracks</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_tracks_by_zones(\n    tracks: pd.DataFrame,\n    zones: list[Polygon] | None = None,\n    method: str = \"list\",\n    ref_point: str = \"bc\",\n    offset: tuple | None = None,\n    col_names: list | None = None,\n    zone_name: str = \"zone\",\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by zones.\n\n    Inputs:\n        tracks - tracks\n        zones - zones (polygon)\n        method - 'list' (default) - List track ids within zones\n                 'filter' - filter tracks within zones\n                 'label' - label tracks with zone index\n        ref_point - the reference point of a track bbox,\n                    br - buttom_right,\n                    bl - bottom_left\n                    bc - bottom_center\n                    cc - center_point,\n                    cl - left_center,\n                    cr - right_center,\n                    tc - top_center,\n                    tl - top_left,\n                    tr - top_right,\n        offset - the offset to ref_point, default is (0, 0)\n        aggregate - combine outputs to one dataframe, add zone column\n        zone_name - if aggregate, the field name of zone variable, default is 'zone'\n        video_index - video index\n        video_tot - total videos\n    Return:\n        Filtered tracks\n    \"\"\"\n    if offset is None:\n        offset = (0, 0)\n    if col_names is None:\n        col_names = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"r3\", \"r4\"]\n\n    try:\n        tracks.columns = col_names\n    except Exception:\n        print(\"Tracks is invalid!\")\n\n    if ref_point == \"cc\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]))\n        ]\n    elif ref_point == \"tc\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + offset[1]))]\n    elif ref_point == \"bc\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]))\n        ]\n    elif ref_point == \"cl\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]))]\n    elif ref_point == \"cr\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] / 2 + offset[1]))\n        ]\n    elif ref_point == \"tl\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + offset[1]))]\n    elif ref_point == \"tr\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + offset[1]))]\n    elif ref_point == \"bl\":\n        g = [Point(xy) for xy in zip((tracks[\"x\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]))]\n    elif ref_point == \"br\":\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]))\n        ]\n    else:\n        g = [\n            Point(xy)\n            for xy in zip((tracks[\"x\"] + tracks[\"w\"] / 2 + offset[0]), (tracks[\"y\"] + tracks[\"h\"] + offset[1]))\n        ]\n\n    geo_tracks = gpd.GeoDataFrame(tracks, geometry=g)\n\n    matched_ids = []\n    pbar = tqdm(total=len(zones), unit=\" zones\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering zones {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering zones \")\n\n    for zone in zones:\n        matched = geo_tracks[geo_tracks.geometry.within(zone)]\n        if len(matched) &gt; 0:\n            matched_ids.append(matched[\"track\"].unique().tolist())\n        pbar.update()\n\n    pbar.close()\n\n    if (method == \"filter\") or (method == \"label\"):\n        tracks[zone_name] = -1\n        for i in range(len(matched_ids)):\n            tracks.loc[tracks[\"track\"].isin(matched_ids[i]), zone_name] = i\n        results = tracks[tracks[zone_name] != -1].copy() if method == \"filter\" else tracks\n    else:\n        results = []\n        if len(matched_ids) &gt; 0:\n            for i in range(len(matched_ids)):\n                result = tracks.loc[tracks[\"track\"].isin(matched_ids[i])].copy()\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_tracks_by_lines","title":"filter_tracks_by_lines  <code>staticmethod</code>","text":"<pre><code>filter_tracks_by_lines(\n    tracks: DataFrame,\n    lines: list[LineString] | None = None,\n    method: str = \"include\",\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by lines.</p> <p>Inputs:     tracks - a DataFrame of tracks, [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED, RESERVED, RESERVED]     lines - a list of LineString     method - filtering method, include (default) - including tracks crossing         the lines, exclude - exclude tracks crossing the lines     video_index - the index of video for processing     video_tot - the total number of videos Return:     a DataFrame of [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED, RESERVED, RESERVED]</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_tracks_by_lines(\n    tracks: pd.DataFrame,\n    lines: list[LineString] | None = None,\n    method: str = \"include\",\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by lines.\n\n    Inputs:\n        tracks - a DataFrame of tracks, [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED, RESERVED, RESERVED]\n        lines - a list of LineString\n        method - filtering method, include (default) - including tracks crossing\n            the lines, exclude - exclude tracks crossing the lines\n        video_index - the index of video for processing\n        video_tot - the total number of videos\n    Return:\n        a DataFrame of [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED, RESERVED, RESERVED]\n    \"\"\"\n    track_ids = tracks[1].unique()\n    ids = []\n\n    pbar = tqdm(total=len(track_ids), unit=\" tracks\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering tracks {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering tracks \")\n\n    for track_id in track_ids:\n        selected = tracks.loc[(tracks[1] == track_id)].copy()\n        if len(selected) &gt; 0:\n            g = selected.apply(\n                lambda track: Polygon([\n                    (track[2], track[3]),\n                    (track[2] + track[4], track[3]),\n                    (track[2] + track[4], track[3] + track[5]),\n                    (track[2], track[3] + track[5]),\n                ]),\n                axis=1,\n            )\n            intersected = True\n            for line in lines:\n                intersected = intersected and any(line.intersects(g).values.tolist())\n\n            if intersected:\n                ids.append(track_id)\n\n        pbar.update()\n\n    pbar.close()\n\n    results = []\n    if method == \"include\":\n        results = tracks.loc[tracks[1].isin(ids)].copy()\n    elif method == \"exclude\":\n        results = tracks.loc[~tracks[1].isin(ids)].copy()\n\n    results.sort_values(by=[0, 1], inplace=True)\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.filter_tracks_by_lines_v2","title":"filter_tracks_by_lines_v2  <code>staticmethod</code>","text":"<pre><code>filter_tracks_by_lines_v2(\n    tracks: DataFrame,\n    lines: list[LineString] | None = None,\n    method: str = \"include\",\n    tolerance: int = 0,\n    bbox_size: int = 0,\n    force_line_indexes: list[int] | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter tracks by lines.</p> <p>Inputs:     tracks - a DataFrame of tracks, [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH,         RESERVED, RESERVED, RESERVED]     lines - a list of LineString     method - filtering method, include (default) - including tracks crossing the         lines, exclude - exclude tracks crossing the lines     tolerance - if a bbox intesect the reference lines of (number of lanes -         tolerance), it is hit. default is 0.     force_line_indexes: the line indexes that a bbox must intersect for matching     bbox_size - the size of detection bbox, default is 0 - the orginal bbox     video_index - the index of video for processing     video_tot - the total number of videos Return:     a DataFrame of [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED,         RESERVED, RESERVED]</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef filter_tracks_by_lines_v2(\n    tracks: pd.DataFrame,\n    lines: list[LineString] | None = None,\n    method: str = \"include\",\n    tolerance: int = 0,\n    bbox_size: int = 0,\n    force_line_indexes: list[int] | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter tracks by lines.\n\n    Inputs:\n        tracks - a DataFrame of tracks, [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH,\n            RESERVED, RESERVED, RESERVED]\n        lines - a list of LineString\n        method - filtering method, include (default) - including tracks crossing the\n            lines, exclude - exclude tracks crossing the lines\n        tolerance - if a bbox intesect the reference lines of (number of lanes -\n            tolerance), it is hit. default is 0.\n        force_line_indexes: the line indexes that a bbox must intersect for matching\n        bbox_size - the size of detection bbox, default is 0 - the orginal bbox\n        video_index - the index of video for processing\n        video_tot - the total number of videos\n    Return:\n        a DataFrame of [FRAME, TRACK_ID, TOPX, TOPY, WIDTH, LENGTH, RESERVED,\n            RESERVED, RESERVED]\n    \"\"\"\n    tracks.columns = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"r3\", \"r4\"]\n    track_ids = tracks[\"track\"].unique()\n    ids = []\n\n    # set hit criterion\n    hit_criterion = len(lines) - tolerance\n    if (hit_criterion &lt; 1) or (hit_criterion &gt; len(lines)):\n        hit_criterion = len(lines)\n\n    pbar = tqdm(total=len(track_ids), unit=\" tracks\")\n    if video_index and video_tot:\n        pbar.set_description_str(f\"Filtering tracks {video_index} of {video_tot}\")\n    else:\n        pbar.set_description_str(\"Filtering tracks \")\n\n    for track_id in track_ids:\n        selected = tracks.loc[(tracks[\"track\"] == track_id)].copy()\n        hit_cnt = 0\n        hit_force = True\n        if len(selected) &gt; 0:\n            if bbox_size == 0:\n                g = selected.apply(\n                    lambda track: Polygon([\n                        (track[\"x\"], track[\"y\"]),\n                        (track[\"x\"] + track[\"w\"], track[\"y\"]),\n                        (track[\"x\"] + track[\"w\"], track[\"y\"] + track[\"h\"]),\n                        (track[\"x\"], track[\"y\"] + track[\"h\"]),\n                    ]),\n                    axis=1,\n                )\n            else:\n                g = selected.apply(\n                    lambda track: Polygon([\n                        (track[\"x\"] + track[\"w\"] / 2 - bbox_size, track[\"y\"] + track[\"h\"] - bbox_size),\n                        (track[\"x\"] + track[\"w\"] / 2 + bbox_size, track[\"y\"] + track[\"h\"] - bbox_size),\n                        (track[\"x\"] + track[\"w\"] / 2 + bbox_size, track[\"y\"] + track[\"h\"]),\n                        (track[\"x\"] + track[\"w\"] / 2 - bbox_size, track[\"y\"] + track[\"h\"]),\n                    ]),\n                    axis=1,\n                )\n\n            for line in lines:\n                if any(line.intersects(g).values.tolist()):\n                    hit_cnt += 1\n\n            if force_line_indexes is not None:\n                force_lines = [lines[i] for i in force_line_indexes]\n                for line in force_lines:\n                    hit_force = any(line.intersects(g).values.tolist())\n\n            if (hit_cnt &gt;= hit_criterion) and hit_force:\n                ids.append(track_id)\n\n        pbar.update()\n\n    pbar.close()\n\n    results = []\n    if method == \"include\":\n        results = tracks.loc[tracks[\"track\"].isin(ids)].copy()\n    elif method == \"exclude\":\n        results = tracks.loc[~tracks[\"track\"].isin(ids)].copy()\n\n    results.sort_values(by=[\"frame\", \"track\"], inplace=True)\n    return results\n</code></pre>"},{"location":"api/filter/filter/#dnt.filter.filter.Filter.interpolate_tracks_rts","title":"interpolate_tracks_rts  <code>staticmethod</code>","text":"<pre><code>interpolate_tracks_rts(\n    tracks: DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    fill_gaps_only: bool = True,\n    smooth_existing: bool = False,\n    process_var: float = 10.0,\n    meas_var_pos: float = 25.0,\n    meas_var_size: float = 16.0,\n    min_track_len: int = 2,\n    max_gap: int = 30,\n    add_interp_flag: bool = True,\n    interp_col: str = \"interp\",\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Backward-compatible wrapper for :func:<code>dnt.track.post_process.interpolate_tracks_rts</code>.</p> Source code in <code>src/dnt/filter/filter.py</code> <pre><code>@staticmethod\ndef interpolate_tracks_rts(\n    tracks: pd.DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    fill_gaps_only: bool = True,\n    smooth_existing: bool = False,\n    process_var: float = 10.0,\n    meas_var_pos: float = 25.0,\n    meas_var_size: float = 16.0,\n    min_track_len: int = 2,\n    max_gap: int = 30,\n    add_interp_flag: bool = True,\n    interp_col: str = \"interp\",\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Backward-compatible wrapper for :func:`dnt.track.post_process.interpolate_tracks_rts`.\"\"\"\n    import importlib.util\n    import sys\n    from pathlib import Path\n\n    post_file = Path(__file__).resolve().parents[1] / \"track\" / \"post_process.py\"\n    module_name = \"dnt_track_post_process_dynamic\"\n    module = sys.modules.get(module_name)\n    if module is None:\n        spec = importlib.util.spec_from_file_location(module_name, post_file)\n        if spec is None or spec.loader is None:\n            raise ImportError(f\"Cannot load post_process module from: {post_file}\")\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        sys.modules[module_name] = module\n\n    _interpolate_tracks_rts = module.interpolate_tracks_rts\n\n    return _interpolate_tracks_rts(\n        tracks=tracks,\n        track_file=track_file,\n        output_file=output_file,\n        col_names=col_names,\n        fill_gaps_only=fill_gaps_only,\n        smooth_existing=smooth_existing,\n        process_var=process_var,\n        meas_var_pos=meas_var_pos,\n        meas_var_size=meas_var_size,\n        min_track_len=min_track_len,\n        max_gap=max_gap,\n        add_interp_flag=add_interp_flag,\n        interp_col=interp_col,\n        verbose=verbose,\n        video_index=video_index,\n        video_tot=video_tot,\n    )\n</code></pre>"},{"location":"api/label/labeler/","title":"Labeler","text":"<p>Video labeling and annotation module.</p> <p>This module provides functionality for drawing and annotating video frames with labels, tracks, and detections. It includes the Labeler class for drawing on videos using OpenCV or FFmpeg backends, and utilities for extracting frames and clips from videos.</p>"},{"location":"api/label/labeler/#dnt.label.labeler.ElementType","title":"ElementType","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported drawing element types.</p> Values <ul> <li>TXT: Text label</li> <li>LINE: Line segment</li> <li>BOX: Rectangle (box)</li> <li>BBOX: Bounding box with label</li> <li>CIRCLE: Circle or disk</li> <li>POLYGON: Filled polygon</li> <li>POLYLINES: Open polyline</li> </ul>"},{"location":"api/label/labeler/#dnt.label.labeler.LabelMethod","title":"LabelMethod","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported rendering backends.</p> Values <ul> <li>OPENCV: use OpenCV to draw labels (default)</li> <li>FFMPEG: use FFmpeg to draw labels with H.265 encoding for better compression</li> <li>CHROME_SAFE: use FFmpeg to draw labels with H.264 encoding and browser-compatible settings for maximum compatibility</li> </ul>"},{"location":"api/label/labeler/#dnt.label.labeler.Encoder","title":"Encoder","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported FFmpeg encoders.</p> Values <ul> <li>LIBX264: H.264 codec (AVC)</li> <li>LIBX265: H.265 codec (HEVC)</li> <li>H264_NVENC: NVIDIA GPU-accelerated H.264</li> <li>HEVC_NVENC: NVIDIA GPU-accelerated H.265</li> </ul>"},{"location":"api/label/labeler/#dnt.label.labeler.Preset","title":"Preset","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported FFmpeg presets.</p> Values <ul> <li>ULTRAFAST: Fastest encoding, largest file</li> <li>SUPERFAST: Very fast encoding</li> <li>VERYFAST: Fast encoding</li> <li>FASTER: Faster encoding</li> <li>FAST: Fast encoding</li> <li>MEDIUM: Medium speed (default for most encoders)</li> <li>SLOW: Slow encoding</li> <li>SLOWER: Very slow encoding</li> <li>VERYSLOW: Slowest encoding, smallest file</li> </ul>"},{"location":"api/label/labeler/#dnt.label.labeler.TrackClipMethod","title":"TrackClipMethod","text":"<p>               Bases: <code>StrEnum</code></p> <p>Track clip selection strategies.</p> Values <ul> <li>ALL: include all tracks (default)</li> <li>RANDOM: randomly select a specified number of tracks, specified by <code>random_number</code></li> <li>SPECIFY: specify track ids to include, provided in <code>track_ids</code></li> </ul>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler","title":"Labeler","text":"<pre><code>Labeler(\n    method: LabelMethod | str = LabelMethod.OPENCV,\n    encoder: Encoder | str = Encoder.LIBX264,\n    preset: Preset | str = Preset.MEDIUM,\n    crf: int = 23,\n    pix_fmt: str = \"bgr24\",\n    compress_message: bool = False,\n    nodraw_empty: bool = True,\n)\n</code></pre> <p>A video labeler for drawing and annotating video frames.</p> <p>This class provides functionality to draw labels, tracks, and detections on video files using either OpenCV or FFmpeg as the backend.</p> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>The drawing method: 'opencv', 'ffmpeg', or 'chrome_safe'.</p> <code>encoder</code> <code>str</code> <p>The video encoder to use with FFmpeg.</p> <code>preset</code> <code>str</code> <p>The encoding preset for quality/speed tradeoff.</p> <code>crf</code> <code>int</code> <p>Constant Rate Factor for compression quality.</p> <code>pix_fmt</code> <code>str</code> <p>Pixel format for video output.</p> <code>compress_message</code> <code>bool</code> <p>Whether to show compressed progress messages.</p> <code>nodraw_empty</code> <code>bool</code> <p>Whether to skip drawing empty frames.</p> <p>Initialize the Labeler.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>LabelMethod | str</code> <p>'opencv' (default) - use opencv to draw labels 'ffmpeg' - use ffmpeg to draw labels 'chrome_safe' - use ffmpeg to draw labels with chrome compatible video format</p> <code>OPENCV</code> <code>encoder</code> <code>Encoder | str</code> <p>'libx264' (default) - use libx264 encoder for ffmpeg 'libx265' - use libx265 encoder for ffmpeg 'h264_nvenc' - use h264_nvenc encoder for ffmpeg 'hevc_nvenc' - use hevc_nvenc encoder for ffmpeg</p> <code>LIBX264</code> <code>preset</code> <code>Preset | str</code> <p>'medium' (default) - use medium preset for ffmpeg 'slow' - use slow preset for ffmpeg 'fast' - use fast preset for ffmpeg</p> <code>MEDIUM</code> <code>crf</code> <code>int</code> <p>23 (default) - use 23 crf for ffmpeg, lower is better quality</p> <code>23</code> <code>pix_fmt</code> <code>str</code> <p>Pixel format for video output. Default is 'bgr24'.</p> <code>'bgr24'</code> <code>compress_message</code> <code>bool</code> <p>False (default) - show compress message in progress bar</p> <code>False</code> <code>nodraw_empty</code> <code>bool</code> <p>True (default) - not draw empty frames</p> <code>True</code> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def __init__(\n    self,\n    method: LabelMethod | str = LabelMethod.OPENCV,\n    encoder: Encoder | str = Encoder.LIBX264,\n    preset: Preset | str = Preset.MEDIUM,\n    crf: int = 23,\n    pix_fmt: str = \"bgr24\",\n    compress_message: bool = False,\n    nodraw_empty: bool = True,\n):\n    \"\"\"Initialize the Labeler.\n\n    Parameters\n    ----------\n    method : LabelMethod | str\n        'opencv' (default) - use opencv to draw labels\n        'ffmpeg' - use ffmpeg to draw labels\n        'chrome_safe' - use ffmpeg to draw labels with chrome compatible video format\n    encoder : Encoder | str\n        'libx264' (default) - use libx264 encoder for ffmpeg\n        'libx265' - use libx265 encoder for ffmpeg\n        'h264_nvenc' - use h264_nvenc encoder for ffmpeg\n        'hevc_nvenc' - use hevc_nvenc encoder for ffmpeg\n    preset : Preset | str\n        'medium' (default) - use medium preset for ffmpeg\n        'slow' - use slow preset for ffmpeg\n        'fast' - use fast preset for ffmpeg\n    crf : int\n        23 (default) - use 23 crf for ffmpeg, lower is better quality\n    pix_fmt : str\n        Pixel format for video output. Default is 'bgr24'.\n    compress_message : bool\n        False (default) - show compress message in progress bar\n    nodraw_empty : bool\n        True (default) - not draw empty frames\n\n    \"\"\"\n    if isinstance(method, LabelMethod):\n        self.method = method.value\n    else:\n        method_value = str(method).lower().strip()\n        valid_methods = {m.value for m in LabelMethod}\n        if method_value not in valid_methods:\n            raise ValueError(f\"Invalid method={method!r}. Choose one of {sorted(valid_methods)}.\")\n        self.method = method_value\n\n    if isinstance(encoder, Encoder):\n        self.encoder = encoder.value\n    else:\n        encoder_value = str(encoder).lower().strip()\n        valid_encoders = {e.value for e in Encoder}\n        if encoder_value not in valid_encoders:\n            raise ValueError(f\"Invalid encoder={encoder!r}. Choose one of {sorted(valid_encoders)}.\")\n        self.encoder = encoder_value\n\n    if isinstance(preset, Preset):\n        self.preset = preset.value\n    else:\n        preset_value = str(preset).lower().strip()\n        valid_presets = {p.value for p in Preset}\n        if preset_value not in valid_presets:\n            raise ValueError(f\"Invalid preset={preset!r}. Choose one of {sorted(valid_presets)}.\")\n        self.preset = preset_value\n\n    self.crf = crf\n    self.pix_fmt = pix_fmt\n    self.compress_message = compress_message\n    self.nodraw_empty = nodraw_empty\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.draw_shapes","title":"draw_shapes","text":"<pre><code>draw_shapes(\n    draw_file: str,\n    output_file: str | None,\n    shapes: list[dict] | dict,\n    base_df: DataFrame | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Add shapes to a draw file in DRAW_COLUMNS format.</p> <p>Parameters:</p> Name Type Description Default <code>draw_file</code> <code>str</code> <p>Path to a draw CSV file. Used for reading existing data and as the default write target.</p> required <code>output_file</code> <code>str</code> <p>Path to write the combined result. If None, writes to draw_file.</p> required <code>shapes</code> <code>list[dict] | dict</code> <p>One or more shape specifications. Each dict may contain:</p> <ul> <li><code>type</code> - <code>\"line\"</code>, <code>\"polyline\"</code>, <code>\"circle\"</code>,   <code>\"rectangle\"</code>, or <code>\"polygon\"</code></li> <li><code>geometry</code> - coordinate pairs, a Shapely geometry, or a WKT string</li> <li><code>color</code> - BGR tuple, e.g. <code>(0, 255, 0)</code> (optional)</li> <li><code>fill</code> - whether to fill the shape (optional)</li> <li><code>alpha</code> - overlay strength 0.0-1.0; aliases: <code>transparent</code>,   <code>transparant</code> (optional)</li> <li><code>size</code> - font size or radius (optional)</li> <li><code>thick</code> - line thickness (optional)</li> <li><code>desc</code> - text description (optional)</li> </ul> required <code>base_df</code> <code>DataFrame</code> <p>Existing draw DataFrame to append to. If None and draw_file exists, that file is loaded instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Combined DataFrame with DRAW_COLUMNS.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a shape specification is invalid (bad type, insufficient points, or unsupported geometry).</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def draw_shapes(\n    self,\n    draw_file: str,\n    output_file: str | None,\n    shapes: list[dict] | dict,\n    base_df: pd.DataFrame | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Add shapes to a draw file in DRAW_COLUMNS format.\n\n    Parameters\n    ----------\n    draw_file : str\n        Path to a draw CSV file. Used for reading existing data and as the\n        default write target.\n    output_file : str, optional\n        Path to write the combined result. If None, writes to *draw_file*.\n    shapes : list[dict] | dict\n        One or more shape specifications. Each dict may contain:\n\n        - ``type`` - ``\"line\"``, ``\"polyline\"``, ``\"circle\"``,\n          ``\"rectangle\"``, or ``\"polygon\"``\n        - ``geometry`` - coordinate pairs, a Shapely geometry, or a WKT string\n        - ``color`` - BGR tuple, e.g. ``(0, 255, 0)`` *(optional)*\n        - ``fill`` - whether to fill the shape *(optional)*\n        - ``alpha`` - overlay strength 0.0-1.0; aliases: ``transparent``,\n          ``transparant`` *(optional)*\n        - ``size`` - font size or radius *(optional)*\n        - ``thick`` - line thickness *(optional)*\n        - ``desc`` - text description *(optional)*\n    base_df : pd.DataFrame, optional\n        Existing draw DataFrame to append to. If None and *draw_file*\n        exists, that file is loaded instead.\n\n    Returns\n    -------\n    pd.DataFrame\n        Combined DataFrame with DRAW_COLUMNS.\n\n    Raises\n    ------\n    ValueError\n        If a shape specification is invalid (bad type, insufficient points,\n        or unsupported geometry).\n\n    \"\"\"\n    if isinstance(shapes, dict):\n        shapes = [shapes]\n\n    if base_df is not None:\n        base_df = self._ensure_draw_columns(base_df.copy())\n    elif os.path.exists(draw_file):\n        base_df = pd.read_csv(\n            draw_file,\n            dtype={\"frame\": int, \"type\": str, \"size\": float, \"desc\": str, \"thick\": int},\n            converters={\"coords\": lambda x: list(literal_eval(x)), \"color\": lambda x: literal_eval(x)},\n        )\n        base_df = self._ensure_draw_columns(base_df)\n    else:\n        base_df = pd.DataFrame(columns=DRAW_COLUMNS)\n\n    def _parse_geom(geom: object) -&gt; object:\n        if isinstance(geom, str):\n            try:\n                from shapely import wkt\n            except ImportError as exc:\n                raise ValueError(\"WKT geometry requires shapely to be installed.\") from exc\n            return wkt.loads(geom)\n        return geom\n\n    def _as_pairs(geom: object) -&gt; list[tuple[float, float]]:\n        if hasattr(geom, \"coords\"):\n            return [(float(x), float(y)) for x, y in geom.coords]\n        if isinstance(geom, (list, tuple, np.ndarray)):\n            vals = list(geom)\n            if len(vals) == 2 and all(isinstance(v, (int, float, np.integer, np.floating)) for v in vals):\n                return [(float(vals[0]), float(vals[1]))]\n            if len(vals) == 4 and all(isinstance(v, (int, float, np.integer, np.floating)) for v in vals):\n                return [(float(vals[0]), float(vals[1])), (float(vals[2]), float(vals[3]))]\n            if (\n                len(vals) &gt;= 2\n                and len(vals) % 2 == 0\n                and all(isinstance(v, (int, float, np.integer, np.floating)) for v in vals)\n            ):\n                return [(float(vals[i]), float(vals[i + 1])) for i in range(0, len(vals), 2)]\n            pairs: list[tuple[float, float]] = []\n            for p in vals:\n                if isinstance(p, (list, tuple, np.ndarray)) and len(p) &gt;= 2:\n                    pairs.append((float(p[0]), float(p[1])))\n                else:\n                    raise ValueError(f\"Unsupported geometry point format: {p!r}\")\n            return pairs\n        raise ValueError(f\"Unsupported geometry coordinates: {type(geom)!r}\")\n\n    default_frames = sorted(base_df[\"frame\"].astype(int).unique().tolist()) if len(base_df) &gt; 0 else [0]\n\n    if not shapes:\n        target_file = output_file or draw_file\n        if target_file:\n            base_df.to_csv(target_file, index=False)\n        return base_df\n\n    rows: list[list[object]] = []\n    for idx, shape in enumerate(shapes):\n        if isinstance(shape, dict):\n            shape_type = str(shape.get(\"type\", \"\")).lower().strip()\n            geom = _parse_geom(shape.get(\"geometry\"))\n            color = self._normalize_color(shape.get(\"color\"), _bright_color(idx))\n            fill = bool(shape.get(\"fill\", False))\n            alpha = float(shape.get(\"alpha\", shape.get(\"transparent\", shape.get(\"transparant\", 0.0))))\n            size = float(shape.get(\"size\", 4.0))\n            thick = int(shape.get(\"thick\", 2))\n            desc = str(shape.get(\"desc\", \"\"))\n        else:\n            geom = _parse_geom(shape)\n            shape_type = \"\"\n            color = _bright_color(idx)\n            fill = False\n            alpha = 0.0\n            size = 4.0\n            thick = 2\n            desc = \"\"\n\n        alpha = min(max(alpha, 0.0), 1.0)\n        target_frames = default_frames\n\n        if not shape_type:\n            if hasattr(geom, \"geom_type\"):\n                geom_type = str(geom.geom_type).lower()\n                if geom_type in {\"linestring\", \"linearring\"}:\n                    shape_type = \"polyline\"\n                elif geom_type == \"polygon\":\n                    shape_type = \"polygon\"\n                elif geom_type == \"point\":\n                    shape_type = \"circle\"\n            elif isinstance(geom, (list, tuple)) and len(geom) == 4:\n                shape_type = \"rectangle\"\n\n        if shape_type == \"line\":\n            coords = _as_pairs(geom)\n            if len(coords) != 2:\n                raise ValueError(\"line requires exactly 2 points.\")\n            elem_type = ElementType.LINE.value\n        elif shape_type in {\"polyline\", \"polylines\"}:\n            coords = _as_pairs(geom)\n            if len(coords) &lt; 2:\n                raise ValueError(\"polyline requires at least 2 points.\")\n            elem_type = ElementType.POLYLINES.value\n        elif shape_type == \"circle\":\n            if hasattr(geom, \"geom_type\") and str(geom.geom_type).lower() == \"point\":\n                coords = [(float(geom.x), float(geom.y))]\n            else:\n                pts = _as_pairs(geom)\n                if len(pts) != 1:\n                    raise ValueError(\"circle requires a center point.\")\n                coords = [pts[0]]\n            elem_type = ElementType.CIRCLE.value\n        elif shape_type in {\"rectangle\", \"rect\", \"box\"}:\n            if hasattr(geom, \"bounds\"):\n                minx, miny, maxx, maxy = geom.bounds\n                coords = [(float(minx), float(miny)), (float(maxx), float(maxy))]\n            else:\n                pts = _as_pairs(geom)\n                if len(pts) != 2:\n                    raise ValueError(\"rectangle requires 2 corner points.\")\n                coords = [pts[0], pts[1]]\n            elem_type = ElementType.BOX.value\n        elif shape_type == \"polygon\":\n            if hasattr(geom, \"geom_type\") and str(geom.geom_type).lower() == \"polygon\":\n                coords = [(float(x), float(y)) for x, y in geom.exterior.coords]\n            else:\n                coords = _as_pairs(geom)\n            if len(coords) &lt; 3:\n                raise ValueError(\"polygon requires at least 3 points.\")\n            if coords[0] != coords[-1]:\n                coords = [*coords, coords[0]]\n            elem_type = ElementType.POLYGON.value\n        else:\n            raise ValueError(f\"Unsupported shape type: {shape_type!r}\")\n\n        for frame in target_frames:\n            rows.append([frame, elem_type, coords, color, size, thick, desc, fill, alpha])\n\n    shape_df = pd.DataFrame(rows, columns=DRAW_COLUMNS)\n    out_df = pd.concat([base_df, shape_df], ignore_index=True)\n\n    out_df.sort_values(by=\"frame\", inplace=True)\n    target_file = output_file or draw_file\n    if target_file:\n        out_df.to_csv(target_file, index=False)\n\n    return out_df\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.draw","title":"draw","text":"<pre><code>draw(\n    input_video: str,\n    output_video: str,\n    draws: DataFrame | None = None,\n    draw_file: str | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>Draw labels on video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to raw video file.</p> required <code>output_video</code> <code>str</code> <p>Path to output labeled video file.</p> required <code>draws</code> <code>DataFrame</code> <p>A DataFrame containing labeling information. If None, reads from draw_file.</p> <code>None</code> <code>draw_file</code> <code>str</code> <p>A txt/csv file with header: ['frame','type','coords','color','size','thick','desc','fill','alpha']</p> <code>None</code> <code>start_frame</code> <code>int</code> <p>Starting frame number. If None, defaults to 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Ending frame number. If None, defaults to last frame.</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> <code>RuntimeError</code> <p>If FFmpeg fails (when using ffmpeg or chrome_safe method).</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def draw(\n    self,\n    input_video: str,\n    output_video: str,\n    draws: pd.DataFrame | None = None,\n    draw_file: str | None = None,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n):\n    \"\"\"Draw labels on video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to raw video file.\n    output_video : str\n        Path to output labeled video file.\n    draws : pd.DataFrame, optional\n        A DataFrame containing labeling information. If None, reads from draw_file.\n    draw_file : str, optional\n        A txt/csv file with header:\n        ['frame','type','coords','color','size','thick','desc','fill','alpha']\n    start_frame : int, optional\n        Starting frame number. If None, defaults to 0.\n    end_frame : int, optional\n        Ending frame number. If None, defaults to last frame.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n    verbose : bool, optional\n        Whether to show progress. Default is True.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n    RuntimeError\n        If FFmpeg fails (when using ffmpeg or chrome_safe method).\n\n    \"\"\"\n    if draws is not None:\n        data = draws\n    else:\n        data = pd.read_csv(\n            draw_file,\n            dtype={\"frame\": int, \"type\": str, \"size\": float, \"desc\": str, \"thick\": int},\n            converters={\"coords\": lambda x: list(literal_eval(x)), \"color\": lambda x: literal_eval(x)},\n        )\n    data = self._ensure_draw_columns(data)\n\n    output_dir = os.path.dirname(output_video)\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    if start_frame is None:\n        start_frame = 0\n    if end_frame is None:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n\n    tot_frames = end_frame - start_frame + 1\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame_to_elements = {k: g for k, g in data.groupby(\"frame\", sort=False)}\n    use_ffmpeg = self.method in {\"ffmpeg\", \"chrome_safe\"}\n    process = None\n    writer = None\n\n    if self.method == \"ffmpeg\":\n        # FFmpeg command to write H.265 encoded video\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",  # Overwrite output file if it exists\n            \"-f\",\n            \"rawvideo\",\n            \"-vcodec\",\n            \"rawvideo\",\n            \"-pix_fmt\",\n            self.pix_fmt,\n            \"-s\",\n            f\"{width}x{height}\",\n            \"-r\",\n            str(fps),\n            \"-i\",\n            \"-\",  # Read input from stdin\n            \"-c:v\",\n            self.encoder,  # H.265 codec\n            \"-preset\",\n            self.preset,  # Adjust preset as needed (ultrafast, fast, medium, slow, etc.)\n            \"-crf\",\n            str(self.crf),  # Constant Rate Factor (higher = more compression, lower = better quality)\n            output_video,\n        ]\n\n        # Start FFmpeg process\n        process = subprocess.Popen(\n            ffmpeg_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE\n        )\n    elif self.method == \"chrome_safe\":\n        # FFmpeg command for browser-safe H.264 output.\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",  # Overwrite output file if it exists\n            \"-f\",\n            \"rawvideo\",\n            \"-vcodec\",\n            \"rawvideo\",\n            \"-pix_fmt\",\n            self.pix_fmt,\n            \"-s\",\n            f\"{width}x{height}\",\n            \"-r\",\n            str(fps),\n            \"-i\",\n            \"-\",  # Read input from stdin\n            \"-c:v\",\n            \"libx264\",  # H.264 codec\n            \"-profile:v\",\n            \"high\",\n            \"-level\",\n            \"4.0\",\n            \"-preset\",\n            \"medium\",\n            \"-crf\",\n            \"23\",\n            \"-pix_fmt\",\n            \"yuv420p\",\n            \"-movflags\",\n            \"+faststart\",\n            \"-an\",\n            output_video,\n        ]\n\n        # Start FFmpeg process\n        process = subprocess.Popen(\n            ffmpeg_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE\n        )\n    else:\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n\n    if verbose:\n        pbar = tqdm(total=tot_frames, unit=\" frames\")\n        if self.compress_message:\n            pbar.set_description_str(\"Labeling\")\n        else:\n            if video_index and video_tot:\n                pbar.set_description_str(f\"Labeling {video_index} of {video_tot}\")\n            else:\n                pbar.set_description_str(f\"Labeling {input_video} \")\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n    while cap.isOpened():\n        pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n        ret, frame = cap.read()\n        if (not ret) or (pos_frame &gt; end_frame):\n            break\n\n        elements = frame_to_elements.get(pos_frame)\n        if elements is None:\n            if use_ffmpeg:\n                try:\n                    process.stdin.write(frame.tobytes())\n                except BrokenPipeError as exc:\n                    ffmpeg_error = process.stderr.read().decode(\"utf-8\", errors=\"replace\") if process.stderr else \"\"\n                    raise RuntimeError(\n                        f\"FFmpeg pipe closed early while writing frame {pos_frame}.\\n{ffmpeg_error}\"\n                    ) from exc\n            else:\n                writer.write(frame)\n            if verbose:\n                pbar.update()\n            continue\n\n        for _, element in elements.iterrows():\n            self._draw_element(\n                frame,\n                {\n                    \"type\": element[\"type\"],\n                    \"coords\": element[\"coords\"],\n                    \"color\": element[\"color\"],\n                    \"size\": element[\"size\"],\n                    \"thick\": element[\"thick\"],\n                    \"desc\": element[\"desc\"],\n                    \"fill\": element[\"fill\"],\n                    \"alpha\": element[\"alpha\"],\n                },\n            )\n\n        if use_ffmpeg:\n            try:\n                process.stdin.write(frame.tobytes())\n            except BrokenPipeError as exc:\n                ffmpeg_error = process.stderr.read().decode(\"utf-8\", errors=\"replace\") if process.stderr else \"\"\n                raise RuntimeError(\n                    f\"FFmpeg pipe closed early while writing frame {pos_frame}.\\n{ffmpeg_error}\"\n                ) from exc\n        else:\n            writer.write(frame)\n\n        if verbose:\n            pbar.update()\n\n    if verbose:\n        pbar.close()\n    # cv2.destroyAllWindows()\n    cap.release()\n    if use_ffmpeg:\n        if process.stdin:\n            process.stdin.close()\n        return_code = process.wait()\n        if return_code != 0:\n            ffmpeg_error = process.stderr.read().decode(\"utf-8\", errors=\"replace\") if process.stderr else \"\"\n            raise RuntimeError(f\"FFmpeg failed with exit code {return_code}.\\n{ffmpeg_error}\")\n    else:\n        writer.release()\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.draw_track_clips","title":"draw_track_clips","text":"<pre><code>draw_track_clips(\n    input_video: str,\n    output_path: str,\n    tracks: DataFrame | None = None,\n    track_file: str | None = None,\n    method: TrackClipMethod | str = TrackClipMethod.ALL,\n    random_number: int = 10,\n    track_ids: list | None = None,\n    start_frame_offset: int = 0,\n    end_frame_offset: int = 0,\n    tail_length: int = 0,\n    label_prefix: bool = False,\n    label_class: bool = False,\n    shapes: list[dict] | None = None,\n    color: tuple[int, int, int] | str | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    size: int = 1,\n    thick: int = 1,\n    tail_size: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>Draw track clips from video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>The raw video file.</p> required <code>output_path</code> <code>str</code> <p>The folder for outputting track clips.</p> required <code>tracks</code> <code>DataFrame</code> <p>The dataframe of tracks. If None, reads from track_file.</p> <code>None</code> <code>track_file</code> <code>str</code> <p>The track file if tracks is None.</p> <code>None</code> <code>method</code> <code>TrackClipMethod | str</code> <p>'all' (default) - all tracks 'random' - random select tracks 'specify' - specify track ids</p> <code>ALL</code> <code>random_number</code> <code>int</code> <p>The number of track ids if method == 'random'. Default is 10.</p> <code>10</code> <code>track_ids</code> <code>list</code> <p>The list of track ids if method == 'specify'.</p> <code>None</code> <code>start_frame_offset</code> <code>int</code> <p>The offset of start frame. Default is 0.</p> <code>0</code> <code>end_frame_offset</code> <code>int</code> <p>The offset of end frame. Default is 0.</p> <code>0</code> <code>tail_length</code> <code>int</code> <p>The tail length. Default is 0. Use <code>-1</code> to draw full history since the first frame of each track.</p> <code>0</code> <code>label_prefix</code> <code>bool</code> <p>If True, add the video file name as the prefix in output file names. Default is False.</p> <code>False</code> <code>size</code> <code>int</code> <p>Font size. Default is 1.</p> <code>1</code> <code>thick</code> <code>int</code> <p>Line thickness. Default is 1.</p> <code>1</code> <code>tail_size</code> <code>int</code> <p>Radius of tail dots. If None, an auto-scaled visible radius is used.</p> <code>None</code> <code>label_class</code> <code>bool</code> <p>Whether to include class in track labels. Default is False.</p> <code>False</code> <code>shapes</code> <code>list[dict]</code> <p>Shape overlays to add before rendering. See <code>draw_shapes</code> for the expected dict format.</p> <code>None</code> <code>color</code> <code>tuple[int, int, int] | str</code> <p>Clip color control: - None: default per-track color scheme - \"random\": random BGR color per clip - (b, g, r): fixed BGR color for all clips</p> <code>None</code> <code>fill</code> <code>bool</code> <p>Whether to fill generated track boxes. Default is False.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>Overlay strength for fill in [0, 1]. Default is 0.0.</p> <code>0.0</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, show progress bar. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def draw_track_clips(\n    self,\n    input_video: str,\n    output_path: str,\n    tracks: pd.DataFrame | None = None,\n    track_file: str | None = None,\n    method: TrackClipMethod | str = TrackClipMethod.ALL,\n    random_number: int = 10,\n    track_ids: list | None = None,\n    start_frame_offset: int = 0,\n    end_frame_offset: int = 0,\n    tail_length: int = 0,\n    label_prefix: bool = False,\n    label_class: bool = False,\n    shapes: list[dict] | None = None,\n    color: tuple[int, int, int] | str | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    size: int = 1,\n    thick: int = 1,\n    tail_size: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n):\n    \"\"\"Draw track clips from video.\n\n    Parameters\n    ----------\n    input_video : str\n        The raw video file.\n    output_path : str\n        The folder for outputting track clips.\n    tracks : pd.DataFrame, optional\n        The dataframe of tracks. If None, reads from track_file.\n    track_file : str, optional\n        The track file if tracks is None.\n    method : TrackClipMethod | str\n        'all' (default) - all tracks\n        'random' - random select tracks\n        'specify' - specify track ids\n    random_number : int\n        The number of track ids if method == 'random'. Default is 10.\n    track_ids : list, optional\n        The list of track ids if method == 'specify'.\n    start_frame_offset : int\n        The offset of start frame. Default is 0.\n    end_frame_offset : int\n        The offset of end frame. Default is 0.\n    tail_length : int\n        The tail length. Default is 0.\n        Use `-1` to draw full history since the first frame of each track.\n    label_prefix : bool\n        If True, add the video file name as the prefix in output file names. Default is False.\n    size : int\n        Font size. Default is 1.\n    thick : int\n        Line thickness. Default is 1.\n    tail_size : int, optional\n        Radius of tail dots. If None, an auto-scaled visible radius is used.\n    label_class : bool\n        Whether to include class in track labels. Default is False.\n    shapes : list[dict], optional\n        Shape overlays to add before rendering. See ``draw_shapes`` for\n        the expected dict format.\n    color : tuple[int, int, int] | str, optional\n        Clip color control:\n        - None: default per-track color scheme\n        - \"random\": random BGR color per clip\n        - (b, g, r): fixed BGR color for all clips\n    fill : bool\n        Whether to fill generated track boxes. Default is False.\n    alpha : float\n        Overlay strength for fill in [0, 1]. Default is 0.0.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n    verbose : bool\n        If True, show progress bar. Default is True.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    if tracks is None:\n        tracks = pd.read_csv(\n            track_file,\n            header=None,\n            dtype={0: int, 1: int, 2: int, 3: int, 4: int, 5: int, 6: float, 7: int, 8: int, 9: int},\n        )\n    tracks.columns = TRACK_COLUMNS\n\n    if isinstance(method, TrackClipMethod):\n        track_clip_method = method.value\n    else:\n        track_clip_method = str(method).lower().strip()\n        valid_methods = {m.value for m in TrackClipMethod}\n        if track_clip_method not in valid_methods:\n            raise ValueError(f\"Invalid method={method!r}. Choose one of {sorted(valid_methods)}.\")\n\n    if track_clip_method == TrackClipMethod.RANDOM.value:\n        track_ids = tracks[\"track\"].unique().tolist()\n        if random_number &lt;= 0:\n            random_number = 10\n        random_number = min(random_number, len(track_ids))\n        track_ids = random.sample(track_ids, random_number)\n    elif track_clip_method == TrackClipMethod.SPECIFY.value:\n        if (track_ids is None) or (len(track_ids) == 0):\n            print(\"No tracks are provided!\")\n            return pd.DataFrame()\n    else:\n        track_ids = tracks[\"track\"].unique().tolist()\n\n    # pbar = tqdm(total=len(track_ids), desc='Labeling tracks ', unit='clips')\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n\n    for id in track_ids:\n        selected_tracks = tracks[tracks[\"track\"] == id].copy()\n        start_frame = max(selected_tracks[\"frame\"].min() - start_frame_offset, 0)\n        end_frame = min(\n            selected_tracks[\"frame\"].max() + end_frame_offset,\n            frame_count - 1,\n        )\n        if label_prefix:\n            out_video = os.path.join(\n                output_path, os.path.splitext(os.path.basename(input_video))[0] + \"_\" + str(id) + \".mp4\"\n            )\n        else:\n            out_video = os.path.join(output_path, str(id) + \".mp4\")\n\n        if isinstance(color, str):\n            color_mode = color.lower().strip()\n            if color_mode == \"random\":\n                clip_color = random.choice(BRIGHT_COLORS_BGR)\n            elif color_mode in {\"none\", \"default\"}:\n                clip_color = None\n            else:\n                raise ValueError(\"color must be None, 'random', or a BGR tuple like (0, 255, 0).\")\n        elif color is None:\n            clip_color = None\n        else:\n            clip_color = self._normalize_color(color)\n\n        self.draw_tracks(\n            input_video=input_video,\n            output_video=out_video,\n            tracks=selected_tracks,\n            color=clip_color,\n            start_frame=start_frame,\n            end_frame=end_frame,\n            verbose=verbose,\n            tail_length=tail_length,\n            thick=thick,\n            size=size,\n            tail_size=tail_size,\n            label_class=label_class,\n            shapes=shapes,\n            fill=fill,\n            alpha=alpha,\n            video_index=video_index,\n            video_tot=video_tot,\n        )\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.draw_tracks","title":"draw_tracks","text":"<pre><code>draw_tracks(\n    input_video: str,\n    output_video: str,\n    tracks: DataFrame | None = None,\n    track_file: str | None = None,\n    label_file: str | None = None,\n    color: tuple[int, int, int] | None = None,\n    thick: int = 2,\n    size: int = 1,\n    tail_length: int = 0,\n    tail_size: int | None = None,\n    label_class: bool = False,\n    shapes: list[dict] | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n)\n</code></pre> <p>Draw tracks on video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to raw video file.</p> required <code>output_video</code> <code>str</code> <p>Path to output labeled video file.</p> required <code>tracks</code> <code>DataFrame</code> <p>DataFrame containing track data. If None, reads from track_file.</p> <code>None</code> <code>track_file</code> <code>str</code> <p>Path to track file if tracks is None.</p> <code>None</code> <code>label_file</code> <code>str</code> <p>Path to save label output.</p> <code>None</code> <code>color</code> <code>tuple[int, int, int]</code> <p>Custom color for drawing. If None, uses default colormap.</p> <code>None</code> <code>tail_length</code> <code>int</code> <p>Length of tail to display. Default is 0. Use <code>-1</code> to draw full history since the first frame of each track.</p> <code>0</code> <code>tail_size</code> <code>int</code> <p>Radius of tail dots. If None, an auto-scaled visible radius is used.</p> <code>None</code> <code>thick</code> <code>int</code> <p>Line thickness. Default is 2.</p> <code>2</code> <code>size</code> <code>int</code> <p>Font size. Default is 1.</p> <code>1</code> <code>label_class</code> <code>bool</code> <p>Whether to display class names. Default is False.</p> <code>False</code> <code>shapes</code> <code>list[dict]</code> <p>Shape overlays to add before rendering. See <code>draw_shapes</code> for the expected dict format.</p> <code>None</code> <code>fill</code> <code>bool</code> <p>Whether to fill generated track boxes. Default is False.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>Overlay strength for fill in [0, 1]. Default is 0.0.</p> <code>0.0</code> <code>start_frame</code> <code>int</code> <p>Starting frame. If None, defaults to 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Ending frame. If None, defaults to last frame.</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing generated draw elements (with DRAW_COLUMNS).</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def draw_tracks(\n    self,\n    input_video: str,\n    output_video: str,\n    tracks: pd.DataFrame | None = None,\n    track_file: str | None = None,\n    label_file: str | None = None,\n    color: tuple[int, int, int] | None = None,\n    thick: int = 2,\n    size: int = 1,\n    tail_length: int = 0,\n    tail_size: int | None = None,\n    label_class: bool = False,\n    shapes: list[dict] | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n    verbose: bool = True,\n):\n    \"\"\"Draw tracks on video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to raw video file.\n    output_video : str\n        Path to output labeled video file.\n    tracks : pd.DataFrame, optional\n        DataFrame containing track data. If None, reads from track_file.\n    track_file : str, optional\n        Path to track file if tracks is None.\n    label_file : str, optional\n        Path to save label output.\n    color : tuple[int, int, int], optional\n        Custom color for drawing. If None, uses default colormap.\n    tail_length : int\n        Length of tail to display. Default is 0.\n        Use `-1` to draw full history since the first frame of each track.\n    tail_size : int, optional\n        Radius of tail dots. If None, an auto-scaled visible radius is used.\n    thick : int\n        Line thickness. Default is 2.\n    size : int\n        Font size. Default is 1.\n    label_class : bool\n        Whether to display class names. Default is False.\n    shapes : list[dict], optional\n        Shape overlays to add before rendering. See ``draw_shapes`` for\n        the expected dict format.\n    fill : bool\n        Whether to fill generated track boxes. Default is False.\n    alpha : float\n        Overlay strength for fill in [0, 1]. Default is 0.0.\n    start_frame : int, optional\n        Starting frame. If None, defaults to 0.\n    end_frame : int, optional\n        Ending frame. If None, defaults to last frame.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n    verbose : bool\n        Whether to show progress. Default is True.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing generated draw elements (with DRAW_COLUMNS).\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    if tracks is None:\n        tracks = pd.read_csv(\n            track_file,\n            header=None,\n            dtype={0: int, 1: int, 2: int, 3: int, 4: int, 5: int, 6: float, 7: int, 8: int, 9: int},\n        )\n    tracks.columns = TRACK_COLUMNS\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    if start_frame is None:\n        start_frame = 0\n    if end_frame is None:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n\n    selected_tracks = tracks.loc[(tracks[\"frame\"] &gt;= start_frame) &amp; (tracks[\"frame\"] &lt;= end_frame)].copy()\n\n    pbar_desc = \"\"\n    if self.compress_message:\n        pbar_desc = \"Generating labels\"\n    else:\n        if video_index and video_tot:\n            pbar_desc = f\"Generating labels {video_index} of {video_tot}\"\n        else:\n            pbar_desc = f\"Generating labels {input_video} \"\n\n    names = load_classes()\n    pbar = tqdm(total=len(selected_tracks), unit=\" frames\", desc=pbar_desc, disable=not verbose)\n    tail_radius = max(1, int((size) * 1)) if tail_size is None else max(1, int(tail_size))\n    alpha = min(max(float(alpha), 0.0), 1.0)\n    results = []\n    for _, track in selected_tracks.iterrows():\n        final_color = _bright_color(int(track[\"track\"])) if color is None else color\n\n        track_id = int(track[\"track\"])\n        cls_desc = names[int(track[\"cls\"])] if label_class else \"\"\n        desc = f\"{track_id} {cls_desc}\"\n        results.append([\n            track[\"frame\"],\n            ElementType.BBOX.value,\n            [(track[\"x\"], track[\"y\"]), (track[\"x\"] + track[\"w\"], track[\"y\"] + track[\"h\"])],\n            final_color,\n            size,\n            thick,\n            desc,\n            fill,\n            alpha,\n        ])\n        if tail_length != 0:\n            current_frame = int(track[\"frame\"])\n            if tail_length == -1:\n                pre_boxes = tracks.loc[\n                    (tracks[\"track\"] == track[\"track\"]) &amp; (tracks[\"frame\"] &lt; current_frame)\n                ].values.tolist()\n            elif tail_length &gt; 0:\n                frames = [*range(current_frame - tail_length, current_frame)]\n                pre_boxes = tracks.loc[\n                    (tracks[\"frame\"].isin(frames)) &amp; (tracks[\"track\"] == track[\"track\"])\n                ].values.tolist()\n            else:\n                pre_boxes = []\n\n            if len(pre_boxes) &gt; 0:\n                for pre_box in pre_boxes:\n                    xc = int(pre_box[2]) + int(pre_box[4] / 2)\n                    yc = int(pre_box[3]) + int(pre_box[5] / 2)\n                    results.append([\n                        track[\"frame\"],\n                        ElementType.CIRCLE.value,\n                        [(xc, yc)],\n                        final_color,\n                        tail_radius,\n                        -1,\n                        \"\",\n                        fill,\n                        alpha,\n                    ])\n\n        pbar.update()\n\n    pbar.close()\n\n    results.sort()\n    results = list(results for results, _ in itertools.groupby(results))\n    df = pd.DataFrame(results, columns=DRAW_COLUMNS)\n    df = self._apply_shapes_with_draw_shapes(df, shapes)\n    df.sort_values(by=\"frame\", inplace=True)\n\n    if output_video:\n        self.draw(\n            input_video=input_video,\n            output_video=output_video,\n            draws=df,\n            start_frame=start_frame,\n            end_frame=end_frame,\n            video_index=video_index,\n            video_tot=video_tot,\n            verbose=verbose,\n        )\n\n    if label_file:\n        df.to_csv(label_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.draw_dets","title":"draw_dets","text":"<pre><code>draw_dets(\n    input_video: str,\n    output_video: str,\n    dets: DataFrame | None = None,\n    det_file: str | None = None,\n    label_file: str | None = None,\n    color: tuple[int, int, int] | None = None,\n    thick: int = 2,\n    size: int = 1,\n    label_score: bool = True,\n    shapes: list[dict] | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n)\n</code></pre> <p>Draw detections on video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to raw video file.</p> required <code>output_video</code> <code>str</code> <p>Path to output labeled video file.</p> required <code>dets</code> <code>DataFrame</code> <p>DataFrame containing detection data. If None, reads from det_file.</p> <code>None</code> <code>det_file</code> <code>str</code> <p>Path to detection file if dets is None.</p> <code>None</code> <code>label_file</code> <code>str</code> <p>Path to save label output.</p> <code>None</code> <code>color</code> <code>tuple[int, int, int]</code> <p>Custom color for drawing. If None, uses default colormap.</p> <code>None</code> <code>thick</code> <code>int</code> <p>Line thickness. Default is 2.</p> <code>2</code> <code>size</code> <code>int</code> <p>Font size. Default is 1.</p> <code>1</code> <code>label_score</code> <code>bool</code> <p>Whether to display detection scores. Default is True.</p> <code>True</code> <code>shapes</code> <code>list[dict]</code> <p>Shape overlays to add before rendering. See <code>draw_shapes</code> for the expected dict format.</p> <code>None</code> <code>fill</code> <code>bool</code> <p>Whether to fill generated detection boxes. Default is False.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>Overlay strength for fill in [0, 1]. Default is 0.0.</p> <code>0.0</code> <code>start_frame</code> <code>int</code> <p>Starting frame. If None, defaults to 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Ending frame. If None, defaults to last frame.</p> <code>None</code> <code>video_index</code> <code>int</code> <p>Video index for batch processing display.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total video count for batch processing display.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing generated draw elements (with DRAW_COLUMNS).</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def draw_dets(\n    self,\n    input_video: str,\n    output_video: str,\n    dets: pd.DataFrame | None = None,\n    det_file: str | None = None,\n    label_file: str | None = None,\n    color: tuple[int, int, int] | None = None,\n    thick: int = 2,\n    size: int = 1,\n    label_score: bool = True,\n    shapes: list[dict] | None = None,\n    fill: bool = False,\n    alpha: float = 0.0,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n):\n    \"\"\"Draw detections on video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to raw video file.\n    output_video : str\n        Path to output labeled video file.\n    dets : pd.DataFrame, optional\n        DataFrame containing detection data. If None, reads from det_file.\n    det_file : str, optional\n        Path to detection file if dets is None.\n    label_file : str, optional\n        Path to save label output.\n    color : tuple[int, int, int], optional\n        Custom color for drawing. If None, uses default colormap.\n    thick : int\n        Line thickness. Default is 2.\n    size : int\n        Font size. Default is 1.\n    label_score : bool\n        Whether to display detection scores. Default is True.\n    shapes : list[dict], optional\n        Shape overlays to add before rendering. See ``draw_shapes`` for\n        the expected dict format.\n    fill : bool\n        Whether to fill generated detection boxes. Default is False.\n    alpha : float\n        Overlay strength for fill in [0, 1]. Default is 0.0.\n    start_frame : int, optional\n        Starting frame. If None, defaults to 0.\n    end_frame : int, optional\n        Ending frame. If None, defaults to last frame.\n    video_index : int, optional\n        Video index for batch processing display.\n    video_tot : int, optional\n        Total video count for batch processing display.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing generated draw elements (with DRAW_COLUMNS).\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    if dets is None:\n        dets = pd.read_csv(det_file, header=None)\n\n    names = load_classes()\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    if start_frame is None:\n        start_frame = 0\n    if end_frame is None:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n\n    selected_dets = dets.loc[(dets[0] &gt;= start_frame) &amp; (dets[0] &lt;= end_frame)].copy()\n\n    pbar = tqdm(total=len(selected_dets), unit=\" dets\")\n    if self.compress_message:\n        pbar.set_description_str(\"Generating labels\")\n    else:\n        if video_index and video_tot:\n            pbar.set_description_str(f\"Generating labels {video_index} of {video_tot}\")\n        else:\n            pbar.set_description_str(f\"Generating labels {input_video} \")\n\n    results = []\n    alpha = min(max(float(alpha), 0.0), 1.0)\n    for _, det in selected_dets.iterrows():\n        final_color = _bright_color(int(det[7])) if color is None else color\n\n        desc = f\"{names[int(det[7])]} {det[6]:.1f}\" if label_score else str(int(det[7]))\n\n        results.append([\n            det[0],\n            ElementType.BBOX.value,\n            [(det[2], det[3]), (det[2] + det[4], det[3] + det[5])],\n            final_color,\n            size,\n            thick,\n            desc,\n            fill,\n            alpha,\n        ])\n        pbar.update()\n\n    results.sort()\n    results = list(results for results, _ in itertools.groupby(results))\n    df = pd.DataFrame(results, columns=DRAW_COLUMNS)\n    df = self._apply_shapes_with_draw_shapes(df, shapes)\n    df.sort_values(by=\"frame\", inplace=True)\n\n    if output_video:\n        self.draw(\n            input_video=input_video,\n            output_video=output_video,\n            draws=df,\n            start_frame=start_frame,\n            end_frame=end_frame,\n            video_index=video_index,\n            video_tot=video_tot,\n        )\n\n    if label_file:\n        df.to_csv(label_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.clip","title":"clip","text":"<pre><code>clip(\n    input_video: str,\n    output_video: str,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    method: LabelMethod | str | None = None,\n)\n</code></pre> <p>Extract a clip from the video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to input video file.</p> required <code>output_video</code> <code>str</code> <p>Path to output clipped video file.</p> required <code>start_frame</code> <code>int</code> <p>Starting frame. If None, defaults to 0.</p> <code>None</code> <code>end_frame</code> <code>int</code> <p>Ending frame. If None, defaults to last frame.</p> <code>None</code> <code>method</code> <code>LabelMethod | str</code> <p>Output backend for clipping. If None, uses the instance method. Supported: 'opencv', 'ffmpeg', 'chrome_safe'.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> <code>ValueError</code> <p>If frame range is invalid or video has no frames.</p> <code>RuntimeError</code> <p>If FFmpeg fails (when using ffmpeg or chrome_safe method).</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def clip(\n    self,\n    input_video: str,\n    output_video: str,\n    start_frame: int | None = None,\n    end_frame: int | None = None,\n    method: LabelMethod | str | None = None,\n):\n    \"\"\"Extract a clip from the video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to input video file.\n    output_video : str\n        Path to output clipped video file.\n    start_frame : int, optional\n        Starting frame. If None, defaults to 0.\n    end_frame : int, optional\n        Ending frame. If None, defaults to last frame.\n    method : LabelMethod | str, optional\n        Output backend for clipping. If None, uses the instance method.\n        Supported: 'opencv', 'ffmpeg', 'chrome_safe'.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n    ValueError\n        If frame range is invalid or video has no frames.\n    RuntimeError\n        If FFmpeg fails (when using ffmpeg or chrome_safe method).\n\n    \"\"\"\n    if method is None:\n        clip_method = self.method\n    elif isinstance(method, LabelMethod):\n        clip_method = method.value\n    else:\n        clip_method = str(method).lower().strip()\n        valid_methods = {m.value for m in LabelMethod}\n        if clip_method not in valid_methods:\n            raise ValueError(f\"Invalid method={method!r}. Choose one of {sorted(valid_methods)}.\")\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if total_frames &lt;= 0:\n        cap.release()\n        raise ValueError(f\"No frames available in video: {input_video}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps &lt;= 0:\n        cap.release()\n        raise ValueError(f\"Invalid FPS ({fps}) for video: {input_video}\")\n\n    if start_frame is None:\n        start_frame = 0\n\n    if end_frame is None:\n        end_frame = total_frames - 1\n\n    start_frame = max(0, min(int(start_frame), total_frames - 1))\n    end_frame = max(0, min(int(end_frame), total_frames - 1))\n    if end_frame &lt; start_frame:\n        end_frame = start_frame\n\n    tot_frames = end_frame - start_frame + 1\n    fps = int(fps)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    output_dir = os.path.dirname(output_video)\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Keep clip output bitrate aligned with source when possible.\n    source_bitrate_bps: int | None = None\n    try:\n        probe = subprocess.run(\n            [\n                \"ffprobe\",\n                \"-v\",\n                \"error\",\n                \"-select_streams\",\n                \"v:0\",\n                \"-show_entries\",\n                \"stream=bit_rate\",\n                \"-of\",\n                \"default=nokey=1:noprint_wrappers=1\",\n                input_video,\n            ],\n            check=False,\n            capture_output=True,\n            text=True,\n        )\n        if probe.returncode == 0:\n            value = probe.stdout.strip()\n            if value.isdigit():\n                source_bitrate_bps = int(value)\n    except FileNotFoundError:\n        source_bitrate_bps = None\n\n    if source_bitrate_bps is None:\n        cap_bitrate_kbps = cap.get(cv2.CAP_PROP_BITRATE)\n        if cap_bitrate_kbps and cap_bitrate_kbps &gt; 0:\n            source_bitrate_bps = int(cap_bitrate_kbps * 1000)\n\n    bitrate_args: list[str] = []\n    if source_bitrate_bps and source_bitrate_bps &gt; 0:\n        bitrate_args = [\n            \"-b:v\",\n            str(source_bitrate_bps),\n            \"-minrate\",\n            str(source_bitrate_bps),\n            \"-maxrate\",\n            str(source_bitrate_bps),\n            \"-bufsize\",\n            str(source_bitrate_bps * 2),\n        ]\n\n    use_ffmpeg = clip_method in {LabelMethod.FFMPEG.value, LabelMethod.CHROME_SAFE.value}\n    process = None\n    writer = None\n\n    if clip_method == LabelMethod.FFMPEG.value:\n        rate_control_args = bitrate_args if bitrate_args else [\"-crf\", str(self.crf)]\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-f\",\n            \"rawvideo\",\n            \"-vcodec\",\n            \"rawvideo\",\n            \"-pix_fmt\",\n            self.pix_fmt,\n            \"-s\",\n            f\"{width}x{height}\",\n            \"-r\",\n            str(fps),\n            \"-i\",\n            \"-\",\n            \"-c:v\",\n            self.encoder,\n            \"-preset\",\n            self.preset,\n            *rate_control_args,\n            output_video,\n        ]\n        process = subprocess.Popen(\n            ffmpeg_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE\n        )\n    elif clip_method == LabelMethod.CHROME_SAFE.value:\n        rate_control_args = bitrate_args if bitrate_args else [\"-crf\", \"23\"]\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-f\",\n            \"rawvideo\",\n            \"-vcodec\",\n            \"rawvideo\",\n            \"-pix_fmt\",\n            self.pix_fmt,\n            \"-s\",\n            f\"{width}x{height}\",\n            \"-r\",\n            str(fps),\n            \"-i\",\n            \"-\",\n            \"-c:v\",\n            \"libx264\",\n            \"-profile:v\",\n            \"high\",\n            \"-level\",\n            \"4.0\",\n            \"-preset\",\n            \"medium\",\n            *rate_control_args,\n            \"-pix_fmt\",\n            \"yuv420p\",\n            \"-movflags\",\n            \"+faststart\",\n            \"-an\",\n            output_video,\n        ]\n        process = subprocess.Popen(\n            ffmpeg_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE\n        )\n    else:\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n\n    pbar = tqdm(total=tot_frames, unit=\" frames\")\n    if self.compress_message:\n        pbar.set_description_str(\"Cutting\")\n    else:\n        pbar.set_description_str(f\"Cutting {input_video} \")\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n    while cap.isOpened():\n        pos_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n        ret, frame = cap.read()\n        if (not ret) or (pos_frame &gt; end_frame):\n            break\n\n        if use_ffmpeg:\n            try:\n                process.stdin.write(frame.tobytes())\n            except BrokenPipeError as exc:\n                ffmpeg_error = process.stderr.read().decode(\"utf-8\", errors=\"replace\") if process.stderr else \"\"\n                raise RuntimeError(\n                    f\"FFmpeg pipe closed early while clipping frame {pos_frame}.\\n{ffmpeg_error}\"\n                ) from exc\n        else:\n            writer.write(frame)\n\n        pbar.update()\n\n    pbar.close()\n    cap.release()\n    if use_ffmpeg:\n        if process.stdin:\n            process.stdin.close()\n        return_code = process.wait()\n        if return_code != 0:\n            ffmpeg_error = process.stderr.read().decode(\"utf-8\", errors=\"replace\") if process.stderr else \"\"\n            raise RuntimeError(f\"FFmpeg failed with exit code {return_code}.\\n{ffmpeg_error}\")\n    else:\n        writer.release()\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.clip_by_time","title":"clip_by_time","text":"<pre><code>clip_by_time(\n    input_video: str,\n    output_video: str,\n    start_sec: float = 0.0,\n    clip_len_sec: float | None = None,\n    method: LabelMethod | str | None = None,\n)\n</code></pre> <p>Extract a clip by time range.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to input video file.</p> required <code>output_video</code> <code>str</code> <p>Path to output clipped video file.</p> required <code>start_sec</code> <code>float</code> <p>Starting second. Negative values are treated as 0.</p> <code>0.0</code> <code>clip_len_sec</code> <code>float</code> <p>Clip length in seconds. If None, clip runs to end of video.</p> <code>None</code> <code>method</code> <code>LabelMethod | str</code> <p>Output backend for clipping. If None, uses the instance method. Supported: 'opencv', 'ffmpeg', 'chrome_safe'.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> <code>ValueError</code> <p>If the video has no frames, FPS is invalid, or clip_len_sec &lt;= 0.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>def clip_by_time(\n    self,\n    input_video: str,\n    output_video: str,\n    start_sec: float = 0.0,\n    clip_len_sec: float | None = None,\n    method: LabelMethod | str | None = None,\n):\n    \"\"\"Extract a clip by time range.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to input video file.\n    output_video : str\n        Path to output clipped video file.\n    start_sec : float\n        Starting second. Negative values are treated as 0.\n    clip_len_sec : float, optional\n        Clip length in seconds. If None, clip runs to end of video.\n    method : LabelMethod | str, optional\n        Output backend for clipping. If None, uses the instance method.\n        Supported: 'opencv', 'ffmpeg', 'chrome_safe'.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n    ValueError\n        If the video has no frames, FPS is invalid, or clip_len_sec &lt;= 0.\n\n    \"\"\"\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if total_frames &lt;= 0:\n        cap.release()\n        raise ValueError(f\"No frames available in video: {input_video}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps &lt;= 0:\n        cap.release()\n        raise ValueError(f\"Invalid FPS ({fps}) for video: {input_video}\")\n\n    start_frame = int(max(0.0, start_sec) * fps)\n    if clip_len_sec is None:\n        end_frame = total_frames - 1\n    else:\n        if clip_len_sec &lt;= 0:\n            cap.release()\n            raise ValueError(f\"clip_len_sec must be &gt; 0, got {clip_len_sec}.\")\n        clip_len_frames = max(1, round(clip_len_sec * fps))\n        end_frame = start_frame + clip_len_frames - 1\n\n    cap.release()\n\n    self.clip(\n        input_video=input_video,\n        output_video=output_video,\n        start_frame=start_frame,\n        end_frame=end_frame,\n        method=method,\n    )\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.export_frames","title":"export_frames  <code>staticmethod</code>","text":"<pre><code>export_frames(\n    input_video: str,\n    frames: list[int],\n    output_path: str,\n    prefix: str | None = None,\n)\n</code></pre> <p>Extract specific frames from video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to input video file.</p> required <code>frames</code> <code>list[int]</code> <p>List of frame numbers to extract.</p> required <code>output_path</code> <code>str</code> <p>Path to output directory for extracted frames.</p> required <code>prefix</code> <code>str</code> <p>Prefix to add to output filenames. If None, no prefix is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>@staticmethod\ndef export_frames(input_video: str, frames: list[int], output_path: str, prefix: str | None = None):\n    \"\"\"Extract specific frames from video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to input video file.\n    frames : list[int]\n        List of frame numbers to extract.\n    output_path : str\n        Path to output directory for extracted frames.\n    prefix : str, optional\n        Prefix to add to output filenames. If None, no prefix is used.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    pbar = tqdm(total=len(frames), unit=\" frames\")\n    pbar.set_description_str(\"Extracting frame\")\n\n    for frame in frames:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n        ret, frame_read = cap.read()\n\n        if prefix is None:\n            frame_file = os.path.join(output_path, str(frame) + \".jpg\")\n        else:\n            frame_file = os.path.join(output_path, prefix + \"-\" + str(frame) + \".jpg\")\n\n        if ret:\n            cv2.imwrite(frame_file, frame_read)\n        else:\n            break\n\n        pbar.update()\n\n    pbar.close()\n    cap.release()\n\n    print(f\"Writing frames to {output_path}\")\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.export_track_frames","title":"export_track_frames  <code>staticmethod</code>","text":"<pre><code>export_track_frames(\n    input_video: str,\n    tracks: DataFrame,\n    output_path: str,\n    bbox: bool = True,\n    prefix: str | None = None,\n    thick: int = 2,\n)\n</code></pre> <p>Extract frames for each track from video.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to input video file.</p> required <code>tracks</code> <code>DataFrame</code> <p>DataFrame containing track data with TRACK_COLUMNS.</p> required <code>output_path</code> <code>str</code> <p>Path to output directory for extracted frames.</p> required <code>bbox</code> <code>bool</code> <p>Whether to draw bounding boxes. Default is True.</p> <code>True</code> <code>prefix</code> <code>str</code> <p>Prefix to add to output filenames. If None, no prefix is used.</p> <code>None</code> <code>thick</code> <code>int</code> <p>Line thickness for bounding boxes. Default is 2.</p> <code>2</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> <code>Exception</code> <p>If tracks DataFrame has invalid format.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>@staticmethod\ndef export_track_frames(\n    input_video: str,\n    tracks: pd.DataFrame,\n    output_path: str,\n    bbox: bool = True,\n    prefix: str | None = None,\n    thick: int = 2,\n):\n    \"\"\"Extract frames for each track from video.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to input video file.\n    tracks : pd.DataFrame\n        DataFrame containing track data with TRACK_COLUMNS.\n    output_path : str\n        Path to output directory for extracted frames.\n    bbox : bool\n        Whether to draw bounding boxes. Default is True.\n    prefix : str, optional\n        Prefix to add to output filenames. If None, no prefix is used.\n    thick : int\n        Line thickness for bounding boxes. Default is 2.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n    Exception\n        If tracks DataFrame has invalid format.\n\n    \"\"\"\n    if (tracks is None) or (len(tracks.columns) &lt; 10):\n        raise ValueError(\"Invalid tracks: DataFrame must have at least 10 columns.\")\n    tracks.columns = TRACK_COLUMNS\n    ids = tracks[\"track\"].unique()\n    os.makedirs(output_path, exist_ok=True)\n\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    pbar = tqdm(total=len(ids), unit=\" frame\")\n    for id in ids:\n        pbar.set_description_str(\"Extracting track: \" + str(id))\n        selected = tracks[tracks[\"track\"] == id]\n        if len(selected) &gt; 0:\n            for _, track in selected.iterrows():\n                frame = track[\"frame\"]\n                cap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n                ret, img = cap.read()\n\n                if ret:\n                    if bbox:\n                        x1 = track[\"x\"]\n                        y1 = track[\"y\"]\n                        x2 = track[\"x\"] + track[\"w\"]\n                        y2 = track[\"y\"] + track[\"h\"]\n                        final_color = _bright_color(int(id))\n                        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), final_color, thick)\n\n                        if prefix is None:\n                            frame_file = os.path.join(output_path, str(id) + \"_\" + str(frame) + \".jpg\")\n                        else:\n                            frame_file = os.path.join(\n                                output_path, prefix + \"-\" + str(id) + \"_\" + str(frame) + \".jpg\"\n                            )\n\n                        cv2.imwrite(frame_file, img)\n                else:\n                    break\n\n            pbar.update()\n\n    pbar.close()\n    cap.release()\n\n    print(f\"Writing frames to {output_path}\")\n</code></pre>"},{"location":"api/label/labeler/#dnt.label.labeler.Labeler.time2frame","title":"time2frame  <code>staticmethod</code>","text":"<pre><code>time2frame(input_video: str, time: float)\n</code></pre> <p>Convert time in seconds to frame number.</p> <p>Parameters:</p> Name Type Description Default <code>input_video</code> <code>str</code> <p>Path to input video file.</p> required <code>time</code> <code>float</code> <p>Time in seconds.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Frame number corresponding to the given time.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the input video cannot be opened.</p> Source code in <code>src/dnt/label/labeler.py</code> <pre><code>@staticmethod\ndef time2frame(input_video: str, time: float):\n    \"\"\"Convert time in seconds to frame number.\n\n    Parameters\n    ----------\n    input_video : str\n        Path to input video file.\n    time : float\n        Time in seconds.\n\n    Returns\n    -------\n    int\n        Frame number corresponding to the given time.\n\n    Raises\n    ------\n    OSError\n        If the input video cannot be opened.\n\n    \"\"\"\n    cap = cv2.VideoCapture(input_video)\n    if not cap.isOpened():\n        raise OSError(\"Couldn't open webcam or video\")\n\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))  # original fps\n    frame = int(video_fps * time)\n    cap.release()\n    return frame\n</code></pre>"},{"location":"api/shared/","title":"Shared Utilities","text":""},{"location":"api/shared/download/","title":"Download","text":""},{"location":"api/shared/files/","title":"Files","text":"<p>File I/O utilities for reading and writing IOU and tracking data.</p> <p>This module provides functions to read and write CSV files containing IOU (Intersection over Union) metrics and tracking information.</p>"},{"location":"api/shared/files/#dnt.shared.files.read_iou","title":"read_iou","text":"<pre><code>read_iou(iou_file)\n</code></pre> <p>Read IOU metrics from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>iou_file</code> <code>str</code> <p>Path to the CSV file containing IOU metrics.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing IOU metrics with columns for video ID, detection ID, and IOU values.</p> Source code in <code>src/dnt/shared/files.py</code> <pre><code>def read_iou(iou_file):\n    \"\"\"Read IOU metrics from a CSV file.\n\n    Parameters\n    ----------\n    iou_file : str\n        Path to the CSV file containing IOU metrics.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing IOU metrics with columns for video ID, detection ID,\n        and IOU values.\n\n    \"\"\"\n    results = pd.read_csv(\n        iou_file, header=None, dtype={0: int, 1: int, 2: float, 3: float, 4: float, 5: float, 6: float, 7: int}\n    )\n    return results\n</code></pre>"},{"location":"api/shared/files/#dnt.shared.files.write_iou","title":"write_iou","text":"<pre><code>write_iou(ious, iou_file)\n</code></pre> <p>Write IOU metrics to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>ious</code> <code>DataFrame</code> <p>DataFrame containing IOU metrics to write.</p> required <code>iou_file</code> <code>str</code> <p>Path to the output CSV file.</p> required Source code in <code>src/dnt/shared/files.py</code> <pre><code>def write_iou(ious, iou_file):\n    \"\"\"Write IOU metrics to a CSV file.\n\n    Parameters\n    ----------\n    ious : pd.DataFrame\n        DataFrame containing IOU metrics to write.\n    iou_file : str\n        Path to the output CSV file.\n\n    \"\"\"\n    ious.to_csv(iou_file, header=False, index=False)\n</code></pre>"},{"location":"api/shared/files/#dnt.shared.files.read_track","title":"read_track","text":"<pre><code>read_track(track_file)\n</code></pre> <p>Read tracking data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>track_file</code> <code>str</code> <p>Path to the CSV file containing tracking information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing tracking data with video ID, track ID, coordinates, and additional tracking metrics.</p> Source code in <code>src/dnt/shared/files.py</code> <pre><code>def read_track(track_file):\n    \"\"\"Read tracking data from a CSV file.\n\n    Parameters\n    ----------\n    track_file : str\n        Path to the CSV file containing tracking information.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing tracking data with video ID, track ID, coordinates,\n        and additional tracking metrics.\n\n    \"\"\"\n    results = pd.read_csv(\n        track_file,\n        header=None,\n        dtype={0: int, 1: int, 2: float, 3: float, 4: float, 5: float, 6: float, 7: int, 8: int, 9: int},\n    )\n    return results\n</code></pre>"},{"location":"api/shared/files/#dnt.shared.files.write_track","title":"write_track","text":"<pre><code>write_track(tracks, track_file)\n</code></pre> <p>Write tracking data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame</code> <p>DataFrame containing tracking data to write.</p> required <code>track_file</code> <code>str</code> <p>Path to the output CSV file.</p> required Source code in <code>src/dnt/shared/files.py</code> <pre><code>def write_track(tracks, track_file):\n    \"\"\"Write tracking data to a CSV file.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame\n        DataFrame containing tracking data to write.\n    track_file : str\n        Path to the output CSV file.\n\n    \"\"\"\n    tracks.to_csv(track_file, header=False, index=False)\n</code></pre>"},{"location":"api/shared/files/#dnt.shared.files.read_spd","title":"read_spd","text":"<pre><code>read_spd(spd_file)\n</code></pre> <p>Read speed data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>spd_file</code> <code>str</code> <p>Path to the CSV file containing speed information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing speed data with columns for video ID, reference, and frame.</p> Source code in <code>src/dnt/shared/files.py</code> <pre><code>def read_spd(spd_file):\n    \"\"\"Read speed data from a CSV file.\n\n    Parameters\n    ----------\n    spd_file : str\n        Path to the CSV file containing speed information.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing speed data with columns for video ID, reference, and frame.\n\n    \"\"\"\n    results = pd.read_csv(spd_file, header=None, dtype={0: int, 1: int, 2: int})\n    return results\n</code></pre>"},{"location":"api/shared/synhcro/","title":"Synchronization","text":""},{"location":"api/shared/synhcro/#dnt.shared.synhcro.Synchronizer","title":"Synchronizer","text":"<pre><code>Synchronizer(\n    videos: List[str],\n    ref_frame: int,\n    ref_time: int,\n    ref_timezone: str = \"US/Eastern\",\n    offsets: Optional[List[int]] = None,\n)\n</code></pre> <p>Synchronizes multiple videos based on reference frame/time and offsets.</p> <p>Args:     videos: List of video file paths.     ref_frame: Reference frame number.     ref_time: Reference time in unix ms.     ref_timezone: Timezone string for reference time.     offsets: Optional list of frame offsets for each video.</p> Source code in <code>src/dnt/shared/synhcro.py</code> <pre><code>def __init__(\n    self,\n    videos: List[str],\n    ref_frame: int,\n    ref_time: int,  # unix ms\n    ref_timezone: str = \"US/Eastern\",\n    offsets: Optional[List[int]] = None,  # offsets in FRAMES\n) -&gt; None:\n    \"\"\"\n    Args:\n        videos: List of video file paths.\n        ref_frame: Reference frame number.\n        ref_time: Reference time in unix ms.\n        ref_timezone: Timezone string for reference time.\n        offsets: Optional list of frame offsets for each video.\n    \"\"\"\n    self.videos = videos\n    self.ref_frame = int(ref_frame)\n    self.ref_time = int(ref_time)\n    self.ref_tz_str = ref_timezone\n    self.ref_tz = timezone(ref_timezone)\n    self.offsets = offsets\n</code></pre>"},{"location":"api/shared/synhcro/#dnt.shared.synhcro.Synchronizer.process","title":"process","text":"<pre><code>process(\n    output_path: Optional[str] = None,\n    local: bool = False,\n    message: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Process all videos, synchronizing frames to unix time and optionally saving results.</p> <p>Args:     output_path: Optional directory to save CSVs.     local: If True, add local time column.     message: If True, print progress messages. Returns:     DataFrame with frame, unix_time, video, and optionally local_time.</p> Source code in <code>src/dnt/shared/synhcro.py</code> <pre><code>def process(self, output_path: Optional[str] = None, local: bool = False, message: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Process all videos, synchronizing frames to unix time and optionally saving results.\n\n    Args:\n        output_path: Optional directory to save CSVs.\n        local: If True, add local time column.\n        message: If True, print progress messages.\n    Returns:\n        DataFrame with frame, unix_time, video, and optionally local_time.\n    \"\"\"\n    if not self.videos:\n        return pd.DataFrame(columns=[\"frame\", \"unix_time\", \"video\"] + ([\"local_time\"] if local else []))\n\n    offsets = self.offsets if self.offsets is not None else [0] * len(self.videos)\n    if len(offsets) != len(self.videos):\n        raise ValueError(f\"offsets length ({len(offsets)}) must match videos length ({len(self.videos)})\")\n\n    results = []\n    ref_frame = self.ref_frame\n    ref_time = self.ref_time  # ms\n\n    for i, video in enumerate(self.videos):\n        df, fps = self.add_unix_time(\n            video, ref_frame, ref_time, message=message, video_index=i + 1, video_tot=len(self.videos)\n        )\n\n        if local:\n            # unix_time is epoch ms -&gt; convert (vectorized)\n            dt = pd.to_datetime(df[\"unix_time\"], unit=\"ms\", utc=True).dt.tz_convert(self.ref_tz_str)\n            df[\"local_time\"] = dt.astype(str)\n\n        results.append(df)\n\n        if output_path:\n            os.makedirs(output_path, exist_ok=True)\n            basename = os.path.splitext(os.path.basename(video))[0]\n            df.to_csv(os.path.join(output_path, f\"{basename}_time.csv\"), index=False)\n\n        # Prepare next video's reference time\n        if i &lt; len(self.videos) - 1:\n            next_video = self.videos[i + 1]\n            next_fps = Detector.get_fps(next_video)\n            if next_fps &lt;= 0:\n                raise ValueError(f\"fps is invalid for {next_video}\")\n\n            if df.empty:\n                raise ValueError(f\"DataFrame for video {video} is empty; cannot determine last unix_time.\")\n\n            ref_frame = 0\n\n    if not results:\n        return pd.DataFrame(columns=[\"frame\", \"unix_time\", \"video\"] + ([\"local_time\"] if local else []))\n\n    return pd.concat(results, ignore_index=True)\n</code></pre>"},{"location":"api/shared/synhcro/#dnt.shared.synhcro.Synchronizer.add_unix_time","title":"add_unix_time  <code>staticmethod</code>","text":"<pre><code>add_unix_time(\n    video: str,\n    ref_frame: int,\n    ref_time: int,\n    video_index: Optional[int] = None,\n    video_tot: Optional[int] = None,\n    message: bool = False,\n) -&gt; Tuple[pd.DataFrame, float]\n</code></pre> <p>Compute unix time for each frame in a video based on a reference frame and time.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Video file path.</p> required <code>ref_frame</code> <code>int</code> <p>Reference frame number.</p> required <code>ref_time</code> <code>int</code> <p>Reference time in unix milliseconds.</p> required <code>video_index</code> <code>int</code> <p>Index of the video for progress messages (default is None).</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total number of videos for progress messages (default is None).</p> <code>None</code> <code>message</code> <code>bool</code> <p>If True, print progress message (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple of (pd.DataFrame, float)</code> <p>DataFrame with columns [frame, unix_time, video], and the video's frames-per-second (fps).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fps is zero or negative, or if video frame count is invalid.</p> Source code in <code>src/dnt/shared/synhcro.py</code> <pre><code>@staticmethod\ndef add_unix_time(\n    video: str,\n    ref_frame: int,\n    ref_time: int,\n    video_index: Optional[int] = None,\n    video_tot: Optional[int] = None,\n    message: bool = False,\n) -&gt; Tuple[pd.DataFrame, float]:\n    \"\"\"\n    Compute unix time for each frame in a video based on a reference frame and time.\n\n    Parameters\n    ----------\n    video : str\n        Video file path.\n    ref_frame : int\n        Reference frame number.\n    ref_time : int\n        Reference time in unix milliseconds.\n    video_index : int, optional\n        Index of the video for progress messages (default is None).\n    video_tot : int, optional\n        Total number of videos for progress messages (default is None).\n    message : bool, optional\n        If True, print progress message (default is False).\n\n    Returns\n    -------\n    tuple of (pd.DataFrame, float)\n        DataFrame with columns [frame, unix_time, video], and the video's frames-per-second (fps).\n\n    Raises\n    ------\n    ValueError\n        If fps is zero or negative, or if video frame count is invalid.\n    \"\"\"\n    fps = Detector.get_fps(video)\n    ms_per_frame = 1000.0 / fps\n    n_frames = int(Detector.get_frames(video))\n    frames = np.arange(n_frames, dtype=np.int64)\n    unix_time = np.round(ref_time + (frames - int(ref_frame)) * ms_per_frame).astype(np.int64)\n    df = pd.DataFrame({\"frame\": frames, \"unix_time\": unix_time, \"video\": video})\n\n    if message:\n        label = f\"{video_index} of {video_tot}\" if (video_index and video_tot) else video\n        tqdm.write(f\"Synced {video} ({n_frames} frames @ {fps:.3f} fps) [{label}]\")\n\n    return df, fps\n</code></pre>"},{"location":"api/shared/synhcro/#dnt.shared.synhcro.Synchronizer.convert_unix_local","title":"convert_unix_local  <code>staticmethod</code>","text":"<pre><code>convert_unix_local(\n    unix_time: int, ref_timezone: str = \"US/Eastern\"\n) -&gt; str\n</code></pre> <p>Convert unix time in ms to ISO8601 string in the given timezone.</p> Source code in <code>src/dnt/shared/synhcro.py</code> <pre><code>@staticmethod\ndef convert_unix_local(unix_time: int, ref_timezone: str = \"US/Eastern\") -&gt; str:\n    \"\"\"\n    Convert unix time in ms to ISO8601 string in the given timezone.\n    \"\"\"\n    tz = timezone(ref_timezone)\n    return datetime.fromtimestamp(unix_time / 1000.0, tz).isoformat()\n</code></pre>"},{"location":"api/shared/util/","title":"Utilities","text":"<p>Utilities for loading class names and creating class mappings.</p> <p>Provides functions to load class definitions from text files and create dictionaries and lists for mapping class indices to names.</p>"},{"location":"api/shared/util/#dnt.shared.util.load_class_dict","title":"load_class_dict","text":"<pre><code>load_class_dict(\n    name_file: str | None = None,\n) -&gt; dict[str, int]\n</code></pre> <p>Load class names from file and create name-to-index mapping.</p> <p>Parameters:</p> Name Type Description Default <code>name_file</code> <code>str | None</code> <p>Path to class names file (one class per line). If None (default), uses 'coco.names' from the shared/data directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Mapping from class name (str) to class index (int). Example: {'person': 0, 'car': 2, 'dog': 16}</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the class names file does not exist.</p> <code>ValueError</code> <p>If the file is empty or cannot be parsed.</p> Notes <p>The class names file should contain one class name per line. The class index is determined by the line number (0-indexed). Empty lines are skipped.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class_dict = load_class_dict()\n&gt;&gt;&gt; idx = class_dict['person']\n&gt;&gt;&gt; print(idx)  # Output: 0\n</code></pre> Source code in <code>src/dnt/shared/util.py</code> <pre><code>def load_class_dict(name_file: str | None = None) -&gt; dict[str, int]:\n    \"\"\"Load class names from file and create name-to-index mapping.\n\n    Parameters\n    ----------\n    name_file : str | None, optional\n        Path to class names file (one class per line). If None (default),\n        uses 'coco.names' from the shared/data directory.\n\n    Returns\n    -------\n    dict[str, int]\n        Mapping from class name (str) to class index (int).\n        Example: {'person': 0, 'car': 2, 'dog': 16}\n\n    Raises\n    ------\n    FileNotFoundError\n        If the class names file does not exist.\n    ValueError\n        If the file is empty or cannot be parsed.\n\n    Notes\n    -----\n    The class names file should contain one class name per line.\n    The class index is determined by the line number (0-indexed).\n    Empty lines are skipped.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class_dict = load_class_dict()\n    &gt;&gt;&gt; idx = class_dict['person']\n    &gt;&gt;&gt; print(idx)  # Output: 0\n\n    \"\"\"\n    if not name_file:\n        lib_root = Path(__file__).resolve().parents[1]\n        name_file = lib_root / \"shared/data/coco.names\"\n    else:\n        name_file = Path(name_file)\n\n    if not name_file.exists():\n        raise FileNotFoundError(f\"Class names file not found: {name_file}\")\n\n    with name_file.open(\"r\", encoding=\"utf-8\") as f:\n        lines = [line.strip() for line in f.readlines()]\n\n    if not lines:\n        raise ValueError(f\"Class names file is empty: {name_file}\")\n\n    class_dict = {name: idx for idx, name in enumerate(lines) if name}\n    return class_dict\n</code></pre>"},{"location":"api/shared/util/#dnt.shared.util.load_classes","title":"load_classes","text":"<pre><code>load_classes(name_file: str | None = None) -&gt; list[str]\n</code></pre> <p>Load class names from file and return as list.</p> <p>Parameters:</p> Name Type Description Default <code>name_file</code> <code>str | None</code> <p>Path to class names file (one class per line). If None (default), uses 'coco.names' from the shared/data directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of class names in order. Empty strings are filtered out. Example: ['person', 'bicycle', 'car', ..., 'toothbrush']</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the class names file does not exist.</p> <code>ValueError</code> <p>If the file is empty.</p> Notes <p>The class index in the returned list corresponds to the order of class names in the input file (0-indexed). Lines with only whitespace are treated as empty and excluded from the result.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; classes = load_classes()\n&gt;&gt;&gt; print(classes[0])  # Output: 'person'\n&gt;&gt;&gt; print(len(classes))  # Output: 80 (for COCO)\n</code></pre> Source code in <code>src/dnt/shared/util.py</code> <pre><code>def load_classes(name_file: str | None = None) -&gt; list[str]:\n    \"\"\"Load class names from file and return as list.\n\n    Parameters\n    ----------\n    name_file : str | None, optional\n        Path to class names file (one class per line). If None (default),\n        uses 'coco.names' from the shared/data directory.\n\n    Returns\n    -------\n    list[str]\n        List of class names in order. Empty strings are filtered out.\n        Example: ['person', 'bicycle', 'car', ..., 'toothbrush']\n\n    Raises\n    ------\n    FileNotFoundError\n        If the class names file does not exist.\n    ValueError\n        If the file is empty.\n\n    Notes\n    -----\n    The class index in the returned list corresponds to the order of class\n    names in the input file (0-indexed). Lines with only whitespace are\n    treated as empty and excluded from the result.\n\n    Examples\n    --------\n    &gt;&gt;&gt; classes = load_classes()\n    &gt;&gt;&gt; print(classes[0])  # Output: 'person'\n    &gt;&gt;&gt; print(len(classes))  # Output: 80 (for COCO)\n\n    \"\"\"\n    if not name_file:\n        lib_root = Path(__file__).resolve().parents[1]\n        name_file = lib_root / \"shared/data/coco.names\"\n    else:\n        name_file = Path(name_file)\n\n    if not name_file.exists():\n        raise FileNotFoundError(f\"Class names file not found: {name_file}\")\n\n    with name_file.open(\"r\", encoding=\"utf-8\") as f:\n        results = [line.strip() for line in f.readlines() if line.strip()]\n\n    if not results:\n        raise ValueError(f\"Class names file is empty: {name_file}\")\n\n    return results\n</code></pre>"},{"location":"api/track/","title":"Tracking","text":"<p>Track module for detection tracking functionality.</p> <p>This module provides tracking utilities and classes for object detection.</p>"},{"location":"api/track/#dnt.track.ReClass","title":"ReClass","text":"<pre><code>ReClass(\n    num_frames: int = 25,\n    threshold: float = 0.75,\n    model: str = \"rtdetr\",\n    weights: str = \"x\",\n    device: str = \"auto\",\n    default_class: int = 0,\n    match_class: list | None = None,\n)\n</code></pre> <p>ReClass is responsible for re-classifying object tracks in video frames using detection results.</p> <p>Attributes:</p> Name Type Description <code>detector</code> <code>Detector</code> <p>The detection model used for re-classification.</p> <code>num_frames</code> <code>int</code> <p>Number of frames to consider for re-classification.</p> <code>threshold</code> <code>float</code> <p>Threshold for matching detections to tracks.</p> <code>default_class</code> <code>int</code> <p>Default class to assign if no match is found.</p> <code>match_class</code> <code>list</code> <p>List of classes to match during re-classification.</p> <p>Methods:</p> Name Description <code>match_mmv</code> <p>Matches a track to detections and computes the average score.</p> <code>re_classify</code> <p>tracks: pd.DataFrame, input_video: str, track_ids: list = None, out_file: str = None, verbose: bool = True</p> <code>) -&gt; pd.DataFrame</code> <p>Re-classifies tracks and returns a DataFrame with results.</p> <p>Re-classify tracks based on detection results.</p> <p>Parameters:</p> Name Type Description Default <code>num_frames</code> <code>int</code> <p>Number of frames to consider for re-classification, default 25</p> <code>25</code> <code>threshold</code> <code>float</code> <p>Threshold for matching, default 0.75</p> <code>0.75</code> <code>model</code> <code>str</code> <p>Detection model to use, default 'rtdetr'</p> <code>'rtdetr'</code> <code>weights</code> <code>str</code> <p>Weights for the detection model, default 'x'</p> <code>'x'</code> <code>device</code> <code>str</code> <p>Device to use for detection, default 'auto'</p> <code>'auto'</code> <code>default_class</code> <code>int</code> <p>Default class to assign if no match found, default 0 (pedestrian)</p> <code>0</code> <code>match_class</code> <code>list</code> <p>List of classes to match, default [1, 36] (bicycle, skateboard/scooter)</p> <code>None</code> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def __init__(\n    self,\n    num_frames: int = 25,\n    threshold: float = 0.75,\n    model: str = \"rtdetr\",\n    weights: str = \"x\",\n    device: str = \"auto\",\n    default_class: int = 0,\n    match_class: list | None = None,\n) -&gt; None:\n    \"\"\"Re-classify tracks based on detection results.\n\n    Parameters\n    ----------\n    num_frames : int\n        Number of frames to consider for re-classification, default 25\n    threshold : float\n        Threshold for matching, default 0.75\n    model : str\n        Detection model to use, default 'rtdetr'\n    weights : str\n        Weights for the detection model, default 'x'\n    device : str\n        Device to use for detection, default 'auto'\n    default_class : int\n        Default class to assign if no match found, default 0 (pedestrian)\n    match_class : list\n        List of classes to match, default [1, 36] (bicycle, skateboard/scooter)\n\n    \"\"\"\n    self.detector = Detector(model=model, device=device)\n    self.num_frames = num_frames\n    self.threshold = threshold\n    self.default_class = default_class\n    self.match_class = match_class if match_class is not None else [1, 36]\n</code></pre>"},{"location":"api/track/#dnt.track.ReClass.match_mmv","title":"match_mmv","text":"<pre><code>match_mmv(\n    track: DataFrame, dets: DataFrame\n) -&gt; tuple[bool, float]\n</code></pre> <p>Match track bboxes to detection bboxes and compute average overlap score.</p> <p>Parameters:</p> Name Type Description Default <code>track</code> <code>DataFrame</code> <p>DataFrame containing track data with columns [x, y, w, h, frame].</p> required <code>dets</code> <code>DataFrame</code> <p>DataFrame containing detection data with columns [x, y, w, h, frame, class].</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>A tuple (hit, avg_score) where: - hit : bool     True if average overlap score meets threshold, False otherwise. - avg_score : float     Average Intersection over Box (IoB) score across all matched detections.</p> Notes <p>Only frames present in both track and detection datasets are considered. The matching uses IoB metric from the engine.iob module.</p> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def match_mmv(self, track: pd.DataFrame, dets: pd.DataFrame) -&gt; tuple[bool, float]:\n    \"\"\"Match track bboxes to detection bboxes and compute average overlap score.\n\n    Parameters\n    ----------\n    track : pd.DataFrame\n        DataFrame containing track data with columns [x, y, w, h, frame].\n    dets : pd.DataFrame\n        DataFrame containing detection data with columns [x, y, w, h, frame, class].\n\n    Returns\n    -------\n    tuple[bool, float]\n        A tuple (hit, avg_score) where:\n        - hit : bool\n            True if average overlap score meets threshold, False otherwise.\n        - avg_score : float\n            Average Intersection over Box (IoB) score across all matched detections.\n\n    Notes\n    -----\n    Only frames present in both track and detection datasets are considered.\n    The matching uses IoB metric from the engine.iob module.\n\n    \"\"\"\n    if track.empty or dets.empty:\n        return False, 0.0\n\n    score = 0.0\n    cnt = 0\n    for _, row in track.iterrows():\n        bboxes = row[[\"x\", \"y\", \"w\", \"h\"]].values.reshape(1, -1)\n        det = dets[dets[\"frame\"] == row[\"frame\"]]\n        if len(det) &gt; 0:\n            match_bboxes = det[[\"x\", \"y\", \"w\", \"h\"]].values\n            _, overlaps_mmv = iobs(bboxes, match_bboxes)\n            if overlaps_mmv.size &gt; 0:\n                max_overlap = np.max(overlaps_mmv)\n                if max_overlap &gt;= self.threshold:\n                    score += max_overlap\n                    cnt += 1\n\n    avg_score = score / cnt if cnt &gt; 0 else 0.0\n    hit = avg_score &gt;= self.threshold\n\n    return hit, avg_score\n</code></pre>"},{"location":"api/track/#dnt.track.ReClass.re_classify","title":"re_classify","text":"<pre><code>re_classify(\n    tracks: DataFrame,\n    input_video: str,\n    track_ids: list | None = None,\n    out_file: str | None = None,\n    verbose: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Re-classify tracks using detection matching against reference image frame samples.</p> <p>For each track, extracts the top N largest frames (by area), runs detection on those frames, and matches detections against the track bboxes using IoB metric. Assigns the highest-scoring match class if confidence exceeds self.threshold.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame</code> <p>Input tracks DataFrame with required columns: track, x, y, w, h, frame. Additional columns are preserved in output.</p> required <code>input_video</code> <code>str</code> <p>Path to source video file from which to extract frame samples.</p> required <code>track_ids</code> <code>list | None</code> <p>List of track IDs to re-classify. If None (default), all tracks in the input are re-classified.</p> <code>None</code> <code>out_file</code> <code>str | None</code> <p>Path to save re-classified results as CSV. If None (default), results are not saved to file.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True (default), display progress bar during re-classification.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output DataFrame with columns [track, cls, avg_score] where: - track : int     Track ID from input tracks. - cls : int     Re-classified class ID. Set to default_class if no match found. - avg_score : float     Maximum IoB score among matched detections, rounded to 2 decimals.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input tracks DataFrame is empty.</p> <code>FileNotFoundError</code> <p>If input_video does not exist.</p> Notes <p>The method considers only the top N frames (self.num_frames) by bounding box area for computational efficiency. It matches detections from match_class list against track bboxes and selects the class with highest average score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from .re_class import ReClass\n&gt;&gt;&gt; tracks = pd.DataFrame({\n...     'frame': [0, 1, 2],\n...     'track': [1, 1, 1],\n...     'x': [100, 102, 104],\n...     'y': [50, 52, 54],\n...     'w': [50, 50, 50],\n...     'h': [100, 100, 100],\n... })\n&gt;&gt;&gt; rc = ReClass(num_frames=2, threshold=0.75, match_class=[1, 36])\n&gt;&gt;&gt; result = rc.re_classify(tracks, 'video.mp4')\n&gt;&gt;&gt; print(result)  # DataFrame with [track, cls, avg_score]\n</code></pre> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def re_classify(\n    self,\n    tracks: pd.DataFrame,\n    input_video: str,\n    track_ids: list | None = None,\n    out_file: str | None = None,\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Re-classify tracks using detection matching against reference image frame samples.\n\n    For each track, extracts the top N largest frames (by area), runs detection on\n    those frames, and matches detections against the track bboxes using IoB metric.\n    Assigns the highest-scoring match class if confidence exceeds self.threshold.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame\n        Input tracks DataFrame with required columns: track, x, y, w, h, frame.\n        Additional columns are preserved in output.\n    input_video : str\n        Path to source video file from which to extract frame samples.\n    track_ids : list | None, optional\n        List of track IDs to re-classify. If None (default), all tracks in\n        the input are re-classified.\n    out_file : str | None, optional\n        Path to save re-classified results as CSV. If None (default), results\n        are not saved to file.\n    verbose : bool, optional\n        If True (default), display progress bar during re-classification.\n\n    Returns\n    -------\n    pd.DataFrame\n        Output DataFrame with columns [track, cls, avg_score] where:\n        - track : int\n            Track ID from input tracks.\n        - cls : int\n            Re-classified class ID. Set to default_class if no match found.\n        - avg_score : float\n            Maximum IoB score among matched detections, rounded to 2 decimals.\n\n    Raises\n    ------\n    ValueError\n        If input tracks DataFrame is empty.\n    FileNotFoundError\n        If input_video does not exist.\n\n    Notes\n    -----\n    The method considers only the top N frames (self.num_frames) by bounding\n    box area for computational efficiency. It matches detections from match_class\n    list against track bboxes and selects the class with highest average score.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from .re_class import ReClass\n    &gt;&gt;&gt; tracks = pd.DataFrame({\n    ...     'frame': [0, 1, 2],\n    ...     'track': [1, 1, 1],\n    ...     'x': [100, 102, 104],\n    ...     'y': [50, 52, 54],\n    ...     'w': [50, 50, 50],\n    ...     'h': [100, 100, 100],\n    ... })\n    &gt;&gt;&gt; rc = ReClass(num_frames=2, threshold=0.75, match_class=[1, 36])\n    &gt;&gt;&gt; result = rc.re_classify(tracks, 'video.mp4')\n    &gt;&gt;&gt; print(result)  # DataFrame with [track, cls, avg_score]\n\n    \"\"\"\n    if tracks.empty:\n        raise ValueError(\"Input tracks DataFrame is empty.\")\n    if not Path(input_video).exists():\n        raise FileNotFoundError(f\"Video file not found: {input_video}\")\n\n    if track_ids is None:\n        track_ids = tracks[\"track\"].unique().tolist()\n\n    results = []\n    if verbose:\n        pbar = tqdm(total=len(track_ids), unit=\"track\", desc=\"Re-classifying tracks\")\n    for track_id in track_ids:\n        target_track = tracks[tracks[\"track\"] == track_id].copy()\n        target_track[\"area\"] = target_track[\"w\"] * target_track[\"h\"]\n        target_track.sort_values(by=\"area\", inplace=True, ascending=False)\n\n        top_frames = target_track.head(self.num_frames) if len(target_track) &gt;= self.num_frames else target_track\n\n        dets = self.detector.detect_frames(input_video, top_frames[\"frame\"].values.tolist())\n\n        matched = []\n        for cls in self.match_class:\n            match_dets = dets[dets[\"class\"] == cls]\n            hit, avg_score = self.match_mmv(top_frames, match_dets)\n            if hit:\n                matched.append((cls, avg_score))\n\n        if len(matched) &gt; 0:\n            cls, avg_score = max(matched, key=lambda x: x[1])\n        else:\n            cls = self.default_class\n            avg_score = 0\n\n        results.append([track_id, cls, round(avg_score, 2)])\n        if verbose:\n            pbar.update()\n    if verbose:\n        pbar.close()\n\n    df = pd.DataFrame(results, columns=[\"track\", \"cls\", \"avg_score\"])\n    if out_file:\n        df.to_csv(out_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/track/#dnt.track.BoostTrackConfig","title":"BoostTrackConfig  <code>dataclass</code>","text":"<pre><code>BoostTrackConfig(\n    model: MOTModels = MOTModels.BOOSTTRACK,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>BoostTrack-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this keeps only higher-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks alive longer when unmatched.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays confirmation and reduces short noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this demands tighter overlap to match detections.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/#dnt.track.BoostTrackConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.BoostTrackConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.BoostTrackConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.BoostTrackConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.BoostTrackConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.BOTSORTConfig","title":"BOTSORTConfig  <code>dataclass</code>","text":"<pre><code>BOTSORTConfig(\n    model: MOTModels = MOTModels.BOTSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    track_high_thresh: float = 0.5,\n    track_low_thresh: float = 0.1,\n    new_track_thresh: float = 0.6,\n    match_thresh: float = 0.8,\n    track_buffer: int = 30,\n    with_reid: bool = True,\n    proximity_thresh: float = 0.5,\n    appearance_thresh: float = 0.25,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>BoTSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: <code>None</code>, file name (auto-resolved), or absolute path. Built-in downloadable options: <code>'resnet50_market1501.pt'</code>, <code>'resnet50_dukemtmcreid.pt'</code>, <code>'resnet50_msmt17.pt'</code>, <code>'resnet50_fc512_market1501.pt'</code>, <code>'resnet50_fc512_dukemtmcreid.pt'</code>, <code>'resnet50_fc512_msmt17.pt'</code>, <code>'mlfn_market1501.pt'</code>, <code>'mlfn_dukemtmcreid.pt'</code>, <code>'mlfn_msmt17.pt'</code>, <code>'hacnn_market1501.pt'</code>, <code>'hacnn_dukemtmcreid.pt'</code>, <code>'hacnn_msmt17.pt'</code>, <code>'mobilenetv2_x1_0_market1501.pt'</code>, <code>'mobilenetv2_x1_0_dukemtmcreid.pt'</code>, <code>'mobilenetv2_x1_0_msmt17.pt'</code>, <code>'mobilenetv2_x1_4_market1501.pt'</code>, <code>'mobilenetv2_x1_4_dukemtmcreid.pt'</code>, <code>'mobilenetv2_x1_4_msmt17.pt'</code>, <code>'osnet_x1_0_market1501.pt'</code>, <code>'osnet_x1_0_dukemtmcreid.pt'</code>, <code>'osnet_x1_0_msmt17.pt'</code>, <code>'osnet_x0_75_market1501.pt'</code>, <code>'osnet_x0_75_dukemtmcreid.pt'</code>, <code>'osnet_x0_75_msmt17.pt'</code>, <code>'osnet_x0_5_market1501.pt'</code>, <code>'osnet_x0_5_dukemtmcreid.pt'</code>, <code>'osnet_x0_5_msmt17.pt'</code>, <code>'osnet_x0_25_market1501.pt'</code>, <code>'osnet_x0_25_dukemtmcreid.pt'</code>, <code>'osnet_x0_25_msmt17.pt'</code>, <code>'osnet_ibn_x1_0_msmt17.pt'</code>, <code>'osnet_ain_x1_0_msmt17.pt'</code>, <code>'lmbn_n_duke.pt'</code>, <code>'lmbn_n_market.pt'</code>, <code>'lmbn_n_cuhk03_d.pt'</code>, <code>'clip_market1501.pt'</code>, <code>'clip_duke.pt'</code>, <code>'clip_veri.pt'</code>, <code>'clip_vehicleid.pt'</code>. Suggestions: use <code>\"osnet_x1_0_msmt17.pt\"</code> for pedestrians or <code>\"clip_vehicleid.pt\"</code> / <code>\"clip_veri.pt\"</code> for vehicles.</p> <code>track_high_thresh</code> <code>float</code> <p>High score threshold for first association (default: <code>0.5</code>). Increasing this is stricter and may reduce false matches but miss tracks.</p> <code>track_low_thresh</code> <code>float</code> <p>Lower score threshold for second association (default: <code>0.1</code>). Increasing this keeps fewer low-confidence detections.</p> <code>new_track_thresh</code> <code>float</code> <p>Threshold to initialize new tracks (default: <code>0.6</code>). Increasing this creates fewer new tracks and can reduce false positives.</p> <code>match_thresh</code> <code>float</code> <p>Matching threshold for association (default: <code>0.8</code>). Increasing this makes association more permissive.</p> <code>track_buffer</code> <code>int</code> <p>Number of frames to keep lost tracks (default: <code>30</code>). Increasing this preserves IDs longer through occlusion, but may cause stale tracks to survive longer.</p> <code>with_reid</code> <code>bool</code> <p>Whether to enable ReID-assisted association (default: <code>True</code>). Options: <code>True</code>, <code>False</code>. Disabling this speeds up tracking but may increase ID switches.</p> <code>proximity_thresh</code> <code>float</code> <p>Proximity threshold for ReID matching (default: <code>0.5</code>). Increasing this requires stronger geometric overlap before ReID is used.</p> <code>appearance_thresh</code> <code>float</code> <p>Appearance similarity threshold for ReID matching (default: <code>0.25</code>). Increasing this requires closer appearance match and is more conservative.</p>"},{"location":"api/track/#dnt.track.BOTSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.BOTSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.BOTSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.BOTSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.BOTSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.ByteTrackConfig","title":"ByteTrackConfig  <code>dataclass</code>","text":"<pre><code>ByteTrackConfig(\n    model: MOTModels = MOTModels.BYTE_TRACK,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    track_thresh: float = 0.5,\n    match_thresh: float = 0.8,\n    track_buffer: int = 30,\n    frame_rate: int = 30,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>ByteTrack-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>track_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.5</code>). Increasing this filters more weak detections.</p> <code>match_thresh</code> <code>float</code> <p>Threshold for matching detections to tracks (default: <code>0.8</code>). Increasing this generally allows looser matching.</p> <code>track_buffer</code> <code>int</code> <p>Number of frames to keep lost tracks (default: <code>30</code>). Increasing this keeps unmatched tracks longer.</p> <code>frame_rate</code> <code>int</code> <p>Source video frame rate used by the tracker (default: <code>30</code>). Set this close to real FPS for best temporal behavior.</p>"},{"location":"api/track/#dnt.track.ByteTrackConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.ByteTrackConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.ByteTrackConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.ByteTrackConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.ByteTrackConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig","title":"DeepOCSORTConfig  <code>dataclass</code>","text":"<pre><code>DeepOCSORTConfig(\n    model: MOTModels = MOTModels.DEEPOCSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n    delta_t: int = 3,\n    inertia: float = 0.2,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>DeepOCSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this keeps fewer low-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps unmatched tracks alive longer.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays track confirmation.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this requires tighter overlap.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p> <code>delta_t</code> <code>int</code> <p>Time gap used by motion compensation (default: <code>3</code>). Increasing this smooths over longer temporal windows.</p> <code>inertia</code> <code>float</code> <p>Motion inertia weight (default: <code>0.2</code>). Increasing this emphasizes velocity continuity.</p>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.DeepOCSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.HybridSORTConfig","title":"HybridSORTConfig  <code>dataclass</code>","text":"<pre><code>HybridSORTConfig(\n    model: MOTModels = MOTModels.HYBRIDSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>HybridSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces weak detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks longer during occlusion.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces early noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this makes IoU matching stricter.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/#dnt.track.HybridSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.HybridSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.HybridSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.HybridSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.HybridSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.MOTBaseConfig","title":"MOTBaseConfig  <code>dataclass</code>","text":"<pre><code>MOTBaseConfig(\n    model: MOTModels = MOTModels.BOTSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n)\n</code></pre> <p>Common configuration fields for BoxMOT tracker creation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>MOTModels</code> <p>BoxMOT tracker backend for this parameter bundle (default: <code>MOTModels.BOTSORT</code>).</p> <code>per_class</code> <code>bool</code> <p>Whether to run tracking independently per class (default: <code>False</code>). Options: <code>True</code>, <code>False</code>. Setting <code>True</code> reduces cross-class ID switches but can create more tracks.</p> <code>extra_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs merged into tracker construction arguments (default: <code>{}</code>). Use this for BoxMOT arguments not explicitly represented in dataclasses.</p>"},{"location":"api/track/#dnt.track.MOTBaseConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.MOTBaseConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.MOTBaseConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.MOTBaseConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.MOTBaseConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.MOTModels","title":"MOTModels","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported tracker backends exposed by BoxMOT.</p> <p>Attributes:</p> Name Type Description <code>BOTSORT</code> <code>str</code> <p>BoT-SORT tracker name used by BoxMOT. Good default when you want motion + appearance matching.</p> <code>BOOSTTRACK</code> <code>str</code> <p>BoostTrack tracker name used by BoxMOT. Usually improves association under difficult motion/crowding.</p> <code>BYTE_TRACK</code> <code>str</code> <p>ByteTrack tracker name used by BoxMOT. Faster and simpler; does not require ReID weights.</p> <code>OCSORT</code> <code>str</code> <p>OCSORT tracker name used by BoxMOT. Motion-centric tracker; useful when appearance features are unreliable.</p> <code>STRONGSORT</code> <code>str</code> <p>StrongSORT tracker name used by BoxMOT. Appearance-heavy tracker; typically more robust to long occlusions.</p> <code>DEEPOCSORT</code> <code>str</code> <p>DeepOCSORT tracker name used by BoxMOT. OCSORT variant enhanced with appearance features.</p> <code>HYBRIDSORT</code> <code>str</code> <p>HybridSORT tracker name used by BoxMOT. Hybrid strategy between motion and appearance matching.</p> <code>SFSORT</code> <code>str</code> <p>SFSort tracker name used by BoxMOT. Lightweight motion-centric tracking for real-time pipelines.</p>"},{"location":"api/track/#dnt.track.OCSORTConfig","title":"OCSORTConfig  <code>dataclass</code>","text":"<pre><code>OCSORTConfig(\n    model: MOTModels = MOTModels.OCSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n    delta_t: int = 3,\n    inertia: float = 0.2,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>OCSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces low-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks alive through longer gaps.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces short-lived false tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this makes matching stricter.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p> <code>delta_t</code> <code>int</code> <p>Time gap used by motion compensation (default: <code>3</code>). Increasing this smooths longer motion history, but may lag quick turns.</p> <code>inertia</code> <code>float</code> <p>Motion inertia weight (default: <code>0.2</code>). Increasing this trusts previous velocity more.</p>"},{"location":"api/track/#dnt.track.OCSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.OCSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.OCSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.OCSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.OCSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.ReIDWeights","title":"ReIDWeights","text":"<p>               Bases: <code>StrEnum</code></p> <p>Built-in BoxMOT ReID weight file names.</p> <p>Use these enum values for <code>reid_weights</code> in tracker parameter dataclasses.</p>"},{"location":"api/track/#dnt.track.SFSORTConfig","title":"SFSORTConfig  <code>dataclass</code>","text":"<pre><code>SFSORTConfig(\n    model: MOTModels = MOTModels.SFSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>SFSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces weak detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks longer through brief misses.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces short-lived noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this requires tighter overlap for matching.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/#dnt.track.SFSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.SFSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.SFSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.SFSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.SFSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.StrongSORTConfig","title":"StrongSORTConfig  <code>dataclass</code>","text":"<pre><code>StrongSORTConfig(\n    model: MOTModels = MOTModels.STRONGSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    max_dist: float = 0.2,\n    max_iou_dist: float = 0.7,\n    max_age: int = 70,\n    n_init: int = 3,\n    nn_budget: int = 100,\n    ema_alpha: float = 0.9,\n    mc_lambda: float = 0.995,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>StrongSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>max_dist</code> <code>float</code> <p>Maximum cosine distance for appearance matching (default: <code>0.2</code>). Increasing this allows less similar appearance matches.</p> <code>max_iou_dist</code> <code>float</code> <p>Maximum IoU distance for geometric matching (default: <code>0.7</code>). Increasing this allows looser geometric matches.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>70</code>). Increasing this retains tracks through longer occlusions.</p> <code>n_init</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays confirmation and reduces unstable IDs.</p> <code>nn_budget</code> <code>int</code> <p>Maximum size of appearance feature gallery (default: <code>100</code>). Increasing this improves long-term matching memory at higher memory cost.</p> <code>ema_alpha</code> <code>float</code> <p>EMA factor for appearance embeddings (default: <code>0.9</code>). Increasing this smooths features more and reduces noise.</p> <code>mc_lambda</code> <code>float</code> <p>Motion compensation blending factor (default: <code>0.995</code>). Increasing this gives more weight to motion compensation.</p>"},{"location":"api/track/#dnt.track.StrongSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/#dnt.track.StrongSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/#dnt.track.StrongSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/#dnt.track.StrongSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.StrongSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker","title":"Tracker","text":"<pre><code>Tracker(\n    config: BoxMOTModelParams | None = None,\n    config_yaml: str | None = None,\n    device: str = \"auto\",\n    half: bool = False,\n    output_score_cls: bool = True,\n    boxmot_verbose: bool = False,\n)\n</code></pre> <p>Unified interface for BoxMOT tracking and track post-processing.</p> <p>This class runs BoxMOT tracking given a detection file and source video. It also provides post-processing utilities to infill missing frames, split tracks by large gaps, and drop short tracks.</p> <p>Attributes:</p> Name Type Description <code>TRACK_FIELDS</code> <code>list[str]</code> <p>Standard output columns for tracking and post-processing utilities (default: class constant).</p> <code>device</code> <code>str</code> <p>Device string used by deep trackers (default: <code>\"auto\"</code>). Options: <code>\"auto\"</code>, <code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code>.</p> <code>half</code> <code>bool</code> <p>Whether half precision is enabled for deep trackers (default: <code>False</code>). Options: <code>True</code>, <code>False</code>. Enabling can improve speed on supported GPUs.</p> <code>boxmot_model</code> <code>MOTModels</code> <p>Selected BoxMOT tracker backend (default: <code>MOTModels.BOTSORT</code>). Options: <code>MOTModels.BOTSORT</code>, <code>MOTModels.BOOSTTRACK</code>, <code>MOTModels.BYTE_TRACK</code>, <code>MOTModels.OCSORT</code>, <code>MOTModels.STRONGSORT</code>, <code>MOTModels.DEEPOCSORT</code>, <code>MOTModels.HYBRIDSORT</code>, <code>MOTModels.SFSORT</code>.</p> <code>boxmot_config</code> <code>BoxMOTModelConfig</code> <p>Configuration dataclass instance for BoxMOT tracker creation (default: model-specific defaults).</p> <code>boxmot_verbose</code> <code>bool</code> <p>If False, suppress BoxMOT INFO/SUCCESS logging output.</p> <code>output_score_cls</code> <code>bool</code> <p>Whether to include tracker <code>score</code> and <code>cls</code> values in outputs. If <code>False</code>, both fields are exported as <code>-1</code> to keep file schema stable.</p> <code>REID_WEIGHTS_DIR</code> <code>Path</code> <p>Directory where relative ReID weights are resolved and stored.</p> <code>DEFAULT_REID_WEIGHT</code> <code>str</code> <p>Fallback ReID weight file name used when a model expects ReID and no weight is explicitly set.</p> <p>Initialize the tracker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BoxMOTModelConfig</code> <p>Configuration bundle for BoxMOT tracker creation. Tracker backend is selected from <code>config.model</code>.</p> <code>None</code> <code>config_yaml</code> <code>str</code> <p>YAML file containing model-aware config. When provided, values loaded from YAML override <code>config</code> input.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device string used by deep trackers (default: <code>\"auto\"</code>, \"cpu\", \"cuda\", \"mps\").</p> <code>'auto'</code> <code>half</code> <code>bool</code> <p>Whether half precision is enabled for deep trackers (default: <code>False</code>).</p> <code>False</code> <code>output_score_cls</code> <code>bool</code> <p>If True, output tracker confidence and class values in <code>score</code> and <code>cls</code> columns. If False, export <code>-1</code> for both fields.</p> <code>True</code> <code>boxmot_verbose</code> <code>bool</code> <p>If False, suppress BoxMOT INFO/SUCCESS logging output.</p> <code>False</code> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def __init__(\n    self,\n    config: BoxMOTModelParams | None = None,\n    config_yaml: str | None = None,\n    device: str = \"auto\",\n    half: bool = False,\n    output_score_cls: bool = True,\n    boxmot_verbose: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the tracker.\n\n    Parameters\n    ----------\n    config : BoxMOTModelConfig, optional\n        Configuration bundle for BoxMOT tracker creation. Tracker backend\n        is selected from `config.model`.\n    config_yaml : str, optional\n        YAML file containing model-aware config. When provided,\n        values loaded from YAML override `config` input.\n    device : str, optional\n        Device string used by deep trackers (default: `\"auto\"`, \"cpu\", \"cuda\", \"mps\").\n    half : bool, optional\n        Whether half precision is enabled for deep trackers (default: `False`).\n    output_score_cls : bool, optional\n        If True, output tracker confidence and class values in `score` and\n        `cls` columns. If False, export `-1` for both fields.\n    boxmot_verbose : bool, optional\n        If False, suppress BoxMOT INFO/SUCCESS logging output.\n\n    \"\"\"\n    self.device = device\n    self.half = half\n    self.boxmot_verbose = boxmot_verbose\n    self.output_score_cls = output_score_cls\n    yaml_path = config_yaml\n    resolved_config = config or config\n    self.model_config_yaml = yaml_path\n\n    if yaml_path:\n        resolved_config = self.import_config_from_yaml(yaml_path)\n\n    if resolved_config is None:\n        resolved_config = self._default_boxmot_config()\n\n    self.boxmot_model = resolved_config.model\n    self.boxmot_config = resolved_config\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.track","title":"track","text":"<pre><code>track(\n    det_file: str,\n    out_file: str,\n    video_file: str | None = None,\n    show: bool = False,\n    video_index: int | None = None,\n    total_videos: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run tracking on a single detection file using BoxMOT.</p> <p>Parameters:</p> Name Type Description Default <code>det_file</code> <code>str</code> <p>Path to detection file (CSV format with columns: frame, x1, y1, width, height, confidence, class_id).</p> required <code>out_file</code> <code>str</code> <p>Path to write tracking results. If empty string, results are not saved.</p> required <code>video_file</code> <code>str</code> <p>Path to source video file. Required for BoxMOT tracker.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True (default: False), display live tracking preview with bounding boxes and track IDs. Press 's' to toggle preview, 'ESC' to hide, 'q' to stop tracking early.</p> <code>False</code> <code>video_index</code> <code>int</code> <p>Index of current video in batch (for progress bar display).</p> <code>None</code> <code>total_videos</code> <code>int</code> <p>Total number of videos in batch (for complete progress context).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tracking results with columns: frame, track, x, y, w, h, score, cls, r3, r4 Each row represents one detected object per frame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If det_file or video_file does not exist.</p> <code>ValueError</code> <p>If video_file is None.</p> Notes <p>The tracker processes detections frame-by-frame, maintaining track IDs across frames. Detection coordinates are converted from (x1, y1, x2, y2) to (x, y, width, height) format for BoxMOT.</p> <p>Track IDs are persistent across frame sequences and reused if tracks are lost and then re-acquired within track_buffer frames.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def track(\n    self,\n    det_file: str,\n    out_file: str,\n    video_file: str | None = None,\n    show: bool = False,\n    video_index: int | None = None,\n    total_videos: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Run tracking on a single detection file using BoxMOT.\n\n    Parameters\n    ----------\n    det_file : str\n        Path to detection file (CSV format with columns:\n        frame, x1, y1, width, height, confidence, class_id).\n    out_file : str\n        Path to write tracking results. If empty string, results are not saved.\n    video_file : str, optional\n        Path to source video file. Required for BoxMOT tracker.\n    show : bool, optional\n        If True (default: False), display live tracking preview with bounding\n        boxes and track IDs. Press 's' to toggle preview, 'ESC' to hide,\n        'q' to stop tracking early.\n    video_index : int, optional\n        Index of current video in batch (for progress bar display).\n    total_videos : int, optional\n        Total number of videos in batch (for complete progress context).\n\n    Returns\n    -------\n    pd.DataFrame\n        Tracking results with columns: frame, track, x, y, w, h, score, cls, r3, r4\n        Each row represents one detected object per frame.\n\n    Raises\n    ------\n    FileNotFoundError\n        If det_file or video_file does not exist.\n    ValueError\n        If video_file is None.\n\n    Notes\n    -----\n    The tracker processes detections frame-by-frame, maintaining track IDs\n    across frames. Detection coordinates are converted from (x1, y1, x2, y2)\n    to (x, y, width, height) format for BoxMOT.\n\n    Track IDs are persistent across frame sequences and reused if tracks\n    are lost and then re-acquired within track_buffer frames.\n\n    \"\"\"\n    if not Path(det_file).exists():\n        msg = f\"Detection file not found: {det_file}\"\n        raise FileNotFoundError(msg)\n\n    if video_file is None:\n        msg = \"Video file required for BoxMOT tracking but not provided.\"\n        raise ValueError(msg)\n    if not Path(video_file).exists():\n        msg = f\"Video file not found: {video_file}\"\n        raise FileNotFoundError(msg)\n\n    return self._track_boxmot(\n        video_file=video_file,\n        det_file=det_file,\n        out_file=out_file,\n        show=show,\n        video_index=video_index,\n        total_videos=total_videos,\n    )\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.track_batch","title":"track_batch","text":"<pre><code>track_batch(\n    det_files: list[str] | None = None,\n    video_files: list[str] | None = None,\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n) -&gt; list[str]\n</code></pre> <p>Run tracking on multiple detection files sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>det_files</code> <code>list[str] | None</code> <p>List of detection file paths. Each file should contain frame-level detections in CSV format. If None (default), returns empty list.</p> <code>None</code> <code>video_files</code> <code>list[str] | None</code> <p>List of corresponding source video file paths for each detection file. Length should match det_files. Required for BoxMOT tracking.</p> <code>None</code> <code>output_path</code> <code>str | None</code> <p>Directory to save tracking results. Track files are named based on input filename with '_track.txt' suffix. If None (default), tracking still runs but results are not persisted.</p> <code>None</code> <code>is_overwrite</code> <code>bool</code> <p>If False (default), skip tracking for videos with existing output files.</p> <code>False</code> <code>is_report</code> <code>bool</code> <p>If True (default), include skipped files in returned list.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of output track file paths. Includes both newly created and existing files (if is_report=True). Empty list if det_files is None.</p> Notes <p>Processing is sequential (not parallel). Each detection file is tracked in order with progress display showing \"Tracking X of Y\".</p> <p>Files matching between det_files and video_files by index position. If video_files is shorter than det_files, missing videos are left None and those detections are skipped.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def track_batch(\n    self,\n    det_files: list[str] | None = None,\n    video_files: list[str] | None = None,\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n) -&gt; list[str]:\n    \"\"\"Run tracking on multiple detection files sequentially.\n\n    Parameters\n    ----------\n    det_files : list[str] | None, optional\n        List of detection file paths. Each file should contain frame-level\n        detections in CSV format. If None (default), returns empty list.\n    video_files : list[str] | None, optional\n        List of corresponding source video file paths for each detection file.\n        Length should match det_files. Required for BoxMOT tracking.\n    output_path : str | None, optional\n        Directory to save tracking results. Track files are named based on\n        input filename with '_track.txt' suffix. If None (default),\n        tracking still runs but results are not persisted.\n    is_overwrite : bool, optional\n        If False (default), skip tracking for videos with existing output files.\n    is_report : bool, optional\n        If True (default), include skipped files in returned list.\n\n    Returns\n    -------\n    list[str]\n        List of output track file paths. Includes both newly created and\n        existing files (if is_report=True). Empty list if det_files is None.\n\n    Notes\n    -----\n    Processing is sequential (not parallel). Each detection file is tracked\n    in order with progress display showing \"Tracking X of Y\".\n\n    Files matching between det_files and video_files by index position.\n    If video_files is shorter than det_files, missing videos are left None\n    and those detections are skipped.\n\n    \"\"\"\n    if det_files is None:\n        return []\n\n    results: list[str] = []\n    total_videos = len(det_files)\n\n    for idx, det_file in enumerate(det_files, start=1):\n        base_filename = os.path.splitext(os.path.basename(det_file))[0].replace(\"_iou\", \"\")\n\n        track_file = None\n        if output_path:\n            os.makedirs(output_path, exist_ok=True)\n            track_file = os.path.join(output_path, base_filename + \"_track.txt\")\n\n        if track_file and not is_overwrite and os.path.exists(track_file):\n            if is_report:\n                results.append(track_file)\n            continue\n\n        # BoxMOT requires a matching source video.\n        video_file = None\n        if video_files is not None:  # noqa: SIM102\n            if idx - 1 &lt; len(video_files):\n                video_file = video_files[idx - 1]\n\n        # run tracking\n        self.track(\n            det_file=det_file,\n            out_file=track_file if track_file else \"\",  # track() expects a path\n            video_file=video_file,\n            video_index=idx,\n            total_videos=total_videos,\n        )\n\n        if track_file:\n            results.append(track_file)\n\n    return results\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.export_config_to_yaml","title":"export_config_to_yaml  <code>staticmethod</code>","text":"<pre><code>export_config_to_yaml(\n    yaml_file: str, config: BoxMOTModelParams\n) -&gt; None\n</code></pre> <p>Export model-aware BoxMOT config to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef export_config_to_yaml(\n    yaml_file: str,\n    config: BoxMOTModelParams,\n) -&gt; None:\n    \"\"\"Export model-aware BoxMOT config to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(config.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.import_config_from_yaml","title":"import_config_from_yaml  <code>staticmethod</code>","text":"<pre><code>import_config_from_yaml(\n    yaml_file: str,\n) -&gt; BoxMOTModelParams\n</code></pre> <p>Import model-aware BoxMOT config from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef import_config_from_yaml(yaml_file: str) -&gt; BoxMOTModelParams:\n    \"\"\"Import model-aware BoxMOT config from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n\n    model = MOTModels(str(data.get(\"model\", MOTModels.BOTSORT.value)))\n    param_cls = Tracker._params_class_for_model(model)\n    return param_cls.from_dict(data)\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.export_params_to_yaml","title":"export_params_to_yaml  <code>staticmethod</code>","text":"<pre><code>export_params_to_yaml(\n    yaml_file: str, params: BoxMOTModelParams\n) -&gt; None\n</code></pre> <p>Export model-aware BoxMOT params to a YAML file (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef export_params_to_yaml(\n    yaml_file: str,\n    params: BoxMOTModelParams,\n) -&gt; None:\n    \"\"\"Export model-aware BoxMOT params to a YAML file (backward-compatible wrapper).\"\"\"\n    Tracker.export_config_to_yaml(yaml_file=yaml_file, config=params)\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.import_params_from_yaml","title":"import_params_from_yaml  <code>staticmethod</code>","text":"<pre><code>import_params_from_yaml(\n    yaml_file: str,\n) -&gt; BoxMOTModelParams\n</code></pre> <p>Import model-aware BoxMOT params from a YAML file (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef import_params_from_yaml(yaml_file: str) -&gt; BoxMOTModelParams:\n    \"\"\"Import model-aware BoxMOT params from a YAML file (backward-compatible wrapper).\"\"\"\n    return Tracker.import_config_from_yaml(yaml_file=yaml_file)\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.export_current_config_to_yaml","title":"export_current_config_to_yaml","text":"<pre><code>export_current_config_to_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export this tracker's active model and config to YAML.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_current_config_to_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export this tracker's active model and config to YAML.\"\"\"\n    self.export_config_to_yaml(\n        yaml_file=yaml_file,\n        config=self.boxmot_config,\n    )\n</code></pre>"},{"location":"api/track/#dnt.track.Tracker.export_current_params_to_yaml","title":"export_current_params_to_yaml","text":"<pre><code>export_current_params_to_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export this tracker's active model and config to YAML (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_current_params_to_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export this tracker's active model and config to YAML (backward-compatible wrapper).\"\"\"\n    self.export_current_config_to_yaml(yaml_file)\n</code></pre>"},{"location":"api/track/post_process/","title":"Post Processing","text":"<p>Post-process utilities for tracked trajectories.</p>"},{"location":"api/track/post_process/#dnt.track.post_process.interpolate_tracks_rts","title":"interpolate_tracks_rts","text":"<pre><code>interpolate_tracks_rts(\n    tracks: DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    fill_gaps_only: bool = True,\n    smooth_existing: bool = False,\n    process_var: float = 10.0,\n    meas_var_pos: float = 25.0,\n    meas_var_size: float = 16.0,\n    min_track_len: int = 2,\n    max_gap: int = 30,\n    add_interp_flag: bool = True,\n    interp_col: str = \"interp\",\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Interpolate trajectory gaps in each track chain using RTS smoothing.</p> <p>Applies a constant-velocity Kalman filter per track on bounding box center and size states, then runs Rauch-Tung-Striebel (RTS) smoothing from FilterPy to produce smooth, continuous trajectories. Missing frames are interpolated with velocity estimates.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame</code> <p>Input track data with columns at minimum: frame, track, x, y, w, h. May also contain cls, score, and other columns which are preserved. If None, <code>track_file</code> is used.</p> <code>None</code> <code>track_file</code> <code>str</code> <p>CSV file path to read tracks from when <code>tracks</code> is None.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>CSV file path to write the interpolated results.</p> <code>None</code> <code>col_names</code> <code>list[str]</code> <p>Column names to apply when input columns are positional integers. Default is [\"frame\",\"track\",\"x\",\"y\",\"w\",\"h\",\"score\",\"cls\",\"r3\",\"r4\"].</p> <code>None</code> <code>fill_gaps_only</code> <code>bool</code> <p>If True (default), only interpolate frames without observations. If False, also smooth observed frames.</p> <code>True</code> <code>smooth_existing</code> <code>bool</code> <p>If True, apply smoothed state to observed frames. Only used when fill_gaps_only is True. Default is False.</p> <code>False</code> <code>process_var</code> <code>float</code> <p>Process noise variance for Kalman filter. Controls model uncertainty. Default is 10.0.</p> <code>10.0</code> <code>meas_var_pos</code> <code>float</code> <p>Measurement noise variance for position (cx, cy). Default is 25.0.</p> <code>25.0</code> <code>meas_var_size</code> <code>float</code> <p>Measurement noise variance for size (w, h). Default is 16.0.</p> <code>16.0</code> <code>min_track_len</code> <code>int</code> <p>Minimum track length to apply interpolation. Tracks shorter than this are returned as-is. Default is 2.</p> <code>2</code> <code>max_gap</code> <code>int</code> <p>Maximum number of consecutive missing frames allowed to interpolate within a track chain. Gaps larger than this value are not filled. Default is 30.</p> <code>30</code> <code>add_interp_flag</code> <code>bool</code> <p>If True (default), add column with interpolation flags (0=observed, 1=interpolated).</p> <code>True</code> <code>interp_col</code> <code>str</code> <p>Name of the interpolation flag column. Default is \"interp\".</p> <code>'interp'</code> <code>verbose</code> <code>bool</code> <p>If True, show tqdm progress bar over tracks. Default is True.</p> <code>True</code> <code>video_index</code> <code>int</code> <p>Current video index for progress description. Default is None.</p> <code>None</code> <code>video_tot</code> <code>int</code> <p>Total videos for progress description. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output tracks with interpolated frames. Columns include all input columns plus interp_col if add_interp_flag is True. Frame indices are continuous within each track after interpolation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tracks has fewer than 6 columns (when columns are not named).</p> Notes <p>The Kalman filter uses an 8-state constant-velocity model:     [cx, vx, cy, vy, w, vw, h, vh] where (cx, cy) is bounding box center, (w, h) is size, and (vx, vy, vw, vh) are their velocities.</p> <p>Input coordinates assume [x, y, w, h] format where x, y is top-left corner. These are converted to center coordinates for Kalman processing.</p> <p>Frame gaps within tracks are filled by interpolation. If a track has missing frames between observations, the filter predicts values for those frames based on velocity estimates from nearby observations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Create sample track with gaps\n&gt;&gt;&gt; tracks = pd.DataFrame({\n...     'frame': [0, 1, 5, 6],\n...     'track': [1, 1, 1, 1],\n...     'x': [10.0, 12.0, 20.0, 22.0],\n...     'y': [20.0, 22.0, 30.0, 32.0],\n...     'w': [100.0, 100.0, 100.0, 100.0],\n...     'h': [50.0, 50.0, 50.0, 50.0],\n... })\n&gt;&gt;&gt; result = interpolate_tracks_rts(tracks, fill_gaps_only=True)\n&gt;&gt;&gt; print(result[['frame', 'track', 'interp']])  # Shows interpolated frames\n</code></pre> Source code in <code>src/dnt/track/post_process.py</code> <pre><code>def interpolate_tracks_rts(\n    tracks: pd.DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    fill_gaps_only: bool = True,\n    smooth_existing: bool = False,\n    process_var: float = 10.0,\n    meas_var_pos: float = 25.0,\n    meas_var_size: float = 16.0,\n    min_track_len: int = 2,\n    max_gap: int = 30,\n    add_interp_flag: bool = True,\n    interp_col: str = \"interp\",\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Interpolate trajectory gaps in each track chain using RTS smoothing.\n\n    Applies a constant-velocity Kalman filter per track on bounding box center\n    and size states, then runs Rauch-Tung-Striebel (RTS) smoothing from FilterPy\n    to produce smooth, continuous trajectories. Missing frames are interpolated\n    with velocity estimates.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame, optional\n        Input track data with columns at minimum: frame, track, x, y, w, h.\n        May also contain cls, score, and other columns which are preserved.\n        If None, ``track_file`` is used.\n    track_file : str, optional\n        CSV file path to read tracks from when ``tracks`` is None.\n    output_file : str, optional\n        CSV file path to write the interpolated results.\n    col_names : list[str], optional\n        Column names to apply when input columns are positional integers.\n        Default is [\"frame\",\"track\",\"x\",\"y\",\"w\",\"h\",\"score\",\"cls\",\"r3\",\"r4\"].\n    fill_gaps_only : bool, optional\n        If True (default), only interpolate frames without observations.\n        If False, also smooth observed frames.\n    smooth_existing : bool, optional\n        If True, apply smoothed state to observed frames. Only used when\n        fill_gaps_only is True. Default is False.\n    process_var : float, optional\n        Process noise variance for Kalman filter. Controls model uncertainty.\n        Default is 10.0.\n    meas_var_pos : float, optional\n        Measurement noise variance for position (cx, cy). Default is 25.0.\n    meas_var_size : float, optional\n        Measurement noise variance for size (w, h). Default is 16.0.\n    min_track_len : int, optional\n        Minimum track length to apply interpolation. Tracks shorter than this\n        are returned as-is. Default is 2.\n    max_gap : int, optional\n        Maximum number of consecutive missing frames allowed to interpolate\n        within a track chain. Gaps larger than this value are not filled.\n        Default is 30.\n    add_interp_flag : bool, optional\n        If True (default), add column with interpolation flags (0=observed, 1=interpolated).\n    interp_col : str, optional\n        Name of the interpolation flag column. Default is \"interp\".\n    verbose : bool, optional\n        If True, show tqdm progress bar over tracks. Default is True.\n    video_index : int, optional\n        Current video index for progress description. Default is None.\n    video_tot : int, optional\n        Total videos for progress description. Default is None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Output tracks with interpolated frames. Columns include all input\n        columns plus interp_col if add_interp_flag is True. Frame indices are\n        continuous within each track after interpolation.\n\n    Raises\n    ------\n    ValueError\n        If tracks has fewer than 6 columns (when columns are not named).\n\n    Notes\n    -----\n    The Kalman filter uses an 8-state constant-velocity model:\n        [cx, vx, cy, vy, w, vw, h, vh]\n    where (cx, cy) is bounding box center, (w, h) is size, and\n    (vx, vy, vw, vh) are their velocities.\n\n    Input coordinates assume [x, y, w, h] format where x, y is top-left corner.\n    These are converted to center coordinates for Kalman processing.\n\n    Frame gaps within tracks are filled by interpolation. If a track has\n    missing frames between observations, the filter predicts values for those\n    frames based on velocity estimates from nearby observations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; # Create sample track with gaps\n    &gt;&gt;&gt; tracks = pd.DataFrame({\n    ...     'frame': [0, 1, 5, 6],\n    ...     'track': [1, 1, 1, 1],\n    ...     'x': [10.0, 12.0, 20.0, 22.0],\n    ...     'y': [20.0, 22.0, 30.0, 32.0],\n    ...     'w': [100.0, 100.0, 100.0, 100.0],\n    ...     'h': [50.0, 50.0, 50.0, 50.0],\n    ... })\n    &gt;&gt;&gt; result = interpolate_tracks_rts(tracks, fill_gaps_only=True)\n    &gt;&gt;&gt; print(result[['frame', 'track', 'interp']])  # Shows interpolated frames\n\n    \"\"\"\n    from filterpy.common import Q_discrete_white_noise\n    from filterpy.kalman import KalmanFilter, rts_smoother\n\n    if col_names is None:\n        col_names = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"r3\", \"r4\"]\n\n    if tracks is None:\n        if not track_file:\n            raise ValueError(\"Either `tracks` or `track_file` must be provided.\")\n        tracks = pd.read_csv(track_file)\n\n    if len(tracks) == 0:\n        out = tracks.copy()\n        if output_file:\n            out.to_csv(output_file, index=False)\n        return out\n\n    df = tracks.copy()\n\n    # Support both positional and named-column track tables.\n    required = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\"]\n    if all(c in df.columns for c in required):\n        work = df.copy()\n    else:\n        if len(df.columns) &lt; len(required):\n            raise ValueError(\"tracks must include at least frame/track/x/y/w/h columns.\")\n        renamed = col_names[: len(df.columns)]\n        work = df.copy()\n        work.columns = renamed\n\n    work = work.sort_values([\"track\", \"frame\"]).reset_index(drop=True)\n    output_rows: list[dict] = []\n    grouped = list(work.groupby(\"track\", sort=False))\n\n    pbar = tqdm(total=len(grouped), unit=\" tracks\", disable=not verbose)\n    if verbose:\n        if video_index is not None and video_tot is not None:\n            pbar.set_description_str(f\"RTS interpolate {video_index} of {video_tot}\")\n        else:\n            pbar.set_description_str(\"RTS interpolate\")\n\n    for track_id, g in grouped:\n        g = g.sort_values(\"frame\").drop_duplicates(\"frame\", keep=\"first\").reset_index(drop=True)\n        if len(g) &lt; min_track_len:\n            rows = g.to_dict(\"records\")\n            if add_interp_flag:\n                for r in rows:\n                    if \"r3\" in g.columns:\n                        r[\"r3\"] = 0\n                    else:\n                        r[interp_col] = 0\n            output_rows.extend(rows)\n            pbar.update(1)\n            continue\n\n        frames_obs = g[\"frame\"].astype(int).to_numpy()\n        frame_start = int(frames_obs.min())\n        frame_end = int(frames_obs.max())\n        frames_full = np.arange(frame_start, frame_end + 1, dtype=int)\n        observed_set = set(frames_obs.tolist())\n        fillable_missing: set[int] = set()\n        for f0, f1 in pairwise(frames_obs):\n            gap = int(f1 - f0 - 1)\n            if 0 &lt; gap &lt;= max_gap:\n                fillable_missing.update(range(int(f0) + 1, int(f1)))\n\n        cx = (g[\"x\"].astype(float) + (g[\"w\"].astype(float) / 2.0)).to_numpy()\n        cy = (g[\"y\"].astype(float) + (g[\"h\"].astype(float) / 2.0)).to_numpy()\n        ww = g[\"w\"].astype(float).to_numpy()\n        hh = g[\"h\"].astype(float).to_numpy()\n        z_map = {int(f): np.array([cx[i], cy[i], ww[i], hh[i]], dtype=float) for i, f in enumerate(frames_obs)}\n        row_map = {int(row[\"frame\"]): row for row in g.to_dict(\"records\")}\n\n        # State: [cx, vx, cy, vy, w, vw, h, vh]\n        kf = KalmanFilter(dim_x=8, dim_z=4)\n        kf.F = np.array(\n            [\n                [1, 1, 0, 0, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 1, 1, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 1],\n            ],\n            dtype=float,\n        )\n        kf.H = np.array(\n            [\n                [1, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 0],\n            ],\n            dtype=float,\n        )\n        q2 = Q_discrete_white_noise(dim=2, dt=1.0, var=process_var)\n        kf.Q = np.zeros((8, 8), dtype=float)\n        for i in range(4):\n            i0 = i * 2\n            kf.Q[i0 : i0 + 2, i0 : i0 + 2] = q2\n        kf.R = np.diag([meas_var_pos, meas_var_pos, meas_var_size, meas_var_size]).astype(float)\n        kf.P = np.eye(8, dtype=float) * 100.0\n        z0 = z_map[frame_start]\n        kf.x = np.array([z0[0], 0.0, z0[1], 0.0, z0[2], 0.0, z0[3], 0.0], dtype=float)\n\n        xs, ps, fs, qs = [], [], [], []\n        for f in frames_full:\n            kf.predict()\n            z = z_map.get(int(f))\n            if z is not None:\n                kf.update(z)\n            xs.append(kf.x.copy())\n            ps.append(kf.P.copy())\n            fs.append(kf.F.copy())\n            qs.append(kf.Q.copy())\n\n        xs_s, _, _, _ = rts_smoother(np.asarray(xs), np.asarray(ps), np.asarray(fs), np.asarray(qs))\n\n        if \"cls\" in g.columns and len(g[\"cls\"].dropna()) &gt; 0:\n            cls_mode = g[\"cls\"].mode()\n            cls_fill = float(cls_mode.iloc[0]) if len(cls_mode) &gt; 0 else -1\n        else:\n            cls_fill = -1\n        score_fill = float(g[\"score\"].mean()) if \"score\" in g.columns and len(g[\"score\"].dropna()) &gt; 0 else -1.0\n\n        for i, frame in enumerate(frames_full.tolist()):\n            sm_cx = float(xs_s[i, 0])\n            sm_cy = float(xs_s[i, 2])\n            sm_w = max(1.0, float(xs_s[i, 4]))\n            sm_h = max(1.0, float(xs_s[i, 6]))\n            sm_x = sm_cx - (sm_w / 2.0)\n            sm_y = sm_cy - (sm_h / 2.0)\n\n            if frame in observed_set:\n                row = dict(row_map[frame])\n                if smooth_existing or (not fill_gaps_only):\n                    row[\"x\"] = sm_x\n                    row[\"y\"] = sm_y\n                    row[\"w\"] = sm_w\n                    row[\"h\"] = sm_h\n                if add_interp_flag:\n                    if \"r3\" in g.columns:\n                        row[\"r3\"] = 0\n                    else:\n                        row[interp_col] = 0\n                output_rows.append(row)\n            else:\n                if frame not in fillable_missing:\n                    continue\n                row = {c: np.nan for c in g.columns}\n                row[\"frame\"] = frame\n                row[\"track\"] = track_id\n                row[\"x\"] = sm_x\n                row[\"y\"] = sm_y\n                row[\"w\"] = sm_w\n                row[\"h\"] = sm_h\n                if \"cls\" in g.columns:\n                    row[\"cls\"] = cls_fill\n                if \"score\" in g.columns:\n                    row[\"score\"] = score_fill\n                if add_interp_flag:\n                    if \"r3\" in g.columns:\n                        row[\"r3\"] = 1\n                    else:\n                        row[interp_col] = 1\n                output_rows.append(row)\n        pbar.update(1)\n\n    pbar.close()\n\n    out = pd.DataFrame(output_rows)\n    if \"r3\" in out.columns:\n        cols = list(out.columns)\n        idx = cols.index(\"r3\")\n        out.rename(columns={\"r3\": interp_col}, inplace=True)\n        cols[idx] = interp_col\n        out = out[cols]\n\n    # Keep compatibility with legacy track file readers that enforce integer dtypes.\n    int_cols = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"cls\", \"r4\", interp_col]\n    for c in int_cols:\n        if c in out.columns:\n            out[c] = out[c].fillna(-1).round().astype(int)\n    if \"score\" in out.columns:\n        out[\"score\"] = out[\"score\"].fillna(-1).astype(float)\n\n    out = out.sort_values([\"frame\", \"track\"]).reset_index(drop=True)\n    if output_file:\n        out.to_csv(output_file, index=False, header=False)\n    return out\n</code></pre>"},{"location":"api/track/post_process/#dnt.track.post_process.link_tracklets","title":"link_tracklets","text":"<pre><code>link_tracklets(\n    tracks: DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    max_gap: int = 20,\n    vel_frames: int = 5,\n    size_ratio_max: float = 2.0,\n    dist_mult: float = 2.5,\n    iou_min: float = 0.05,\n    w_d: float = 1.0,\n    w_iou: float = 1.0,\n    w_s: float = 0.3,\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Reconnect broken tracklets using global optimal 1-to-1 matching.</p> <p>Links tracklets (short track segments) by computing a cost matrix based on spatial proximity, appearance similarity (IoU), and size consistency. Uses linear sum assignment (Hungarian algorithm) to find optimal matches, then merges tracklets via union-find to handle transitive connections.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame | None</code> <p>Input track data with columns: frame, track, x, y, w, h, and optionally score, cls, interp, r4. If None (default), <code>track_file</code> is used.</p> <code>None</code> <code>track_file</code> <code>str | None</code> <p>CSV file path to read tracks from when <code>tracks</code> is None.</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>CSV file path to write linked results. If None (default), results are not saved to file.</p> <code>None</code> <code>col_names</code> <code>list[str] | None</code> <p>Column names to apply when input has positional integer columns. Default: [\"frame\",\"track\",\"x\",\"y\",\"w\",\"h\",\"score\",\"cls\",\"interp\",\"r4\"].</p> <code>None</code> <code>max_gap</code> <code>int</code> <p>Maximum frame gap between tracklet end and start to attempt linking. Default is 20.</p> <code>20</code> <code>vel_frames</code> <code>int</code> <p>Number of recent frames to use for velocity estimation (polynomial fit). Default is 5.</p> <code>5</code> <code>size_ratio_max</code> <code>float</code> <p>Maximum allowed width/height ratio between tracklet end and start. Default is 2.0. Values outside [1/ratio_max, ratio_max] are rejected.</p> <code>2.0</code> <code>dist_mult</code> <code>float</code> <p>Distance threshold multiplier: distance_threshold = dist_mult * sqrt(area). Default is 2.5. Larger values allow more spatial flexibility.</p> <code>2.5</code> <code>iou_min</code> <code>float</code> <p>Minimum Intersection over Union (IoU) between predicted and actual start box. Default is 0.05. Range [0.0, 1.0].</p> <code>0.05</code> <code>w_d</code> <code>float</code> <p>Weight for normalized distance cost in weighted sum. Default is 1.0.</p> <code>1.0</code> <code>w_iou</code> <code>float</code> <p>Weight for (1 - IoU) cost in weighted sum. Default is 1.0.</p> <code>1.0</code> <code>w_s</code> <code>float</code> <p>Weight for size inconsistency cost (log ratio) in weighted sum. Default is 0.3 (smaller weight for size).</p> <code>0.3</code> <code>verbose</code> <code>bool</code> <p>If True (default), display tqdm progress bar over tracklets.</p> <code>True</code> <code>video_index</code> <code>int | None</code> <p>Current video index for progress description. Default is None.</p> <code>None</code> <code>video_tot</code> <code>int | None</code> <p>Total number of videos for progress description. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output tracks with linked IDs. Same columns as input. Track IDs are remapped so that all frames belonging to a logical track share the same ID. Frame and track are sorted in output.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tracks has fewer than 6 columns and no named columns provided.</p> <code>FileNotFoundError</code> <p>If track_file path does not exist.</p> Notes <p>Algorithm Overview:</p> <ol> <li>Extract descriptor for each track: endpoints, velocity, bounding boxes, class</li> <li>Build cost matrix using spatial (distance, IoU), appearance (class), and    size (width/height ratio) metrics with weighted combination</li> <li>Solve linear sum assignment problem (Hungarian algorithm) to find optimal    1-to-1 tracklet pairings with minimum total cost</li> <li>Use Union-Find (Disjoint Set Union) to handle transitive merges:    if tracklet A links to B and B links to C, they all get merged to same group</li> <li>Remap all track IDs according to merged components</li> </ol> <p>Cost Function Details:</p> <ul> <li>Velocity is estimated using polynomial fit (1st order) on recent observed frames</li> <li>Predicted next tracklet start = end_position + velocity * temporal_gap</li> <li>Distance is normalized by sqrt(bounding_box_area) for scale invariance</li> <li>Only considers tracklets from same class (if class info available)</li> <li>Skips linking if temporal gap, size ratio, or distance threshold exceeded</li> </ul> <p>Input Requirements:</p> <ul> <li>Requires \"frame\", \"track\", \"x\", \"y\", \"w\", \"h\" columns minimum</li> <li>If \"interp\" column exists, uses only rows with interp==0 for velocity estimation</li> <li>If \"cls\" column exists, only links tracklets with same class</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Create sample tracklets\n&gt;&gt;&gt; tracks = pd.DataFrame({\n...     'frame': [0, 1, 10, 11, 20, 21],\n...     'track': [1, 1, 2, 2, 3, 3],\n...     'x': [10, 12, 25, 27, 40, 42],\n...     'y': [20, 22, 35, 37, 50, 52],\n...     'w': [50, 50, 50, 50, 50, 50],\n...     'h': [100, 100, 100, 100, 100, 100],\n...     'cls': [1, 1, 1, 1, 1, 1],\n... })\n&gt;&gt;&gt; linked = link_tracklets(tracks, max_gap=15, verbose=False)\n&gt;&gt;&gt; # Track IDs may now be remapped: e.g., [1, 1, 1, 1, 1, 1]\n&gt;&gt;&gt; print(linked['track'].unique())  # All in same track if linked\n</code></pre> Source code in <code>src/dnt/track/post_process.py</code> <pre><code>def link_tracklets(\n    tracks: pd.DataFrame | None = None,\n    track_file: str | None = None,\n    output_file: str | None = None,\n    col_names: list[str] | None = None,\n    max_gap: int = 20,\n    vel_frames: int = 5,\n    size_ratio_max: float = 2.0,\n    dist_mult: float = 2.5,\n    iou_min: float = 0.05,\n    w_d: float = 1.0,\n    w_iou: float = 1.0,\n    w_s: float = 0.3,\n    verbose: bool = True,\n    video_index: int | None = None,\n    video_tot: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Reconnect broken tracklets using global optimal 1-to-1 matching.\n\n    Links tracklets (short track segments) by computing a cost matrix based on\n    spatial proximity, appearance similarity (IoU), and size consistency.\n    Uses linear sum assignment (Hungarian algorithm) to find optimal matches,\n    then merges tracklets via union-find to handle transitive connections.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame | None, optional\n        Input track data with columns: frame, track, x, y, w, h, and optionally\n        score, cls, interp, r4. If None (default), ``track_file`` is used.\n    track_file : str | None, optional\n        CSV file path to read tracks from when ``tracks`` is None.\n    output_file : str | None, optional\n        CSV file path to write linked results. If None (default), results\n        are not saved to file.\n    col_names : list[str] | None, optional\n        Column names to apply when input has positional integer columns.\n        Default: [\"frame\",\"track\",\"x\",\"y\",\"w\",\"h\",\"score\",\"cls\",\"interp\",\"r4\"].\n    max_gap : int, optional\n        Maximum frame gap between tracklet end and start to attempt linking.\n        Default is 20.\n    vel_frames : int, optional\n        Number of recent frames to use for velocity estimation (polynomial fit).\n        Default is 5.\n    size_ratio_max : float, optional\n        Maximum allowed width/height ratio between tracklet end and start.\n        Default is 2.0. Values outside [1/ratio_max, ratio_max] are rejected.\n    dist_mult : float, optional\n        Distance threshold multiplier: distance_threshold = dist_mult * sqrt(area).\n        Default is 2.5. Larger values allow more spatial flexibility.\n    iou_min : float, optional\n        Minimum Intersection over Union (IoU) between predicted and actual start box.\n        Default is 0.05. Range [0.0, 1.0].\n    w_d : float, optional\n        Weight for normalized distance cost in weighted sum. Default is 1.0.\n    w_iou : float, optional\n        Weight for (1 - IoU) cost in weighted sum. Default is 1.0.\n    w_s : float, optional\n        Weight for size inconsistency cost (log ratio) in weighted sum.\n        Default is 0.3 (smaller weight for size).\n    verbose : bool, optional\n        If True (default), display tqdm progress bar over tracklets.\n    video_index : int | None, optional\n        Current video index for progress description. Default is None.\n    video_tot : int | None, optional\n        Total number of videos for progress description. Default is None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Output tracks with linked IDs. Same columns as input. Track IDs are\n        remapped so that all frames belonging to a logical track share the same ID.\n        Frame and track are sorted in output.\n\n    Raises\n    ------\n    ValueError\n        If tracks has fewer than 6 columns and no named columns provided.\n    FileNotFoundError\n        If track_file path does not exist.\n\n    Notes\n    -----\n    **Algorithm Overview:**\n\n    1. Extract descriptor for each track: endpoints, velocity, bounding boxes, class\n    2. Build cost matrix using spatial (distance, IoU), appearance (class), and\n       size (width/height ratio) metrics with weighted combination\n    3. Solve linear sum assignment problem (Hungarian algorithm) to find optimal\n       1-to-1 tracklet pairings with minimum total cost\n    4. Use Union-Find (Disjoint Set Union) to handle transitive merges:\n       if tracklet A links to B and B links to C, they all get merged to same group\n    5. Remap all track IDs according to merged components\n\n    **Cost Function Details:**\n\n    - Velocity is estimated using polynomial fit (1st order) on recent observed frames\n    - Predicted next tracklet start = end_position + velocity * temporal_gap\n    - Distance is normalized by sqrt(bounding_box_area) for scale invariance\n    - Only considers tracklets from same class (if class info available)\n    - Skips linking if temporal gap, size ratio, or distance threshold exceeded\n\n    **Input Requirements:**\n\n    - Requires \"frame\", \"track\", \"x\", \"y\", \"w\", \"h\" columns minimum\n    - If \"interp\" column exists, uses only rows with interp==0 for velocity estimation\n    - If \"cls\" column exists, only links tracklets with same class\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; # Create sample tracklets\n    &gt;&gt;&gt; tracks = pd.DataFrame({\n    ...     'frame': [0, 1, 10, 11, 20, 21],\n    ...     'track': [1, 1, 2, 2, 3, 3],\n    ...     'x': [10, 12, 25, 27, 40, 42],\n    ...     'y': [20, 22, 35, 37, 50, 52],\n    ...     'w': [50, 50, 50, 50, 50, 50],\n    ...     'h': [100, 100, 100, 100, 100, 100],\n    ...     'cls': [1, 1, 1, 1, 1, 1],\n    ... })\n    &gt;&gt;&gt; linked = link_tracklets(tracks, max_gap=15, verbose=False)\n    &gt;&gt;&gt; # Track IDs may now be remapped: e.g., [1, 1, 1, 1, 1, 1]\n    &gt;&gt;&gt; print(linked['track'].unique())  # All in same track if linked\n\n    \"\"\"\n\n    def _iou_xywh(a: tuple[float, float, float, float], b: tuple[float, float, float, float]) -&gt; float:\n        \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes.\n\n        Parameters\n        ----------\n        a : tuple[float, float, float, float]\n            Bounding box A as (x, y, width, height).\n        b : tuple[float, float, float, float]\n            Bounding box B as (x, y, width, height).\n\n        Returns\n        -------\n        float\n            IoU value in range [0.0, 1.0].\n\n        Notes\n        -----\n        Uses standard IoU formula: intersection / union.\n        Coordinates are in (x, y, width, height) format where (x, y) is top-left.\n\n        \"\"\"\n        ax, ay, aw, ah = a\n        bx, by, bw, bh = b\n        ax2, ay2 = ax + aw, ay + ah\n        bx2, by2 = bx + bw, by + bh\n        ix1, iy1 = max(ax, bx), max(ay, by)\n        ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n        iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)\n        inter = iw * ih\n        union = max(aw * ah + bw * bh - inter, 1e-6)\n        return inter / union\n\n    def _estimate_velocity(frames: np.ndarray, cx: np.ndarray, cy: np.ndarray, k: int) -&gt; tuple[float, float]:\n        \"\"\"Estimate velocity using recent observations via polynomial fitting.\n\n        Parameters\n        ----------\n        frames : np.ndarray\n            Array of frame numbers (timestamps) where observations occur.\n        cx : np.ndarray\n            Array of center x-coordinates corresponding to frames.\n        cy : np.ndarray\n            Array of center y-coordinates corresponding to frames.\n        k : int\n            Number of recent frames to use for velocity estimation.\n            Uses last k points if available, otherwise uses all points.\n\n        Returns\n        -------\n        tuple[float, float]\n            Velocity (vx, vy) as pixels per frame.\n            Returns (0.0, 0.0) if fewer than 2 observations available.\n\n        Notes\n        -----\n        Uses 1st-order polynomial (linear) fit via np.polyfit for robust\n        velocity estimation. Falls back to simple difference (cx[-1]-cx[-2])/dt\n        if fitting fails or insufficient unique frame times.\n\n        \"\"\"\n        n = len(frames)\n        if n &lt; 2:\n            return 0.0, 0.0\n        s = max(0, n - k)\n        t = frames[s:].astype(float)\n        x = cx[s:].astype(float)\n        y = cy[s:].astype(float)\n        if len(t) &lt; 2 or np.allclose(t, t[0]):\n            dt = float(max(frames[-1] - frames[-2], 1))\n            return float((cx[-1] - cx[-2]) / dt), float((cy[-1] - cy[-2]) / dt)\n        vx = float(np.polyfit(t, x, 1)[0])\n        vy = float(np.polyfit(t, y, 1)[0])\n        return vx, vy\n\n    class _DSU:\n        \"\"\"Disjoint Set Union (Union-Find) data structure for tracklet merging.\n\n        Efficiently tracks which tracklet IDs belong to the same connected component\n        using path compression and union by root heuristics.\n\n        Attributes\n        ----------\n        parent : dict[int, int]\n            Parent map where parent[x] points to parent node. If parent[x] == x,\n            then x is a root (representative) of its component.\n\n        Methods\n        -------\n        find(x: int) -&gt; int\n            Find the root representative of x's component with path compression.\n        union(a: int, b: int) -&gt; None\n            Merge components containing a and b under a's root representative.\n\n        Examples\n        --------\n        &gt;&gt;&gt; dsu = _DSU([1, 2, 3, 4])\n        &gt;&gt;&gt; dsu.union(1, 2)  # Merge components\n        &gt;&gt;&gt; dsu.union(2, 3)  # Also connects 1 and 3\n        &gt;&gt;&gt; dsu.find(1) == dsu.find(3)  # Both have same root\n        True\n\n        \"\"\"\n\n        def __init__(self, elems: list[int]) -&gt; None:\n            \"\"\"Initialize DSU with elements in separate components.\n\n            Parameters\n            ----------\n            elems : list[int]\n                List of element IDs to initialize. Each starts in its own component.\n\n            \"\"\"\n            self.parent = {e: e for e in elems}\n\n        def find(self, x: int) -&gt; int:\n            \"\"\"Find root representative of x's component with path compression.\n\n            Parameters\n            ----------\n            x : int\n                Element ID to find.\n\n            Returns\n            -------\n            int\n                Root representative (parent[root] == root).\n\n            \"\"\"\n            p = self.parent[x]\n            if p != x:\n                self.parent[x] = self.find(p)\n            return self.parent[x]\n\n        def union(self, a: int, b: int) -&gt; None:\n            \"\"\"Merge components containing a and b under a's root representative.\n\n            Parameters\n            ----------\n            a : int\n                Element in first component.\n            b : int\n                Element in second component.\n\n            Notes\n            -----\n            Updates parent[root_b] = root_a so all members of b's component\n            now point to a's root as their ultimate parent.\n\n            \"\"\"\n            ra, rb = self.find(a), self.find(b)\n            if ra != rb:\n                self.parent[rb] = ra\n\n    if col_names is None:\n        col_names = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\", \"score\", \"cls\", \"interp\", \"r4\"]\n\n    if tracks is None:\n        if not track_file:\n            raise ValueError(\"Either `tracks` or `track_file` must be provided.\")\n        tracks = pd.read_csv(track_file, header=None)\n\n    if len(tracks) == 0:\n        out = tracks.copy()\n        if output_file:\n            out.to_csv(output_file, index=False, header=False)\n        return out\n\n    df = tracks.copy()\n    required = [\"frame\", \"track\", \"x\", \"y\", \"w\", \"h\"]\n    if not all(c in df.columns for c in required):\n        if len(df.columns) &lt; 6:\n            raise ValueError(\"tracks must include at least frame/track/x/y/w/h columns.\")\n        df.columns = col_names[: len(df.columns)]\n    if \"cls\" not in df.columns and \"class\" in df.columns:\n        df = df.rename(columns={\"class\": \"cls\"})\n    if \"interp\" not in df.columns:\n        df[\"interp\"] = 0\n    else:\n        df[\"interp\"] = pd.to_numeric(df[\"interp\"], errors=\"coerce\").fillna(0).astype(int)\n\n    df = df.sort_values([\"frame\", \"track\"]).reset_index(drop=True)\n    df[\"cx\"] = df[\"x\"].astype(float) + (df[\"w\"].astype(float) / 2.0)\n    df[\"cy\"] = df[\"y\"].astype(float) + (df[\"h\"].astype(float) / 2.0)\n    df[\"area\"] = df[\"w\"].astype(float) * df[\"h\"].astype(float)\n\n    descriptors: dict[int, dict] = {}\n    grouped = list(df.groupby(\"track\", sort=False))\n    pbar = tqdm(total=len(grouped), unit=\" tracklets\", disable=not verbose)\n    if verbose:\n        if video_index is not None and video_tot is not None:\n            pbar.set_description_str(f\"Link tracklets {video_index} of {video_tot}\")\n        else:\n            pbar.set_description_str(\"Link tracklets\")\n\n    for tid, g in grouped:\n        # Treat only interp==1 as synthesized points; 0/-1 are real detections.\n        g_real = g[g[\"interp\"] != 1].sort_values(\"frame\")\n        if len(g_real) &lt; 2:\n            descriptors[int(tid)] = {\"stitchable\": False}\n            pbar.update(1)\n            continue\n        t_start = int(g_real[\"frame\"].iloc[0])\n        t_end = int(g_real[\"frame\"].iloc[-1])\n        start_row = g_real.iloc[0]\n        end_row = g_real.iloc[-1]\n        vx, vy = _estimate_velocity(\n            g_real[\"frame\"].to_numpy(),\n            g_real[\"cx\"].to_numpy(),\n            g_real[\"cy\"].to_numpy(),\n            vel_frames,\n        )\n        descriptors[int(tid)] = {\n            \"stitchable\": True,\n            \"track\": int(tid),\n            \"cls\": int(end_row[\"cls\"]) if \"cls\" in g_real.columns else -1,\n            \"t_start\": t_start,\n            \"t_end\": t_end,\n            \"start_c\": (float(start_row[\"cx\"]), float(start_row[\"cy\"])),\n            \"end_c\": (float(end_row[\"cx\"]), float(end_row[\"cy\"])),\n            \"start_box\": (\n                float(start_row[\"x\"]),\n                float(start_row[\"y\"]),\n                float(start_row[\"w\"]),\n                float(start_row[\"h\"]),\n            ),\n            \"end_box\": (\n                float(end_row[\"x\"]),\n                float(end_row[\"y\"]),\n                float(end_row[\"w\"]),\n                float(end_row[\"h\"]),\n            ),\n            \"area_end\": max(float(end_row[\"area\"]), 1.0),\n            \"vx\": vx,\n            \"vy\": vy,\n        }\n        pbar.update(1)\n    pbar.close()\n\n    stitchable = [d for d in descriptors.values() if d.get(\"stitchable\", False)]\n    if len(stitchable) &lt;= 1:\n        out = df.drop(columns=[\"cx\", \"cy\", \"area\"])\n        if output_file:\n            out.to_csv(output_file, index=False, header=False)\n        return out\n\n    ends = sorted(stitchable, key=lambda d: (d[\"t_end\"], d[\"track\"]))\n    starts = sorted(stitchable, key=lambda d: (d[\"t_start\"], d[\"track\"]))\n    n_end, n_start = len(ends), len(starts)\n    inf = 1e9\n    cost = np.full((n_end, n_start), inf, dtype=float)\n\n    for i, a in enumerate(ends):\n        for j, b in enumerate(starts):\n            if a[\"track\"] == b[\"track\"]:\n                continue\n            dt = b[\"t_start\"] - a[\"t_end\"]\n            if dt &lt; 1 or dt &gt; max_gap:\n                continue\n            if a[\"cls\"] != b[\"cls\"]:\n                continue\n            wi, hi = max(a[\"end_box\"][2], 1.0), max(a[\"end_box\"][3], 1.0)\n            wj, hj = max(b[\"start_box\"][2], 1.0), max(b[\"start_box\"][3], 1.0)\n            w_ratio, h_ratio = wj / wi, hj / hi\n            if not (1.0 / size_ratio_max &lt;= w_ratio &lt;= size_ratio_max):\n                continue\n            if not (1.0 / size_ratio_max &lt;= h_ratio &lt;= size_ratio_max):\n                continue\n\n            pred_cx = a[\"end_c\"][0] + a[\"vx\"] * dt\n            pred_cy = a[\"end_c\"][1] + a[\"vy\"] * dt\n            sx, sy = b[\"start_c\"]\n            dist = float(np.hypot(pred_cx - sx, pred_cy - sy))\n            dist_thr = dist_mult * np.sqrt(a[\"area_end\"]) * (1.0 + (0.03 * dt))\n            if dist &gt;= dist_thr:\n                continue\n\n            pred_box = (pred_cx - (wi / 2.0), pred_cy - (hi / 2.0), wi, hi)\n            iou = _iou_xywh(pred_box, b[\"start_box\"])\n            if iou &lt; iou_min:\n                continue\n\n            dist_norm = dist / (np.sqrt(a[\"area_end\"]) + 1e-6)\n            iou_cost = 1.0 - iou\n            size_cost = abs(np.log(max(w_ratio, 1e-6))) + abs(np.log(max(h_ratio, 1e-6)))\n            c = (w_d * dist_norm) + (w_iou * iou_cost) + (w_s * size_cost)\n            cost[i, j] = c\n\n    matches: list[tuple[int, int]] = []\n    try:\n        from scipy.optimize import linear_sum_assignment\n\n        ri, ci = linear_sum_assignment(cost)\n        for r, c in zip(ri, ci, strict=True):\n            if cost[r, c] &lt; inf:\n                matches.append((r, c))\n    except Exception:\n        used_r: set[int] = set()\n        used_c: set[int] = set()\n        finite_pairs = np.argwhere(cost &lt; inf)\n        finite_pairs = sorted(finite_pairs, key=lambda rc: float(cost[rc[0], rc[1]]))\n        for r, c in finite_pairs:\n            r_i, c_i = int(r), int(c)\n            if r_i in used_r or c_i in used_c:\n                continue\n            used_r.add(r_i)\n            used_c.add(c_i)\n            matches.append((r_i, c_i))\n\n    dsu = _DSU([int(d[\"track\"]) for d in stitchable])\n    for r, c in matches:\n        a_tid = int(ends[r][\"track\"])\n        b_tid = int(starts[c][\"track\"])\n        dsu.union(a_tid, b_tid)\n\n    comps: dict[int, list[int]] = {}\n    for d in stitchable:\n        tid = int(d[\"track\"])\n        root = dsu.find(tid)\n        comps.setdefault(root, []).append(tid)\n\n    tstart_by_tid = {int(d[\"track\"]): int(d[\"t_start\"]) for d in stitchable}\n    rep_map: dict[int, int] = {}\n    for members in comps.values():\n        rep = min(members, key=lambda t: (tstart_by_tid.get(t, 10**9), t))\n        for t in members:\n            rep_map[t] = rep\n\n    all_tids = df[\"track\"].astype(int).unique().tolist()\n    for tid in all_tids:\n        rep_map.setdefault(int(tid), int(tid))\n\n    out = df.copy()\n    out[\"track\"] = out[\"track\"].astype(int).map(rep_map).astype(int)\n    out = out.drop(columns=[\"cx\", \"cy\", \"area\"]).sort_values([\"frame\", \"track\"]).reset_index(drop=True)\n\n    if output_file:\n        out.to_csv(output_file, index=False, header=False)\n    return out\n</code></pre>"},{"location":"api/track/re_class/","title":"Re-classification","text":"<p>Provide the ReClass class for re-classifying object tracks in video frames.</p> <p>Use detection results, with support for configurable models, thresholds, and classes.</p>"},{"location":"api/track/re_class/#dnt.track.re_class.ReClass","title":"ReClass","text":"<pre><code>ReClass(\n    num_frames: int = 25,\n    threshold: float = 0.75,\n    model: str = \"rtdetr\",\n    weights: str = \"x\",\n    device: str = \"auto\",\n    default_class: int = 0,\n    match_class: list | None = None,\n)\n</code></pre> <p>ReClass is responsible for re-classifying object tracks in video frames using detection results.</p> <p>Attributes:</p> Name Type Description <code>detector</code> <code>Detector</code> <p>The detection model used for re-classification.</p> <code>num_frames</code> <code>int</code> <p>Number of frames to consider for re-classification.</p> <code>threshold</code> <code>float</code> <p>Threshold for matching detections to tracks.</p> <code>default_class</code> <code>int</code> <p>Default class to assign if no match is found.</p> <code>match_class</code> <code>list</code> <p>List of classes to match during re-classification.</p> <p>Methods:</p> Name Description <code>match_mmv</code> <p>Matches a track to detections and computes the average score.</p> <code>re_classify</code> <p>tracks: pd.DataFrame, input_video: str, track_ids: list = None, out_file: str = None, verbose: bool = True</p> <code>) -&gt; pd.DataFrame</code> <p>Re-classifies tracks and returns a DataFrame with results.</p> <p>Re-classify tracks based on detection results.</p> <p>Parameters:</p> Name Type Description Default <code>num_frames</code> <code>int</code> <p>Number of frames to consider for re-classification, default 25</p> <code>25</code> <code>threshold</code> <code>float</code> <p>Threshold for matching, default 0.75</p> <code>0.75</code> <code>model</code> <code>str</code> <p>Detection model to use, default 'rtdetr'</p> <code>'rtdetr'</code> <code>weights</code> <code>str</code> <p>Weights for the detection model, default 'x'</p> <code>'x'</code> <code>device</code> <code>str</code> <p>Device to use for detection, default 'auto'</p> <code>'auto'</code> <code>default_class</code> <code>int</code> <p>Default class to assign if no match found, default 0 (pedestrian)</p> <code>0</code> <code>match_class</code> <code>list</code> <p>List of classes to match, default [1, 36] (bicycle, skateboard/scooter)</p> <code>None</code> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def __init__(\n    self,\n    num_frames: int = 25,\n    threshold: float = 0.75,\n    model: str = \"rtdetr\",\n    weights: str = \"x\",\n    device: str = \"auto\",\n    default_class: int = 0,\n    match_class: list | None = None,\n) -&gt; None:\n    \"\"\"Re-classify tracks based on detection results.\n\n    Parameters\n    ----------\n    num_frames : int\n        Number of frames to consider for re-classification, default 25\n    threshold : float\n        Threshold for matching, default 0.75\n    model : str\n        Detection model to use, default 'rtdetr'\n    weights : str\n        Weights for the detection model, default 'x'\n    device : str\n        Device to use for detection, default 'auto'\n    default_class : int\n        Default class to assign if no match found, default 0 (pedestrian)\n    match_class : list\n        List of classes to match, default [1, 36] (bicycle, skateboard/scooter)\n\n    \"\"\"\n    self.detector = Detector(model=model, device=device)\n    self.num_frames = num_frames\n    self.threshold = threshold\n    self.default_class = default_class\n    self.match_class = match_class if match_class is not None else [1, 36]\n</code></pre>"},{"location":"api/track/re_class/#dnt.track.re_class.ReClass.match_mmv","title":"match_mmv","text":"<pre><code>match_mmv(\n    track: DataFrame, dets: DataFrame\n) -&gt; tuple[bool, float]\n</code></pre> <p>Match track bboxes to detection bboxes and compute average overlap score.</p> <p>Parameters:</p> Name Type Description Default <code>track</code> <code>DataFrame</code> <p>DataFrame containing track data with columns [x, y, w, h, frame].</p> required <code>dets</code> <code>DataFrame</code> <p>DataFrame containing detection data with columns [x, y, w, h, frame, class].</p> required <p>Returns:</p> Type Description <code>tuple[bool, float]</code> <p>A tuple (hit, avg_score) where: - hit : bool     True if average overlap score meets threshold, False otherwise. - avg_score : float     Average Intersection over Box (IoB) score across all matched detections.</p> Notes <p>Only frames present in both track and detection datasets are considered. The matching uses IoB metric from the engine.iob module.</p> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def match_mmv(self, track: pd.DataFrame, dets: pd.DataFrame) -&gt; tuple[bool, float]:\n    \"\"\"Match track bboxes to detection bboxes and compute average overlap score.\n\n    Parameters\n    ----------\n    track : pd.DataFrame\n        DataFrame containing track data with columns [x, y, w, h, frame].\n    dets : pd.DataFrame\n        DataFrame containing detection data with columns [x, y, w, h, frame, class].\n\n    Returns\n    -------\n    tuple[bool, float]\n        A tuple (hit, avg_score) where:\n        - hit : bool\n            True if average overlap score meets threshold, False otherwise.\n        - avg_score : float\n            Average Intersection over Box (IoB) score across all matched detections.\n\n    Notes\n    -----\n    Only frames present in both track and detection datasets are considered.\n    The matching uses IoB metric from the engine.iob module.\n\n    \"\"\"\n    if track.empty or dets.empty:\n        return False, 0.0\n\n    score = 0.0\n    cnt = 0\n    for _, row in track.iterrows():\n        bboxes = row[[\"x\", \"y\", \"w\", \"h\"]].values.reshape(1, -1)\n        det = dets[dets[\"frame\"] == row[\"frame\"]]\n        if len(det) &gt; 0:\n            match_bboxes = det[[\"x\", \"y\", \"w\", \"h\"]].values\n            _, overlaps_mmv = iobs(bboxes, match_bboxes)\n            if overlaps_mmv.size &gt; 0:\n                max_overlap = np.max(overlaps_mmv)\n                if max_overlap &gt;= self.threshold:\n                    score += max_overlap\n                    cnt += 1\n\n    avg_score = score / cnt if cnt &gt; 0 else 0.0\n    hit = avg_score &gt;= self.threshold\n\n    return hit, avg_score\n</code></pre>"},{"location":"api/track/re_class/#dnt.track.re_class.ReClass.re_classify","title":"re_classify","text":"<pre><code>re_classify(\n    tracks: DataFrame,\n    input_video: str,\n    track_ids: list | None = None,\n    out_file: str | None = None,\n    verbose: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Re-classify tracks using detection matching against reference image frame samples.</p> <p>For each track, extracts the top N largest frames (by area), runs detection on those frames, and matches detections against the track bboxes using IoB metric. Assigns the highest-scoring match class if confidence exceeds self.threshold.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>DataFrame</code> <p>Input tracks DataFrame with required columns: track, x, y, w, h, frame. Additional columns are preserved in output.</p> required <code>input_video</code> <code>str</code> <p>Path to source video file from which to extract frame samples.</p> required <code>track_ids</code> <code>list | None</code> <p>List of track IDs to re-classify. If None (default), all tracks in the input are re-classified.</p> <code>None</code> <code>out_file</code> <code>str | None</code> <p>Path to save re-classified results as CSV. If None (default), results are not saved to file.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True (default), display progress bar during re-classification.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output DataFrame with columns [track, cls, avg_score] where: - track : int     Track ID from input tracks. - cls : int     Re-classified class ID. Set to default_class if no match found. - avg_score : float     Maximum IoB score among matched detections, rounded to 2 decimals.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input tracks DataFrame is empty.</p> <code>FileNotFoundError</code> <p>If input_video does not exist.</p> Notes <p>The method considers only the top N frames (self.num_frames) by bounding box area for computational efficiency. It matches detections from match_class list against track bboxes and selects the class with highest average score.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from .re_class import ReClass\n&gt;&gt;&gt; tracks = pd.DataFrame({\n...     'frame': [0, 1, 2],\n...     'track': [1, 1, 1],\n...     'x': [100, 102, 104],\n...     'y': [50, 52, 54],\n...     'w': [50, 50, 50],\n...     'h': [100, 100, 100],\n... })\n&gt;&gt;&gt; rc = ReClass(num_frames=2, threshold=0.75, match_class=[1, 36])\n&gt;&gt;&gt; result = rc.re_classify(tracks, 'video.mp4')\n&gt;&gt;&gt; print(result)  # DataFrame with [track, cls, avg_score]\n</code></pre> Source code in <code>src/dnt/track/re_class.py</code> <pre><code>def re_classify(\n    self,\n    tracks: pd.DataFrame,\n    input_video: str,\n    track_ids: list | None = None,\n    out_file: str | None = None,\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Re-classify tracks using detection matching against reference image frame samples.\n\n    For each track, extracts the top N largest frames (by area), runs detection on\n    those frames, and matches detections against the track bboxes using IoB metric.\n    Assigns the highest-scoring match class if confidence exceeds self.threshold.\n\n    Parameters\n    ----------\n    tracks : pd.DataFrame\n        Input tracks DataFrame with required columns: track, x, y, w, h, frame.\n        Additional columns are preserved in output.\n    input_video : str\n        Path to source video file from which to extract frame samples.\n    track_ids : list | None, optional\n        List of track IDs to re-classify. If None (default), all tracks in\n        the input are re-classified.\n    out_file : str | None, optional\n        Path to save re-classified results as CSV. If None (default), results\n        are not saved to file.\n    verbose : bool, optional\n        If True (default), display progress bar during re-classification.\n\n    Returns\n    -------\n    pd.DataFrame\n        Output DataFrame with columns [track, cls, avg_score] where:\n        - track : int\n            Track ID from input tracks.\n        - cls : int\n            Re-classified class ID. Set to default_class if no match found.\n        - avg_score : float\n            Maximum IoB score among matched detections, rounded to 2 decimals.\n\n    Raises\n    ------\n    ValueError\n        If input tracks DataFrame is empty.\n    FileNotFoundError\n        If input_video does not exist.\n\n    Notes\n    -----\n    The method considers only the top N frames (self.num_frames) by bounding\n    box area for computational efficiency. It matches detections from match_class\n    list against track bboxes and selects the class with highest average score.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from .re_class import ReClass\n    &gt;&gt;&gt; tracks = pd.DataFrame({\n    ...     'frame': [0, 1, 2],\n    ...     'track': [1, 1, 1],\n    ...     'x': [100, 102, 104],\n    ...     'y': [50, 52, 54],\n    ...     'w': [50, 50, 50],\n    ...     'h': [100, 100, 100],\n    ... })\n    &gt;&gt;&gt; rc = ReClass(num_frames=2, threshold=0.75, match_class=[1, 36])\n    &gt;&gt;&gt; result = rc.re_classify(tracks, 'video.mp4')\n    &gt;&gt;&gt; print(result)  # DataFrame with [track, cls, avg_score]\n\n    \"\"\"\n    if tracks.empty:\n        raise ValueError(\"Input tracks DataFrame is empty.\")\n    if not Path(input_video).exists():\n        raise FileNotFoundError(f\"Video file not found: {input_video}\")\n\n    if track_ids is None:\n        track_ids = tracks[\"track\"].unique().tolist()\n\n    results = []\n    if verbose:\n        pbar = tqdm(total=len(track_ids), unit=\"track\", desc=\"Re-classifying tracks\")\n    for track_id in track_ids:\n        target_track = tracks[tracks[\"track\"] == track_id].copy()\n        target_track[\"area\"] = target_track[\"w\"] * target_track[\"h\"]\n        target_track.sort_values(by=\"area\", inplace=True, ascending=False)\n\n        top_frames = target_track.head(self.num_frames) if len(target_track) &gt;= self.num_frames else target_track\n\n        dets = self.detector.detect_frames(input_video, top_frames[\"frame\"].values.tolist())\n\n        matched = []\n        for cls in self.match_class:\n            match_dets = dets[dets[\"class\"] == cls]\n            hit, avg_score = self.match_mmv(top_frames, match_dets)\n            if hit:\n                matched.append((cls, avg_score))\n\n        if len(matched) &gt; 0:\n            cls, avg_score = max(matched, key=lambda x: x[1])\n        else:\n            cls = self.default_class\n            avg_score = 0\n\n        results.append([track_id, cls, round(avg_score, 2)])\n        if verbose:\n            pbar.update()\n    if verbose:\n        pbar.close()\n\n    df = pd.DataFrame(results, columns=[\"track\", \"cls\", \"avg_score\"])\n    if out_file:\n        df.to_csv(out_file, index=False)\n\n    return df\n</code></pre>"},{"location":"api/track/tracker/","title":"Tracker","text":"<p>BoxMOT tracking utilities and unified tracker interface.</p> <p>This module provides the Tracker class for BoxMOT tracking and post-processing utilities for infilling, clustering, and filtering tracks.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTModels","title":"MOTModels","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported tracker backends exposed by BoxMOT.</p> <p>Attributes:</p> Name Type Description <code>BOTSORT</code> <code>str</code> <p>BoT-SORT tracker name used by BoxMOT. Good default when you want motion + appearance matching.</p> <code>BOOSTTRACK</code> <code>str</code> <p>BoostTrack tracker name used by BoxMOT. Usually improves association under difficult motion/crowding.</p> <code>BYTE_TRACK</code> <code>str</code> <p>ByteTrack tracker name used by BoxMOT. Faster and simpler; does not require ReID weights.</p> <code>OCSORT</code> <code>str</code> <p>OCSORT tracker name used by BoxMOT. Motion-centric tracker; useful when appearance features are unreliable.</p> <code>STRONGSORT</code> <code>str</code> <p>StrongSORT tracker name used by BoxMOT. Appearance-heavy tracker; typically more robust to long occlusions.</p> <code>DEEPOCSORT</code> <code>str</code> <p>DeepOCSORT tracker name used by BoxMOT. OCSORT variant enhanced with appearance features.</p> <code>HYBRIDSORT</code> <code>str</code> <p>HybridSORT tracker name used by BoxMOT. Hybrid strategy between motion and appearance matching.</p> <code>SFSORT</code> <code>str</code> <p>SFSort tracker name used by BoxMOT. Lightweight motion-centric tracking for real-time pipelines.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.ReIDWeights","title":"ReIDWeights","text":"<p>               Bases: <code>StrEnum</code></p> <p>Built-in BoxMOT ReID weight file names.</p> <p>Use these enum values for <code>reid_weights</code> in tracker parameter dataclasses.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig","title":"MOTBaseConfig  <code>dataclass</code>","text":"<pre><code>MOTBaseConfig(\n    model: MOTModels = MOTModels.BOTSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n)\n</code></pre> <p>Common configuration fields for BoxMOT tracker creation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>MOTModels</code> <p>BoxMOT tracker backend for this parameter bundle (default: <code>MOTModels.BOTSORT</code>).</p> <code>per_class</code> <code>bool</code> <p>Whether to run tracking independently per class (default: <code>False</code>). Options: <code>True</code>, <code>False</code>. Setting <code>True</code> reduces cross-class ID switches but can create more tracks.</p> <code>extra_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs merged into tracker construction arguments (default: <code>{}</code>). Use this for BoxMOT arguments not explicitly represented in dataclasses.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.MOTBaseConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig","title":"BoTSORTConfig  <code>dataclass</code>","text":"<pre><code>BoTSORTConfig(\n    model: MOTModels = MOTModels.BOTSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    track_high_thresh: float = 0.5,\n    track_low_thresh: float = 0.1,\n    new_track_thresh: float = 0.6,\n    match_thresh: float = 0.8,\n    track_buffer: int = 30,\n    with_reid: bool = True,\n    proximity_thresh: float = 0.5,\n    appearance_thresh: float = 0.25,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>BoTSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: <code>None</code>, file name (auto-resolved), or absolute path. Built-in downloadable options: <code>'resnet50_market1501.pt'</code>, <code>'resnet50_dukemtmcreid.pt'</code>, <code>'resnet50_msmt17.pt'</code>, <code>'resnet50_fc512_market1501.pt'</code>, <code>'resnet50_fc512_dukemtmcreid.pt'</code>, <code>'resnet50_fc512_msmt17.pt'</code>, <code>'mlfn_market1501.pt'</code>, <code>'mlfn_dukemtmcreid.pt'</code>, <code>'mlfn_msmt17.pt'</code>, <code>'hacnn_market1501.pt'</code>, <code>'hacnn_dukemtmcreid.pt'</code>, <code>'hacnn_msmt17.pt'</code>, <code>'mobilenetv2_x1_0_market1501.pt'</code>, <code>'mobilenetv2_x1_0_dukemtmcreid.pt'</code>, <code>'mobilenetv2_x1_0_msmt17.pt'</code>, <code>'mobilenetv2_x1_4_market1501.pt'</code>, <code>'mobilenetv2_x1_4_dukemtmcreid.pt'</code>, <code>'mobilenetv2_x1_4_msmt17.pt'</code>, <code>'osnet_x1_0_market1501.pt'</code>, <code>'osnet_x1_0_dukemtmcreid.pt'</code>, <code>'osnet_x1_0_msmt17.pt'</code>, <code>'osnet_x0_75_market1501.pt'</code>, <code>'osnet_x0_75_dukemtmcreid.pt'</code>, <code>'osnet_x0_75_msmt17.pt'</code>, <code>'osnet_x0_5_market1501.pt'</code>, <code>'osnet_x0_5_dukemtmcreid.pt'</code>, <code>'osnet_x0_5_msmt17.pt'</code>, <code>'osnet_x0_25_market1501.pt'</code>, <code>'osnet_x0_25_dukemtmcreid.pt'</code>, <code>'osnet_x0_25_msmt17.pt'</code>, <code>'osnet_ibn_x1_0_msmt17.pt'</code>, <code>'osnet_ain_x1_0_msmt17.pt'</code>, <code>'lmbn_n_duke.pt'</code>, <code>'lmbn_n_market.pt'</code>, <code>'lmbn_n_cuhk03_d.pt'</code>, <code>'clip_market1501.pt'</code>, <code>'clip_duke.pt'</code>, <code>'clip_veri.pt'</code>, <code>'clip_vehicleid.pt'</code>. Suggestions: use <code>\"osnet_x1_0_msmt17.pt\"</code> for pedestrians or <code>\"clip_vehicleid.pt\"</code> / <code>\"clip_veri.pt\"</code> for vehicles.</p> <code>track_high_thresh</code> <code>float</code> <p>High score threshold for first association (default: <code>0.5</code>). Increasing this is stricter and may reduce false matches but miss tracks.</p> <code>track_low_thresh</code> <code>float</code> <p>Lower score threshold for second association (default: <code>0.1</code>). Increasing this keeps fewer low-confidence detections.</p> <code>new_track_thresh</code> <code>float</code> <p>Threshold to initialize new tracks (default: <code>0.6</code>). Increasing this creates fewer new tracks and can reduce false positives.</p> <code>match_thresh</code> <code>float</code> <p>Matching threshold for association (default: <code>0.8</code>). Increasing this makes association more permissive.</p> <code>track_buffer</code> <code>int</code> <p>Number of frames to keep lost tracks (default: <code>30</code>). Increasing this preserves IDs longer through occlusion, but may cause stale tracks to survive longer.</p> <code>with_reid</code> <code>bool</code> <p>Whether to enable ReID-assisted association (default: <code>True</code>). Options: <code>True</code>, <code>False</code>. Disabling this speeds up tracking but may increase ID switches.</p> <code>proximity_thresh</code> <code>float</code> <p>Proximity threshold for ReID matching (default: <code>0.5</code>). Increasing this requires stronger geometric overlap before ReID is used.</p> <code>appearance_thresh</code> <code>float</code> <p>Appearance similarity threshold for ReID matching (default: <code>0.25</code>). Increasing this requires closer appearance match and is more conservative.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoTSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig","title":"BoostTrackConfig  <code>dataclass</code>","text":"<pre><code>BoostTrackConfig(\n    model: MOTModels = MOTModels.BOOSTTRACK,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>BoostTrack-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this keeps only higher-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks alive longer when unmatched.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays confirmation and reduces short noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this demands tighter overlap to match detections.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.BoostTrackConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig","title":"ByteTrackConfig  <code>dataclass</code>","text":"<pre><code>ByteTrackConfig(\n    model: MOTModels = MOTModels.BYTE_TRACK,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    track_thresh: float = 0.5,\n    match_thresh: float = 0.8,\n    track_buffer: int = 30,\n    frame_rate: int = 30,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>ByteTrack-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>track_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.5</code>). Increasing this filters more weak detections.</p> <code>match_thresh</code> <code>float</code> <p>Threshold for matching detections to tracks (default: <code>0.8</code>). Increasing this generally allows looser matching.</p> <code>track_buffer</code> <code>int</code> <p>Number of frames to keep lost tracks (default: <code>30</code>). Increasing this keeps unmatched tracks longer.</p> <code>frame_rate</code> <code>int</code> <p>Source video frame rate used by the tracker (default: <code>30</code>). Set this close to real FPS for best temporal behavior.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.ByteTrackConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig","title":"OCSORTConfig  <code>dataclass</code>","text":"<pre><code>OCSORTConfig(\n    model: MOTModels = MOTModels.OCSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n    delta_t: int = 3,\n    inertia: float = 0.2,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>OCSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces low-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks alive through longer gaps.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces short-lived false tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this makes matching stricter.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p> <code>delta_t</code> <code>int</code> <p>Time gap used by motion compensation (default: <code>3</code>). Increasing this smooths longer motion history, but may lag quick turns.</p> <code>inertia</code> <code>float</code> <p>Motion inertia weight (default: <code>0.2</code>). Increasing this trusts previous velocity more.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.OCSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig","title":"StrongSORTConfig  <code>dataclass</code>","text":"<pre><code>StrongSORTConfig(\n    model: MOTModels = MOTModels.STRONGSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    max_dist: float = 0.2,\n    max_iou_dist: float = 0.7,\n    max_age: int = 70,\n    n_init: int = 3,\n    nn_budget: int = 100,\n    ema_alpha: float = 0.9,\n    mc_lambda: float = 0.995,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>StrongSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>max_dist</code> <code>float</code> <p>Maximum cosine distance for appearance matching (default: <code>0.2</code>). Increasing this allows less similar appearance matches.</p> <code>max_iou_dist</code> <code>float</code> <p>Maximum IoU distance for geometric matching (default: <code>0.7</code>). Increasing this allows looser geometric matches.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>70</code>). Increasing this retains tracks through longer occlusions.</p> <code>n_init</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays confirmation and reduces unstable IDs.</p> <code>nn_budget</code> <code>int</code> <p>Maximum size of appearance feature gallery (default: <code>100</code>). Increasing this improves long-term matching memory at higher memory cost.</p> <code>ema_alpha</code> <code>float</code> <p>EMA factor for appearance embeddings (default: <code>0.9</code>). Increasing this smooths features more and reduces noise.</p> <code>mc_lambda</code> <code>float</code> <p>Motion compensation blending factor (default: <code>0.995</code>). Increasing this gives more weight to motion compensation.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.StrongSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig","title":"DeepOCSORTConfig  <code>dataclass</code>","text":"<pre><code>DeepOCSORTConfig(\n    model: MOTModels = MOTModels.DEEPOCSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n    delta_t: int = 3,\n    inertia: float = 0.2,\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>DeepOCSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this keeps fewer low-confidence detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps unmatched tracks alive longer.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this delays track confirmation.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this requires tighter overlap.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p> <code>delta_t</code> <code>int</code> <p>Time gap used by motion compensation (default: <code>3</code>). Increasing this smooths over longer temporal windows.</p> <code>inertia</code> <code>float</code> <p>Motion inertia weight (default: <code>0.2</code>). Increasing this emphasizes velocity continuity.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.DeepOCSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig","title":"HybridSORTConfig  <code>dataclass</code>","text":"<pre><code>HybridSORTConfig(\n    model: MOTModels = MOTModels.HYBRIDSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    reid_weights: ReIDWeights\n    | str\n    | None = ReIDWeights.OSNET_X1_0_MSMT17,\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>HybridSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>reid_weights</code> <code>ReIDWeights | str | None</code> <p>Optional ReID weights path (default: <code>\"osnet_x1_0_msmt17.pt\"</code>). Options: same built-in <code>.pt</code> names listed in <code>BoTSORTParams.reid_weights</code>.</p> <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces weak detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks longer during occlusion.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces early noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this makes IoU matching stricter.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.HybridSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig","title":"SFSORTConfig  <code>dataclass</code>","text":"<pre><code>SFSORTConfig(\n    model: MOTModels = MOTModels.SFSORT,\n    per_class: bool = False,\n    extra_kwargs: dict[str, Any] = dict(),\n    det_thresh: float = 0.3,\n    max_age: int = 30,\n    min_hits: int = 3,\n    iou_threshold: float = 0.3,\n    asso_func: str = \"iou\",\n)\n</code></pre> <p>               Bases: <code>MOTBaseConfig</code></p> <p>SFSORT-specific parameters.</p> <p>Attributes:</p> Name Type Description <code>det_thresh</code> <code>float</code> <p>Detection confidence threshold (default: <code>0.3</code>). Increasing this reduces weak detections.</p> <code>max_age</code> <code>int</code> <p>Maximum age of unmatched tracks (default: <code>30</code>). Increasing this keeps tracks longer through brief misses.</p> <code>min_hits</code> <code>int</code> <p>Minimum hits before track confirmation (default: <code>3</code>). Increasing this reduces short-lived noisy tracks.</p> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for association (default: <code>0.3</code>). Increasing this requires tighter overlap for matching.</p> <code>asso_func</code> <code>str</code> <p>Association function name (default: <code>\"iou\"</code>). Typical options: <code>\"iou\"</code>, <code>\"giou\"</code>, <code>\"diou\"</code>, <code>\"ciou\"</code>, <code>\"centroid\"</code>.</p>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig.to_kwargs","title":"to_kwargs","text":"<pre><code>to_kwargs() -&gt; dict[str, Any]\n</code></pre> <p>Convert dataclass fields to keyword arguments for BoxMOT tracker creation.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert dataclass fields to keyword arguments for BoxMOT tracker creation.\"\"\"\n    kwargs = asdict(self)\n    kwargs.pop(\"model\", None)\n    kwargs.pop(\"extra_kwargs\", None)\n    kwargs.update(self.extra_kwargs)\n    return kwargs\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return dataclass values as a serializable dictionary.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dataclass values as a serializable dictionary.\"\"\"\n    return self._yaml_safe(asdict(self))\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; MOTBaseConfig\n</code></pre> <p>Build a parameter object from a dictionary.</p> <p>Unknown keys are stored in <code>extra_kwargs</code>.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"MOTBaseConfig\":\n    \"\"\"Build a parameter object from a dictionary.\n\n    Unknown keys are stored in `extra_kwargs`.\n    \"\"\"\n    valid_fields = {f.name for f in fields(cls)}\n    known = {k: v for k, v in data.items() if k in valid_fields}\n    unknown = {k: v for k, v in data.items() if k not in valid_fields}\n\n    if \"model\" in known and not isinstance(known[\"model\"], MOTModels):\n        known[\"model\"] = MOTModels(str(known[\"model\"]))\n\n    params = cls(**known)\n    if unknown:\n        params.extra_kwargs.update(unknown)\n    return params\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig.export_yaml","title":"export_yaml","text":"<pre><code>export_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export parameters to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export parameters to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.SFSORTConfig.import_yaml","title":"import_yaml  <code>classmethod</code>","text":"<pre><code>import_yaml(yaml_file: str) -&gt; MOTBaseConfig\n</code></pre> <p>Import parameters from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@classmethod\ndef import_yaml(cls, yaml_file: str) -&gt; \"MOTBaseConfig\":\n    \"\"\"Import parameters from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker","title":"Tracker","text":"<pre><code>Tracker(\n    config: BoxMOTModelParams | None = None,\n    config_yaml: str | None = None,\n    device: str = \"auto\",\n    half: bool = False,\n    output_score_cls: bool = True,\n    boxmot_verbose: bool = False,\n)\n</code></pre> <p>Unified interface for BoxMOT tracking and track post-processing.</p> <p>This class runs BoxMOT tracking given a detection file and source video. It also provides post-processing utilities to infill missing frames, split tracks by large gaps, and drop short tracks.</p> <p>Attributes:</p> Name Type Description <code>TRACK_FIELDS</code> <code>list[str]</code> <p>Standard output columns for tracking and post-processing utilities (default: class constant).</p> <code>device</code> <code>str</code> <p>Device string used by deep trackers (default: <code>\"auto\"</code>). Options: <code>\"auto\"</code>, <code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code>.</p> <code>half</code> <code>bool</code> <p>Whether half precision is enabled for deep trackers (default: <code>False</code>). Options: <code>True</code>, <code>False</code>. Enabling can improve speed on supported GPUs.</p> <code>boxmot_model</code> <code>MOTModels</code> <p>Selected BoxMOT tracker backend (default: <code>MOTModels.BOTSORT</code>). Options: <code>MOTModels.BOTSORT</code>, <code>MOTModels.BOOSTTRACK</code>, <code>MOTModels.BYTE_TRACK</code>, <code>MOTModels.OCSORT</code>, <code>MOTModels.STRONGSORT</code>, <code>MOTModels.DEEPOCSORT</code>, <code>MOTModels.HYBRIDSORT</code>, <code>MOTModels.SFSORT</code>.</p> <code>boxmot_config</code> <code>BoxMOTModelConfig</code> <p>Configuration dataclass instance for BoxMOT tracker creation (default: model-specific defaults).</p> <code>boxmot_verbose</code> <code>bool</code> <p>If False, suppress BoxMOT INFO/SUCCESS logging output.</p> <code>output_score_cls</code> <code>bool</code> <p>Whether to include tracker <code>score</code> and <code>cls</code> values in outputs. If <code>False</code>, both fields are exported as <code>-1</code> to keep file schema stable.</p> <code>REID_WEIGHTS_DIR</code> <code>Path</code> <p>Directory where relative ReID weights are resolved and stored.</p> <code>DEFAULT_REID_WEIGHT</code> <code>str</code> <p>Fallback ReID weight file name used when a model expects ReID and no weight is explicitly set.</p> <p>Initialize the tracker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BoxMOTModelConfig</code> <p>Configuration bundle for BoxMOT tracker creation. Tracker backend is selected from <code>config.model</code>.</p> <code>None</code> <code>config_yaml</code> <code>str</code> <p>YAML file containing model-aware config. When provided, values loaded from YAML override <code>config</code> input.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device string used by deep trackers (default: <code>\"auto\"</code>, \"cpu\", \"cuda\", \"mps\").</p> <code>'auto'</code> <code>half</code> <code>bool</code> <p>Whether half precision is enabled for deep trackers (default: <code>False</code>).</p> <code>False</code> <code>output_score_cls</code> <code>bool</code> <p>If True, output tracker confidence and class values in <code>score</code> and <code>cls</code> columns. If False, export <code>-1</code> for both fields.</p> <code>True</code> <code>boxmot_verbose</code> <code>bool</code> <p>If False, suppress BoxMOT INFO/SUCCESS logging output.</p> <code>False</code> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def __init__(\n    self,\n    config: BoxMOTModelParams | None = None,\n    config_yaml: str | None = None,\n    device: str = \"auto\",\n    half: bool = False,\n    output_score_cls: bool = True,\n    boxmot_verbose: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the tracker.\n\n    Parameters\n    ----------\n    config : BoxMOTModelConfig, optional\n        Configuration bundle for BoxMOT tracker creation. Tracker backend\n        is selected from `config.model`.\n    config_yaml : str, optional\n        YAML file containing model-aware config. When provided,\n        values loaded from YAML override `config` input.\n    device : str, optional\n        Device string used by deep trackers (default: `\"auto\"`, \"cpu\", \"cuda\", \"mps\").\n    half : bool, optional\n        Whether half precision is enabled for deep trackers (default: `False`).\n    output_score_cls : bool, optional\n        If True, output tracker confidence and class values in `score` and\n        `cls` columns. If False, export `-1` for both fields.\n    boxmot_verbose : bool, optional\n        If False, suppress BoxMOT INFO/SUCCESS logging output.\n\n    \"\"\"\n    self.device = device\n    self.half = half\n    self.boxmot_verbose = boxmot_verbose\n    self.output_score_cls = output_score_cls\n    yaml_path = config_yaml\n    resolved_config = config or config\n    self.model_config_yaml = yaml_path\n\n    if yaml_path:\n        resolved_config = self.import_config_from_yaml(yaml_path)\n\n    if resolved_config is None:\n        resolved_config = self._default_boxmot_config()\n\n    self.boxmot_model = resolved_config.model\n    self.boxmot_config = resolved_config\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.track","title":"track","text":"<pre><code>track(\n    det_file: str,\n    out_file: str,\n    video_file: str | None = None,\n    show: bool = False,\n    video_index: int | None = None,\n    total_videos: int | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run tracking on a single detection file using BoxMOT.</p> <p>Parameters:</p> Name Type Description Default <code>det_file</code> <code>str</code> <p>Path to detection file (CSV format with columns: frame, x1, y1, width, height, confidence, class_id).</p> required <code>out_file</code> <code>str</code> <p>Path to write tracking results. If empty string, results are not saved.</p> required <code>video_file</code> <code>str</code> <p>Path to source video file. Required for BoxMOT tracker.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True (default: False), display live tracking preview with bounding boxes and track IDs. Press 's' to toggle preview, 'ESC' to hide, 'q' to stop tracking early.</p> <code>False</code> <code>video_index</code> <code>int</code> <p>Index of current video in batch (for progress bar display).</p> <code>None</code> <code>total_videos</code> <code>int</code> <p>Total number of videos in batch (for complete progress context).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tracking results with columns: frame, track, x, y, w, h, score, cls, r3, r4 Each row represents one detected object per frame.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If det_file or video_file does not exist.</p> <code>ValueError</code> <p>If video_file is None.</p> Notes <p>The tracker processes detections frame-by-frame, maintaining track IDs across frames. Detection coordinates are converted from (x1, y1, x2, y2) to (x, y, width, height) format for BoxMOT.</p> <p>Track IDs are persistent across frame sequences and reused if tracks are lost and then re-acquired within track_buffer frames.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def track(\n    self,\n    det_file: str,\n    out_file: str,\n    video_file: str | None = None,\n    show: bool = False,\n    video_index: int | None = None,\n    total_videos: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Run tracking on a single detection file using BoxMOT.\n\n    Parameters\n    ----------\n    det_file : str\n        Path to detection file (CSV format with columns:\n        frame, x1, y1, width, height, confidence, class_id).\n    out_file : str\n        Path to write tracking results. If empty string, results are not saved.\n    video_file : str, optional\n        Path to source video file. Required for BoxMOT tracker.\n    show : bool, optional\n        If True (default: False), display live tracking preview with bounding\n        boxes and track IDs. Press 's' to toggle preview, 'ESC' to hide,\n        'q' to stop tracking early.\n    video_index : int, optional\n        Index of current video in batch (for progress bar display).\n    total_videos : int, optional\n        Total number of videos in batch (for complete progress context).\n\n    Returns\n    -------\n    pd.DataFrame\n        Tracking results with columns: frame, track, x, y, w, h, score, cls, r3, r4\n        Each row represents one detected object per frame.\n\n    Raises\n    ------\n    FileNotFoundError\n        If det_file or video_file does not exist.\n    ValueError\n        If video_file is None.\n\n    Notes\n    -----\n    The tracker processes detections frame-by-frame, maintaining track IDs\n    across frames. Detection coordinates are converted from (x1, y1, x2, y2)\n    to (x, y, width, height) format for BoxMOT.\n\n    Track IDs are persistent across frame sequences and reused if tracks\n    are lost and then re-acquired within track_buffer frames.\n\n    \"\"\"\n    if not Path(det_file).exists():\n        msg = f\"Detection file not found: {det_file}\"\n        raise FileNotFoundError(msg)\n\n    if video_file is None:\n        msg = \"Video file required for BoxMOT tracking but not provided.\"\n        raise ValueError(msg)\n    if not Path(video_file).exists():\n        msg = f\"Video file not found: {video_file}\"\n        raise FileNotFoundError(msg)\n\n    return self._track_boxmot(\n        video_file=video_file,\n        det_file=det_file,\n        out_file=out_file,\n        show=show,\n        video_index=video_index,\n        total_videos=total_videos,\n    )\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.track_batch","title":"track_batch","text":"<pre><code>track_batch(\n    det_files: list[str] | None = None,\n    video_files: list[str] | None = None,\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n) -&gt; list[str]\n</code></pre> <p>Run tracking on multiple detection files sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>det_files</code> <code>list[str] | None</code> <p>List of detection file paths. Each file should contain frame-level detections in CSV format. If None (default), returns empty list.</p> <code>None</code> <code>video_files</code> <code>list[str] | None</code> <p>List of corresponding source video file paths for each detection file. Length should match det_files. Required for BoxMOT tracking.</p> <code>None</code> <code>output_path</code> <code>str | None</code> <p>Directory to save tracking results. Track files are named based on input filename with '_track.txt' suffix. If None (default), tracking still runs but results are not persisted.</p> <code>None</code> <code>is_overwrite</code> <code>bool</code> <p>If False (default), skip tracking for videos with existing output files.</p> <code>False</code> <code>is_report</code> <code>bool</code> <p>If True (default), include skipped files in returned list.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of output track file paths. Includes both newly created and existing files (if is_report=True). Empty list if det_files is None.</p> Notes <p>Processing is sequential (not parallel). Each detection file is tracked in order with progress display showing \"Tracking X of Y\".</p> <p>Files matching between det_files and video_files by index position. If video_files is shorter than det_files, missing videos are left None and those detections are skipped.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def track_batch(\n    self,\n    det_files: list[str] | None = None,\n    video_files: list[str] | None = None,\n    output_path: str | None = None,\n    is_overwrite: bool = False,\n    is_report: bool = True,\n) -&gt; list[str]:\n    \"\"\"Run tracking on multiple detection files sequentially.\n\n    Parameters\n    ----------\n    det_files : list[str] | None, optional\n        List of detection file paths. Each file should contain frame-level\n        detections in CSV format. If None (default), returns empty list.\n    video_files : list[str] | None, optional\n        List of corresponding source video file paths for each detection file.\n        Length should match det_files. Required for BoxMOT tracking.\n    output_path : str | None, optional\n        Directory to save tracking results. Track files are named based on\n        input filename with '_track.txt' suffix. If None (default),\n        tracking still runs but results are not persisted.\n    is_overwrite : bool, optional\n        If False (default), skip tracking for videos with existing output files.\n    is_report : bool, optional\n        If True (default), include skipped files in returned list.\n\n    Returns\n    -------\n    list[str]\n        List of output track file paths. Includes both newly created and\n        existing files (if is_report=True). Empty list if det_files is None.\n\n    Notes\n    -----\n    Processing is sequential (not parallel). Each detection file is tracked\n    in order with progress display showing \"Tracking X of Y\".\n\n    Files matching between det_files and video_files by index position.\n    If video_files is shorter than det_files, missing videos are left None\n    and those detections are skipped.\n\n    \"\"\"\n    if det_files is None:\n        return []\n\n    results: list[str] = []\n    total_videos = len(det_files)\n\n    for idx, det_file in enumerate(det_files, start=1):\n        base_filename = os.path.splitext(os.path.basename(det_file))[0].replace(\"_iou\", \"\")\n\n        track_file = None\n        if output_path:\n            os.makedirs(output_path, exist_ok=True)\n            track_file = os.path.join(output_path, base_filename + \"_track.txt\")\n\n        if track_file and not is_overwrite and os.path.exists(track_file):\n            if is_report:\n                results.append(track_file)\n            continue\n\n        # BoxMOT requires a matching source video.\n        video_file = None\n        if video_files is not None:  # noqa: SIM102\n            if idx - 1 &lt; len(video_files):\n                video_file = video_files[idx - 1]\n\n        # run tracking\n        self.track(\n            det_file=det_file,\n            out_file=track_file if track_file else \"\",  # track() expects a path\n            video_file=video_file,\n            video_index=idx,\n            total_videos=total_videos,\n        )\n\n        if track_file:\n            results.append(track_file)\n\n    return results\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.export_config_to_yaml","title":"export_config_to_yaml  <code>staticmethod</code>","text":"<pre><code>export_config_to_yaml(\n    yaml_file: str, config: BoxMOTModelParams\n) -&gt; None\n</code></pre> <p>Export model-aware BoxMOT config to a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef export_config_to_yaml(\n    yaml_file: str,\n    config: BoxMOTModelParams,\n) -&gt; None:\n    \"\"\"Export model-aware BoxMOT config to a YAML file.\"\"\"\n    out_path = Path(yaml_file)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(config.to_dict(), f, sort_keys=False)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.import_config_from_yaml","title":"import_config_from_yaml  <code>staticmethod</code>","text":"<pre><code>import_config_from_yaml(\n    yaml_file: str,\n) -&gt; BoxMOTModelParams\n</code></pre> <p>Import model-aware BoxMOT config from a YAML file.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef import_config_from_yaml(yaml_file: str) -&gt; BoxMOTModelParams:\n    \"\"\"Import model-aware BoxMOT config from a YAML file.\"\"\"\n    with Path(yaml_file).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f) or {}\n    if not isinstance(data, dict):\n        msg = f\"Invalid YAML content in {yaml_file}: expected a mapping.\"\n        raise ValueError(msg)\n\n    model = MOTModels(str(data.get(\"model\", MOTModels.BOTSORT.value)))\n    param_cls = Tracker._params_class_for_model(model)\n    return param_cls.from_dict(data)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.export_params_to_yaml","title":"export_params_to_yaml  <code>staticmethod</code>","text":"<pre><code>export_params_to_yaml(\n    yaml_file: str, params: BoxMOTModelParams\n) -&gt; None\n</code></pre> <p>Export model-aware BoxMOT params to a YAML file (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef export_params_to_yaml(\n    yaml_file: str,\n    params: BoxMOTModelParams,\n) -&gt; None:\n    \"\"\"Export model-aware BoxMOT params to a YAML file (backward-compatible wrapper).\"\"\"\n    Tracker.export_config_to_yaml(yaml_file=yaml_file, config=params)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.import_params_from_yaml","title":"import_params_from_yaml  <code>staticmethod</code>","text":"<pre><code>import_params_from_yaml(\n    yaml_file: str,\n) -&gt; BoxMOTModelParams\n</code></pre> <p>Import model-aware BoxMOT params from a YAML file (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>@staticmethod\ndef import_params_from_yaml(yaml_file: str) -&gt; BoxMOTModelParams:\n    \"\"\"Import model-aware BoxMOT params from a YAML file (backward-compatible wrapper).\"\"\"\n    return Tracker.import_config_from_yaml(yaml_file=yaml_file)\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.export_current_config_to_yaml","title":"export_current_config_to_yaml","text":"<pre><code>export_current_config_to_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export this tracker's active model and config to YAML.</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_current_config_to_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export this tracker's active model and config to YAML.\"\"\"\n    self.export_config_to_yaml(\n        yaml_file=yaml_file,\n        config=self.boxmot_config,\n    )\n</code></pre>"},{"location":"api/track/tracker/#dnt.track.tracker.Tracker.export_current_params_to_yaml","title":"export_current_params_to_yaml","text":"<pre><code>export_current_params_to_yaml(yaml_file: str) -&gt; None\n</code></pre> <p>Export this tracker's active model and config to YAML (backward-compatible wrapper).</p> Source code in <code>src/dnt/track/tracker.py</code> <pre><code>def export_current_params_to_yaml(self, yaml_file: str) -&gt; None:\n    \"\"\"Export this tracker's active model and config to YAML (backward-compatible wrapper).\"\"\"\n    self.export_current_config_to_yaml(yaml_file)\n</code></pre>"}]}